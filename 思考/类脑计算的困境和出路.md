其实相对量化地认识到一个问题有多难本身也是一个非常困难的事情，因为如果能知道一个事情本身有多难，也需要对这个事情实现的路径有一定的概念，才能了解路径上各种障碍。

一个层面是神经元仿真平台的构建上严重缺乏软硬件的架构思维，导致这十几年的类脑计算平台上的扑腾大部分是在缴这方面的学费，至今都没走上正轨。我们今天的计算机系统能构建如此庞大复杂的生态，同时使用门槛极低，海量从业者能保持一定秩序地相互协作，很大程度归功于计算机系统整体宏观架构上的分层与解耦合设计。而类脑计算想要重塑一整套计算系统，则严重缺乏这种分层与解耦合的思维，对分层与解耦合的理解非常浅显，缺乏对复杂性的敬畏，一脚踩进了软硬件协同的泥潭里，在基本的分层基石都没有建立的情况下，为了提高类脑芯片的集成度，还引入大量硬件约束，只为了获得那么一点集成度的提高和功耗的下降。我在学校的时候主要做的就是这一块软件的适配，类脑芯片的这种糟糕设计给软件带来了大量问题，当然也带来了不少水论文的机会，我也水了不少类脑编译器的论文，不过我也一直都有种类脑芯片创造一个本来不存在的问题再通过类脑编译器去解决的感觉。其实国际上不少做类脑计算的大组也都在重复这样一个过程，当然我这种做类脑编译器的思路已经算是在往解耦合的方向努力了，大多数组更多是围绕自己设计的类脑芯片的约束下来重新设计类脑算法，这些算法主要要解决的不是具体的类脑智能问题，而是芯片约束带来的问题，有种带着镣铐跳舞的感觉。

此外，在类脑计算整个发展的路径上，整个类脑领域又逐渐和脉冲神经网络（SNN），忆阻器，存算一体等概念产生了一定的绑定，更是给软硬件架构带来毁灭性冲击。忆阻器把模拟计算引入了进来，存算一体把非冯架构引入了进来，而SNN又一定程度把深度学习挡在了外面。这几点每一个都在给整个领域引入巨大的难度。现阶段大部分类脑方面的努力都在和这些问题做斗争，我们哪里还有精力去思考如何类脑来制造更强的智能？我博士最后一年写了一篇关于类脑解耦合的论文发在了nature正刊上，核心想法是想把类脑计算系统从这种缺乏架构思维的困局中拉回来，至少拉回到和现在的计算机系统的起点处。但说实话，这种工作其实也没有解决什么科学问题，因为即使拉回来了，类脑计算系统的能力现在也只处于计算机系统起步的年代，注意这里不是类比，类脑计算系统和计算机系统并不是平行的赛道，两者其实就是一个赛道！所以那篇论文我自己都觉得没多大用，从计算机的视角看更没有带来太多新的东西。更多是一种比较含蓄的方式尝试指出类脑计算领域现在的问题。

我们仔细回想一下类脑最初的目标：实现类脑智能。为什么我们却强行给这条路赋予了类脑芯片、SNN、忆阻器等镣铐？实现类脑智能基于现有的计算机系统做到底遇到了什么问题需要我们重塑整个软硬件呢？其实类脑计算折腾了这么多，获得的成就却远不如基于计算机系统的深度学习在实现智能方面走得远。其实深度学习的成功也给类脑计算领域带来了巨大的压力，一方面确实打开了原先类脑计算在算法层面的思路，但另一方面又降低了类脑方向的标识度。毕竟广大做深度学习的人并不认为自己是做类脑的，那么做类脑的人怎么标识出自己是做类脑的呢？这种尴尬的状况导致目前大多数做类脑的人一方面积极拥抱深度学习，另一方面又要加入一些SNN等佐料来体现和纯粹做深度学习的不同，这和最初的目标真的是相差太远了，甚至不能认为是朝着这个目标的方向努力。

当然，现代计算机系统存在大量问题，深度学习也存在大量问题，离类脑智能还差非常远。这也是类脑计算的论文在motivation中常常会写的内容。但类脑计算在解决这些问题的方式上，都偏离了各个领域专业的做法，显得极其业余。计算机系统确实存在很多瓶颈，但类脑芯片的设计上主要考虑绝对不是PPA，深度学习也存在大量缺陷，但类脑算法设计上也都缺乏完整的逻辑链来阐述如何解决这些缺陷，一定程度上对这些领域已取得的成果所解决的问题缺乏基本的认知和敬畏，你指望这样的方法论能帮你突破现有的系统？

当然上面这个层面讲的更多是现阶段战术层面的问题，导致很多年的努力更多是在解决自己创造出来的问题，并且是现有系统已经解决得很好得问题。更严重的其实是战略层面的问题，也是我前面说的第二个层面的问题。

第二个层面的不靠谱是对整个类脑计算问题实现难度的认知层面的，当然我相信所有做类脑的人都肯定认为实现类脑智能是极其困难的，但这种困难是缺乏量化的认知的。并且从神经形态计算做全脑仿真这样的技术路径也可以看出来，即使认为这个方向非常困难，整个方向的宏观技术路线仍然是严重低估了难度的。目前的方向一定程度上还是从神经元的规模这个维度来度量我们走到了哪一步，先理解神经元的模型，接下来是几百个神经元的虫子的模型，再理解小鼠规模的模型，然后理解猴脑，最后到人脑规模，对困难的认知是基于规模的线性认知。

这种技术路径隐含的假设很大概率是错误的，我目前也是因此完全抛弃了类脑计算现有的路径。当然了，我相信这些问题所有做类脑的人应该也又切身体会，喷归喷，问题是出路在哪里？我对于任何领域肯定不会光喷不给新的解决思路。篇幅原因，这篇就写到这里了，下次再详细聊一聊战略层面的困难在哪里以及出路在哪里。

觉得类脑的核心还是智能结构和learning rule/algorithm, 而不在于spiking, 或者忆阻器这类实现方式。确实该抛掉这些枷锁，着力在结构和learning rule上。

束缚住早期人工智能的算法复杂度为什么似乎并不是当今的深度学习的主要瓶颈？算法复杂度理论是针对计算机算法的一套理论，对于特定问题（例如下围棋），其最小算法复杂度是与问题本身的结构高度相关的，理论上这些问题的复杂度就高到无法求解。而当今的深度学习仍然是计算机算法，为什么在这些问题上显然没有被理论上的算法复杂度锁死呢？

==相信很多人都能给出不少解释，不过我想从更加宏观的角度来阐述这个问题。凝聚态物理有一篇非常有名的论文《More is Different》，也算是为凝聚态物理正名的一篇文章。这篇文章的核心思想正是论文的标题。物理学长期的方法论一直都是还原论的思想，将粒子打碎成更小的基本粒子，从而将很多规律统一到更少的规律上，如果我们能找到更加大一统的规律，我们就可以更全面的认识宇宙。而More is Different这篇文章指出，认识世界不止包含每个基本组件的规律，更包含了组件之间的组织形式。当体系的复杂性增加到一定程度，会涌现出与基本组件规律无关的新规律，而这部分规律也是认识世界的一部分。

更具体一点，学科X的基本实体遵循学科Y的定律并不意味着学科X仅仅是Y的应用，X具有全新的规律。例如固体/多体物理基于粒子物理，化学基于多体物理，分子生物学又基于化学，细胞生物学进一步基于分子生物学，……，心理学基于生理学，社会科学又基于心理学。层级隔的越远，这种规律的独立性体现得越直观，毕竟社会科学的规律靠研究粒子物理怕是永远也得不出来。这就是层级的体现，每一层涌现的新规律都独立于下面层级的规律。这种独立性的另一个体现在于，即使我们更换了支撑底层系统的构成方式与规律，只要高层级规律所依赖的基本抽象不变，高层级的很多规律仍然会成立。

涌现

当然涌现性并不特殊，随着系统复杂性的增加，涌现会频繁发生，更关键的是复杂性的维度，举个简单的例子，比如计算机系统的基本组成就是一个CPU以及相应的外设，如果沿着数量的维度增加复杂性，就会涌现出超算、集群、云、互联网；如果沿着指令序列的维度增加复杂性，就会涌现出繁荣复杂的软件生态，不同的维度涌现出的规律也是各不相同的。

深度学习和图灵机的关系也前面说到的不同层级一样：深度学习基于图灵机，但深度学习具有独立于图灵机的能力，让我们这几年在很多传统图灵机算法领域取得突破性进展。那么深度学习带来的新规律是来自于哪个维度呢？

大家可以想一个问题，既然神经网络基于图灵机，又能解决很多传统图灵机算法解决不了的问题，那么我们能不能像拿着标准答案抄作业一样，用传统程序的方式，抄一个效果和神经网络差不多的程序来。比如说五十年前给你一个训练好的resnet模型作为参考答案，让你按照计算机程序的方式写一个图像分类器达到resnet模型的效果。你会发现这个程序虽然可以在当时的计算机上运行，而且结果正确，但是你理解不了。你可能发现这个程序尝试用卷积提取了很多边角特征，但这些特征非常多，而且很多你都没法理解，你更没法理解后面基于这些特征是怎样靠着magic number一样的数字做一些加减乘除就可以得到这么好的图像分类效果，其实这个维度是描述复杂性。

注意这里说的描述复杂性和通常说的算法复杂性不是一个意思，算法复杂性是执行时间和空间占用的规模，而我这里说的描述复杂性是指算法设计和描述的复杂程度。这个复杂性我很难找到一个非常合适的定义，比较接近的定义是柯氏复杂性，但也不完全准确。柯式复杂性的定义是生成一个给定字符串的图灵机算法的最小比特数，这个字符串可以泛化成各种具体任务，而柯式复杂性描述的是完成这个任务最少的逻辑量。当然这个定义不是特别准确，大家可以思考一下这个柯式复杂性可以把一个看起来很复杂的任务压缩到什么程度。我们平常写代码经常考虑代码复用、分层抽象、解耦合，而柯式复杂性描述的是你复用抽象解耦的极限。

实际上我们大多数程序的实际大小远没有达到柯氏复杂性的极限，（当然也没有必要，因为程序会变得非常难懂）。像很多非常大型的软件工程，像操作系统，数据库等，代码量巨大，但描述复杂性并不高，毕竟通过抽象就可以化简成很基础的几个算法写到教科书里。

而图像分类这一类问题之所以难，本质上是因为描述复杂性很高，需要打海量补丁来修各种边界条件。所以在我们的思想实验里面，大家拿着深度学习搞出来的标准答案在图灵机算法层面抄作业也无从下手。

从这个角度看，图灵机可以支持的算法集合是图灵可计算函数集合，甚至包含了模拟整个宇宙的算法在里面。但我们人类可以设计的算法，只是其中的低描述复杂性算法集合，因为人可以维护的描述复杂性是有限的。虽然我们通过抽象和解耦合可以做出非常庞大的软件工程，但描述复杂性是抽象和解耦合的极限，这些庞大的软件工程虽然规模巨大，但既然我们还能通过抽象和解耦合来维护，本质上也是一种“虚胖”，还是一些低描述复杂性算法。

可以说，描述复杂性的层级是根本性的，因为它是抽象和分层的极限对应的复杂性提升，而其他维度的涌现则仍然是通过抽象和分层在低描述复杂性图灵机算法集合里面扑腾。

低描述复杂性算法集合在整个图灵可计算函数中沧海一粟都算不上。而刚刚的思想实验里给的训练好的深度学习模型则是高描述复杂性算法，这也是为什么深度学习模型经常被诟病可解释性差的原因，本质上是人可以维护的复杂性太低，毕竟可解释的意思是可以用简单的几句话描述它的工作原理，如果我们用几千万句话解释一个深度学习模型是怎么处理各种边角料，从而实现高准确率的分类，大家会接受这种解释么？

从另一方面说，深度学习给了我们探索高复杂性算法的机会。而深度学习带来的一系列突破，也正是描述复杂性的层级提升涌现出的能力，毕竟高复杂性可是可以模拟整个宇宙的。那这种层级的突破又是怎么实现的呢？其实我们可以看到，层级的提升必然带来对低层级细节的放弃，例如物理上典型的热力学，当我们以热力学的视角来看一团气体时，我们只关注温度体积压力等指标，而放弃了每个气体分子的动量能量等指标（实际上温度放到微观尺度上都没法定义），在微观层面看一团气体，参数量是非常庞大的，在宏观层面看却非常简洁。回到图灵机算法和深度学习模型上，同样一个resnet模型，不同人训练出来的模型参数具体数值肯定差异巨大，翻译成算法来理解是截然不同的两个算法，但大家不会当做两个不同的深度学习模型，因为在这个过程中我们放弃了对于参数（也就是具体规则）的把控，只关注模型结构了。所以我们可以很轻松的训练一个参数规模成千上万的超大模型，这个模型在算法层面的描述复杂性是非常高的，但在模型层面的描述复杂性则很低，所以我们才能维护这样的模型。

所以进一步推广，深度学习模型的集合是图灵可计算函数的一个很小的子集，但也是非常庞大的，而且基本都属于高复杂性算法的集合内。但我们类可以探索的仍然是低描述复杂性模型子集，同样对于整个深度学习模型集合而言连沧海一粟都算不上。所谓深度学习的瓶颈可能更多是低复杂性深度学习模型的瓶颈，远远谈不上是深度学习的瓶颈。

铺垫了这么多，我们回到类脑，基于层级的思想，我们再来看看人类大脑。大脑的神经元和突触数量可能现在早已经被各种nlp的大模型碾压下去了，但通用智能的能力却碾压目前各种大模型。因为人类大脑的描述复杂性很可能还在好多个层级之上，是一个描述复杂性惊人的超级系统。

层级的观念之所以重要是因为，很多能力和概念在低层级是压根没有的，就像微观层面的气体分子没有温度的概念，涌现上去才有。图灵机也没有训练的概念，深度学习才有。那么大脑的学习到底对应的是深度学习的训练还是某种高层级才涌现出的概念？这个靠我们在深度学习模型上扑腾是没有用的，我们能扑腾的永远只是低复杂性模型的浅滩。

反过来说，这种对类脑智能的探索远不是规模的问题，规模可以虚胖，但描述复杂性是非常难突围的，每一层都很难。整个计算机领域因为图灵机建立了根基，经过这么多年发展，终于通过深度学习爬上了第一层台阶，而我们所期望的AGI，很可能是在复杂性大山的山顶。

当然，人类大脑站着这座山非常高的位置，而它最下面的台阶肯定不是深度学习，但高层级规律是有独立性的。无论走哪个坡面，层级到了那个高度，即使得不到类脑智能，我们也可以得到其他智能。

说了这么多，其实我所反对的是退回去重新找台阶的做法。深度学习不行，所以我们要退回去重新找一个台阶。甚至图灵机是不是也不行，我们要找新的计算模型。而SNN目前连第一级台阶也迈不上去，即使迈上去了，虽然可能可以看到和深度学习不同的风景，但能收获的也只是这个复杂性层级能带来的果子。

---
# 一个疯狂的想法
设计模型结构，训练参数，利用模型结构的低复杂度，产生参数的高维复杂度。
低到高，本质是一种升维，高维看低维的表示，就是一种抽象。

维度决定了一切

设计某种进化算法，训练模型结构，生成高复杂度（甚至人难以理解的模型结构（类似大脑）），然后给他输入输出，进行学习。

大脑，分层与解耦

亿万年的进化，为什么塑造了今天的大脑，塑造了今天的人类，如何进行逆向工程，还原到最初的起点，生命的起点。

时间上的连续，本质就是一种量变，一种涌现，智能的涌现，并不是在一个临界点，智能的产生是不能精细化，从这种宏观的角度来看，只能抓住这个模糊的、大概的过程。

还原论的思维模式，去思考，

  

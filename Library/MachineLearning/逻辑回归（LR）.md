# 是什么？
- 二分类算法
- 目标是预测一个二分类结果，即输出变量Y只有两个可能的取值（通常表示0或1）。它通过一种名为Sigmoid函数的转换，将线性回归的输出值（可以是任何实数）映射到0到1之间的概率值，从而实现分类的目的。


## 从线性回归到逻辑回归
线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数$\theta$，满足$Y=X\theta$ 。此时Y是连续的，所以是回归模型。如果想要将Y变成离散的，一个可行的办法是，我们对Y在做一次函数变换，变为$g(Y)$。映射到某个实数区间，再规定一个实数区间是类别A，另一个实数区间是类别B，这就得到了二元分类模型。
## 数学表达
函数g一般取为sigmoid函数：

$$
g(z)=\frac{1}{1+e^{−z}}
$$
![[Pasted image 20240809142121.png]]

导数性质：
$$
g^{'}(z)=g(z)(1-g(z))
$$
其中$z=x\theta$，这样得到了二元逻辑回归模型的一般形式：
$$
h_{\theta}(x) = \frac{1}{1+e^{-x\theta}}
$$
如果$h_{\theta}(x)>0.5$，即$x\theta>0$，则y为1，$h_{\theta}(x)$的值越小，分类为0的概率越高，反之，值越大分类为1的概率越高。

写出矩阵模式：
$$
h_{\theta}(X) = \frac{1}{1+e^{-X\theta}}
$$
输出为$m\times1$的维度。$X$为样本特征矩阵，为$m\times n$的维度。$\theta$为分类的模型系数，为$n \times 1$向量。

## 二元逻辑回归的损失函数
用最大似然函数法来推导损失函数。

有了Sigmoid函数，由于取值在[0,1]，我们就可以将$h_{\theta}(x)$视为类1的后验概率估计$P(y=1|x)$ 

假设样本输出是0或者1两类。那么有：
$$
　P(y=1|x,θ)=h_θ(x) 
$$
$$
　P(y=0|x,θ)=1−h_θ(x)
$$
把这两个式子写成一个式子，就是：
$$
P(y|x,\theta) =   
h_θ(x)^y(1−h_θ(x))^{1−y}
$$
然后进行对数似然函数最大化，函数取反即损失函数$J(\theta)$。交叉熵函数。

似然函数的代数表达式：
![[Pasted image 20240809151107.png]]

其中m为样本的个数。

我们让对数似然函数最大，也就是让损失函数最小。

![[Pasted image 20240809151200.png]]

![[Pasted image 20240809151210.png]]

然后就是梯度下降法优化这个损失函数了。



# 为什么？
- 为什么要用sigmoid函数

性质优秀，方便使用（Sigmoid函数是平滑的，而且任意阶可导，一阶二阶导数可以直接由函数值得到不用进行求导，这在实现中很实用）


- 损失函数如何推导出来的

将似然函数最大化取反得出


- 为什么不用误差平方和当代价函数呢

![[Pasted image 20240809151425.png]]

![[Pasted image 20240809151508.png]]

# 优缺点

## 优点：

1. **简单且易于实现**：
   - 逻辑回归的模型结构相对简单，易于理解和实现。对于小规模数据集和简单的分类问题，逻辑回归通常表现良好。

2. **结果具有可解释性**：
   - 逻辑回归的输出是概率值，这使得结果易于解释。系数的符号（正/负）可以直接表明特征与目标变量的正向或负向关系。

3. **计算效率高**：
   - 逻辑回归模型的训练和预测速度都很快，尤其是在特征数量适中的情况下。

4. **适用于二分类问题**：
   - 逻辑回归专门设计用于二分类问题。对于多分类问题，逻辑回归也可以通过 `One-vs-Rest` (OvR) 或 `Softmax` 函数扩展使用。

5. **无需大规模的数据集**：
   - 在数据量较小的情况下，逻辑回归仍能表现良好，不易过拟合。

6. **能够处理多重共线性**：
   - 虽然多重共线性会影响模型的解释性，但逻辑回归仍能在存在多重共线性的情况下运行，并输出合理的结果。

## 缺点：

1. **假设线性关系**：
   - 逻辑回归假设自变量与因变量之间存在线性关系。如果特征与目标变量之间的关系高度非线性，逻辑回归的表现会较差。

2. **容易受到异常值影响**：
   - 逻辑回归对异常值敏感。异常值可能会显著影响模型的系数，导致预测结果不准确。

3. **不适用于复杂非线性问题**：
   - 对于复杂的非线性分类问题，逻辑回归的表现可能不如决策树、支持向量机、神经网络等更复杂的模型。

4. **对特征工程的依赖较强**：
   - 逻辑回归在原始特征不能很好地表示数据的情况下表现不佳，因此需要较多的特征工程，如特征缩放、多项式特征生成等。

5. **对数据分布的要求**：
   - 逻辑回归要求输入数据独立同分布（IID），如果数据之间有强相关性，模型的表现可能受到影响。

6. **无法处理大量不相关特征**：
   - 当存在大量不相关或冗余特征时，逻辑回归模型容易出现过拟合问题。此时，可能需要使用正则化（如 L1、L2）来改善模型性能。

## 总结：

逻辑回归是一种功能强大且易于理解的分类算法，适用于大多数二分类问题，尤其是在特征和目标变量之间存在线性关系时。然而，对于更复杂的非线性问题或数据量较大且复杂的情况，可能需要考虑更复杂的模型。



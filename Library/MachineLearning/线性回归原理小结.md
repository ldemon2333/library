# 1. 线性回归的模型函数和损失函数
线性回归遇到的问题一般是这样的。我们有m个样本，每个样本对应n维特征和一个结果输出，如下：
$$
(x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)},y_1), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_m)
$$
我们的问题是，对于一个新的$(x_1^{(x)}, x_2^{(x)}, ...x_n^{(x)})$，它所对应的$y_x$是多少呢？如果这个问题里面的y是连续的，则是一个回归问题，否则是一个分类问题。

对于n维特征的样本数据，线性回归对应的模型是：

$h_\theta(x_1, x_2, ...x_n) = \theta_0 + \theta_{1}x_1 + ... + \theta_{n}x_{n}$，其中$\theta_i(i=0,1,2,...,n)$为模型参数，$x_i(i=0,1,2,...,n)$为每个样本的n个特征值。这个表示可以简化，增加一个特征$x_0=1$，这样$h_\theta(x_0, x_1, ...x_n) = \sum\limits_{i=0}^{n}\theta_{i}x_{i}$。

进一步用矩阵形式表达：
$$
h_\mathbf{\theta}(\mathbf{X}) = \mathbf{X\theta}
$$
其中，假设函数$h_\mathbf{\theta}(\mathbf{X})$为$m\times 1$的向量，$\theta$为$n\times 1$的向量，X为m$\times$n。

一般线性回归我们用均方差误差作为损失函数。损失函数的表示如下：
$$
J(\theta_0, \theta_1..., \theta_n) = \sum\limits_{i=1}^{m}(h_\theta(x_0^{(i)}, x_1^{(i)}, ...x_n^{(i)}) - y_i)^2
$$
进一步用矩阵形式表达损失函数：
$$
J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y})
$$

# 2.线性回归的算法
对应线性回归的损失函数$J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y})$，我们常用的有两种方法来求损失函数最小化。一种是梯度下降法，一种是最小二乘法。

# 3.线性回归的推广：多项式回归
这里写一个只有两个特征的二次方多项式回归的模型：
$$
h_\theta(x_1, x_2) = \theta_0 + \theta_{1}x_1 + \theta_{2}x_{2} + \theta_{3}x_1^{2} + \theta_{4}x_2^{2} + \theta_{5}x_{1}x_2
$$
我们令$x_0 = 1, x_1 = x_1, x_2 = x_2, x_3 =x_1^{2}, x_4 = x_2^{2}, x_5 =  x_{1}x_2$，这样我们就得到了下式：

$$
h_\theta(x_1, x_2) = \theta_0 + \theta_{1}x_1 + \theta_{2}x_{2} + \theta_{3}x_3 + \theta_{4}x_4 + \theta_{5}x_5
$$
可以发现，我们又重新回到了线性回归，这是一个五元线性回归。对于每个二元样本特征$(x_1,x_2)$，我们得到一个五元样本特征$(1, x_1, x_2, x_{1}^2, x_{2}^2, x_{1}x_2)$，通过这个改进的五元样本特征，我们重新把不是线性回归的函数变回线性回归。

# 4. 线性回归的推广：广义线性回归
对于特征y做推广，比如输出$\mathbf{Y}$不满足和$\mathbf{X}$的线性关系，但是$ln\mathbf{Y}$和$\mathbf{X}$满足线性关系，模型函数如下：
$$
ln\mathbf{Y} = \mathbf{X\theta}
$$
这样对于每个样本的输入y，我们用lny去对于，从而仍然可以用线性回归的算法去处理这个问题。
假设这个函数是单调可微函数$g(.)$，则一般化的广义线性回归形式是：
$$
\mathbf{g}(\mathbf{Y}) = \mathbf{X\theta}
$$
这个函数g称为联系函数。

# 5.线性回归的正则化
为了防止模型的过拟合，我们在建立线性模型的时候经常需要加入正则化项。一般有L1正则化和L2正则化。

线性回归的L1正则化通常称为Lasso回归，它和一般线性回归的区别是在损失函数上增加了一个L1正则化的项，L1正则化的项有一个常数系数$\alpha$来调节损失函数的均方差项和正则化项的权重，具体Lasso回归的损失函数表达式如下：
$$
J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y}) + \alpha\|\theta\|_1
$$
其中n为样本个数，$\alpha$为常数系数，需要进行调优。$\|\theta\|_1$为L1范数

Lasso回归可以使得一些特征的系数变小，甚至还使一些绝对值较小的系数直接变为0。增强模型的泛化能力。

线性回归的L2正则化通常称为Ridge回归，它和一般线性回归的区别是在损失函数上增加了一个L2正则化项，和Lasso回归的区别是Ridge回归的正则化项时L2范数。
$$
J(\mathbf\theta) = \frac{1}{2}(\mathbf{X\theta} - \mathbf{Y})^T(\mathbf{X\theta} - \mathbf{Y}) + \frac{1}{2}\alpha\|\theta\|_2^2
$$
Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，但和Lasso回归比，这会使得模型的特征留的特别多，模型解释性差。


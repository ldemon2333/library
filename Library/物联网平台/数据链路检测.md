模拟海量设备接入，查看中间件瓶颈

物联网平台的数据链路 **EMQX → DMP-MQTT → DMP-Kernel → RocketMQ → DMP-Data → ClickHouse**
![[架构图 1.png]]

# EMQX
docker stats 7638
![[Pasted image 20250209111248.png]]

连接日志：
2025-02-09T10:49:26.871448+08:00 [warning] line: 411, mfa: emqx_alarm:do_actions/3, msg: alarm_is_deactivated, name: <<"conn_congestion/inner:cloud-os-connect-mqtt-1/eea5a03930724c379f61b0961eae2942">>
2025-02-09T10:50:48.564815+08:00 [warning] line: 405, message: <<"connection congested: #{buffer => 4096,clientid => <<\"inner:cloud-os-connect-mqtt-1\">>,conn_state => connected,connected_at => 1738817012137,high_msgq_watermark => 8192,high_watermark => 1048576,memory => 1740856,message_queue_len => 0,peername => <<\"192.168.6.17:31832\">>,pid => <<\"<0.20223.77>\">>,proto_name => <<\"MQTT\">>,proto_ver => 4,recbuf => 390488,recv_cnt => 53702747,recv_oct => 2096806"...>>, mfa: emqx_alarm:do_actions/3, msg: alarm_is_activated, name: <<"conn_congestion/inner:cloud-os-connect-mqtt-1/eea5a03930724c379f61b0961eae2942">>
![[Pasted image 20250209105256.png]]

## **第一条日志（10:49:26）**

```log
2025-02-09T10:49:26.871448+08:00 [warning] line: 411, mfa: emqx_alarm:do_actions/3, msg: alarm_is_deactivated, name: <<"conn_congestion/inner:cloud-os-connect-mqtt-1/eea5a03930724c379f61b0961eae2942">>
```

**解析：**

- **时间戳**：`2025-02-09T10:49:26.871448+08:00`
- **日志级别**：`[warning]`（警告）
- **行号**：`line: 411`
- **触发模块**：`mfa: emqx_alarm:do_actions/3`（EMQX 的报警处理逻辑）
- **消息**：`alarm_is_deactivated`（**拥塞警报已解除**）
- **警报名称**：`conn_congestion/inner:cloud-os-connect-mqtt-1/eea5a03930724c379f61b0961eae2942`

 **含义**： 在 **10:49:26**，**之前的“连接拥塞”问题已解除**，说明 EMQX 认为这个 MQTT 连接恢复了正常。

---

## **第二条日志（10:50:48）**

```log
2025-02-09T10:50:48.564815+08:00 [warning] line: 405, message: <<"connection congested: #{buffer => 4096,clientid => <<\"inner:cloud-os-connect-mqtt-1\">>,conn_state => connected,connected_at => 1738817012137,high_msgq_watermark => 8192,high_watermark => 1048576,memory => 1740856,message_queue_len => 0,peername => <<\"192.168.6.17:31832\">>,pid => <<\"<0.20223.77>\">>,proto_name => <<\"MQTT\">>,proto_ver => 4,recbuf => 390488,recv_cnt => 53702747,recv_oct => 2096806"...>>, mfa: emqx_alarm:do_actions/3, msg: alarm_is_activated, name: <<"conn_congestion/inner:cloud-os-connect-mqtt-1/eea5a03930724c379f61b0961eae2942">>
```

**解析：**

- **时间戳**：`2025-02-09T10:50:48.564815+08:00`
- **日志级别**：`[warning]`（警告）
- **行号**：`line: 405`
- **消息**：
    - **"connection congested"**：表示该连接发生了拥塞（即消息处理速度跟不上流量）。
    - **clientid**：`inner:cloud-os-connect-mqtt-1`（MQTT 设备 ID）。
    - **conn_state**：`connected`（仍然连接中）。
    - **high_msgq_watermark**：`8192`（消息队列的高水位线）。
    - **high_watermark**：`1048576`（连接的流量控制阈值）。
    - **memory**：`1740856`（占用的内存，约 1.7MB）。
    - **message_queue_len**：`0`（当前消息队列长度是 0，表明消息未积压）。
    - **peername**：`192.168.6.17:31832`（客户端 IP 和端口）。
    - **recv_cnt**：`53702747`（接收到的 MQTT 消息数）。
    - **recv_oct**：`2096806`（接收的总字节数）。
- **触发模块**：`mfa: emqx_alarm:do_actions/3`
- **消息**：`alarm_is_activated`（**拥塞警报被触发**）。
- **警报名称**：`conn_congestion/inner:cloud-os-connect-mqtt-1/eea5a03930724c379f61b0961eae2942`

**含义**：

- 这条日志表明，**1 分钟 22 秒之后（10:50:48）连接又出现了拥塞**，导致 EMQX 再次触发 "连接拥塞" 警报。


## **总结**

1. **10:49:26** - EMQX 解除 "连接拥塞" 警报（连接恢复正常）。
2. **10:50:48** - EMQX **再次触发** "连接拥塞" 警报（连接又出现问题）。
3. 说明该设备 **inner:cloud-os-connect-mqtt-1** 的 MQTT 连接出现 **反复的拥塞问题**，可能是**客户端发送过快、网络不稳定或服务器处理能力受限**。


# dmp-mqtt
2025-02-09T11:16:36.153+0800 INFO  [pool-1-thread-2] i.c.o.c.c.thread.TaskExecutor.monitor:87 - [task executor] monitoring... name=mqttMessageProcessor, core=9, max=36, keepAlive=180ms, activeThreadNum=0, taskQueue=0/5000, rejectTaskNum=0
2025-02-09T11:16:54.394+0800 INFO  [pool-1-thread-1] i.c.o.c.component.log.TraceLogCache.refresh:59 - [TraceLog] fetch up to date config by redis, properties={"name":"cloud-os-connect-mqtt","enable":true,"identify":[]}
2025-02-09T11:17:29.531+0800 INFO  [pool-1-thread-2] i.c.o.c.c.http.HttpComponent.lambda$init$0:77 - [httpComponent] connection pool monitoring... leased=0, pending=0, available=0, max=5
2025-02-09T11:17:29.565+0800 INFO  [thread-pool-util-monitor-0] i.c.os.common.utils.ThreadPoolUtil.lambda$static$2:39 -
all thread pools(count: 2) status:
mqttDevice-offline-%d, java.util.concurrent.ScheduledThreadPoolExecutor@63387340[Running, pool size = 1, active threads = 0, queued tasks = 1, completed tasks = 1104]
thread-pool-util-monitor-%d, java.util.concurrent.ScheduledThreadPoolExecutor@6707fbff[Running, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 33152]

2025-02-09T11:17:36.153+0800 INFO  [pool-1-thread-1] i.c.o.c.c.thread.TaskExecutor.monitor:87 - [task executor] monitoring... name=mqttMessageProcessor, core=9, max=36, keepAlive=180ms, activeThreadNum=0, taskQueue=0/5000, rejectTaskNum=0
2025-02-09T11:17:54.394+0800 INFO  [pool-1-thread-2] i.c.o.c.component.log.TraceLogCache.refresh:59 - [TraceLog] fetch up to date config by redis, properties={"name":"cloud-os-connect-mqtt","enable":true,"identify":[]}
2025-02-09T11:18:29.531+0800 INFO  [pool-1-thread-1] i.c.o.c.c.http.HttpComponent.lambda$init$0:77 - [httpComponent] connection pool monitoring... leased=0, pending=0, available=0, max=5
2025-02-09T11:18:29.565+0800 INFO  [thread-pool-util-monitor-0] i.c.os.common.utils.ThreadPoolUtil.lambda$static$2:39 -
all thread pools(count: 2) status:
mqttDevice-offline-%d, java.util.concurrent.ScheduledThreadPoolExecutor@63387340[Running, pool size = 1, active threads = 0, queued tasks = 1, completed tasks = 1104]
thread-pool-util-monitor-%d, java.util.concurrent.ScheduledThreadPoolExecutor@6707fbff[Running, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 33153]

2025-02-09T11:18:36.153+0800 INFO  [pool-1-thread-2] i.c.o.c.c.thread.TaskExecutor.monitor:87 - [task executor] monitoring... name=mqttMessageProcessor, core=9, max=36, keepAlive=180ms, activeThreadNum=9, taskQueue=846/5000, rejectTaskNum=0
2025-02-09T11:18:54.396+0800 INFO  [pool-1-thread-2] i.c.o.c.component.log.TraceLogCache.refresh:59 - [TraceLog] fetch up to date config by redis, properties={"name":"cloud-os-connect-mqtt","enable":true,"identify":[]}
2025-02-09T11:19:29.532+0800 INFO  [pool-1-thread-1] i.c.o.c.c.http.HttpComponent.lambda$init$0:77 - [httpComponent] connection pool monitoring... leased=0, pending=0, available=0, max=5
2025-02-09T11:19:29.564+0800 INFO  [thread-pool-util-monitor-0] i.c.os.common.utils.ThreadPoolUtil.lambda$static$2:39 -
all thread pools(count: 2) status:
mqttDevice-offline-%d, java.util.concurrent.ScheduledThreadPoolExecutor@63387340[Running, pool size = 1, active threads = 0, queued tasks = 1, completed tasks = 1104]
thread-pool-util-monitor-%d, java.util.concurrent.ScheduledThreadPoolExecutor@6707fbff[Running, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 33154]

2025-02-09T11:19:36.153+0800 INFO  [pool-1-thread-2] i.c.o.c.c.thread.TaskExecutor.monitor:87 - [task executor] monitoring... name=mqttMessageProcessor, core=9, max=36, keepAlive=180ms, activeThreadNum=36, taskQueue=4990/5000, rejectTaskNum=34372


这段日志是 `dmp-mqtt` 服务的系统运行监控日志


## **日志字段解析**


```
2025-02-09T11:18:36.153+0800 INFO  [pool-1-thread-2] i.c.o.c.c.thread.TaskExecutor.monitor:87 - [task executor] monitoring... name=mqttMessageProcessor, core=9, max=36, keepAlive=180ms, activeThreadNum=9, taskQueue=846/5000, rejectTaskNum=0
```

- **`name=mqttMessageProcessor`** → MQTT 消息处理任务执行器
- **`core=9`** → 线程池的核心线程数是 9
- **`max=36`** → 线程池最大线程数是 36
- **`keepAlive=180ms`** → 线程的存活时间是 180ms（可能是短生命周期的任务）
- **`activeThreadNum=9`** → 当前正在运行的线程数为 9
- **`taskQueue=846/5000`** → 任务队列中有 **846** 个任务，最大容量是 **5000**
- **`rejectTaskNum=0`** → 没有被丢弃的任务（即所有任务都在排队或执行中）

当 `taskQueue` 逼近 `5000`，而 `activeThreadNum` 达到 `max=36` 时，说明服务器的 MQTT 处理能力接近极限：

```
2025-02-09T11:19:36.153+0800 INFO  [pool-1-thread-2] i.c.o.c.c.thread.TaskExecutor.monitor:87 - [task executor] monitoring... name=mqttMessageProcessor, core=9, max=36, keepAlive=180ms, activeThreadNum=36, taskQueue=4990/5000, rejectTaskNum=34372
```

**⚠️ 问题点：**

- **`taskQueue=4990/5000`** → 任务队列接近满载
- **`activeThreadNum=36`** → 线程池已满载（36 个线程全在运行）
- **`rejectTaskNum=34372`** → **大量任务被丢弃（34,372 个）**，可能导致 MQTT 消息处理延迟或丢失

问题

1. **MQTT 任务队列接近满载**
    - `taskQueue=4990/5000`，`activeThreadNum=36` → 线程池已满载，任务几乎溢出
    - `rejectTaskNum=34372` → 超过 34,000 个任务被丢弃，**可能导致 MQTT 消息处理延迟或丢失**


当前 **MQTT 任务处理** 负载 **极高**，线程池接近崩溃边缘，建议 **优化线程池配置** 并 **监控服务器负载** 以防止 MQTT 消息丢失。


# dmp-kernel
正常

# RocketMQ
正常

# 后续正常

# 步进模式
单步和多步

如果我们想给单步模式的模块输入 `shape = [T, N, *]` 的序列数据，通常需要手动做一个时间上的循环，将数据拆成 `T` 个 `shape = [N, *]` 的数据并逐步输入进去。让我们新建一层IF神经元，设置为单步模式，将数据逐步输入并得到输出：

### `torch.unsqueeze` 是什么？

`unsqueeze` 用于在张量的指定位置插入一个维度（维度大小为 1）。

若我们给与一个新的输入，则应该先清除神经元之前的状态，让其恢复到初始化状态，可以通过调用模块的 `self.reset()` 函数实现：

若网络使用了有状态的模块，在训练和推理时，务必在处理完毕一个batch的数据后进行重置：

```
from spikingjelly.activation_based import functional
# ...
for x, label in tqdm(train_data_loader):
    # ...
    optimizer.zero_grad()
    y = net(x)
    loss = criterion(y, label)
    loss.backward()
    optimizer.step()

    functional.reset_net(net)
    # Never forget to reset the network!
```

# 传播模式
若一个网络全部由单步模块构成，则整个网络的计算顺序是按照逐步传播(step-by-step)的模式进行，例如：
```
for t in range(T):
    x = x_seq[t]
    y = net(x)
    y_seq_step_by_step.append(y.unsqueeze(0))

y_seq_step_by_step = torch.cat(y_seq_step_by_step, 0)
```

如果网络全部由多步模块构成，则整个网络的计算顺序是按照逐层传播(layer-by-layer)的模式进行，例如：
```
import torch
import torch.nn as nn
from spikingjelly.activation_based import neuron, functional, layer
T = 4
N = 2
C = 8
x_seq = torch.rand([T, N, C]) * 64.

net = nn.Sequential(
    layer.Linear(C, 4),
    neuron.IFNode(),
    layer.Linear(4, 2),
    neuron.IFNode()
)

functional.set_step_mode(net, step_mode='m')
with torch.no_grad():
    y_seq_layer_by_layer = x_seq
    for i in range(net.__len__()):
        y_seq_layer_by_layer = net[i](y_seq_layer_by_layer)
```

如果网络全部由多步模块构成，则整个网络的计算顺序是按照逐层传播(layer-by-layer)的模式进行，例如：
```
import torch
import torch.nn as nn
from spikingjelly.activation_based import neuron, functional, layer
T = 4
N = 2
C = 8
x_seq = torch.rand([T, N, C]) * 64.

net = nn.Sequential(
    layer.Linear(C, 4),
    neuron.IFNode(),
    layer.Linear(4, 2),
    neuron.IFNode()
)

functional.set_step_mode(net, step_mode='m')
with torch.no_grad():
    y_seq_layer_by_layer = x_seq
    for i in range(net.__len__()):
        y_seq_layer_by_layer = net[i](y_seq_layer_by_layer)
```

逐步传播和逐层传播，实际上只是计算顺序不同，它们的计算结果是完全相同的：

下面的图片展示了逐步传播构建计算图的顺序：
![[Pasted image 20250424121621.png]]

下面的图片展示了逐层传播构建计算图的顺序：
![[Pasted image 20250424121630.png]]

SNN的计算图有2个维度，分别是时间步数和网络深度，网络的传播实际上就是生成完整计算图的过程，正如上面的2张图片所示。实际上，逐步传播是深度优先遍历，而逐层传播是广度优先遍历。

尽管两者区别仅在于计算顺序，但计算速度和内存消耗上会略有区别。

- 在使用梯度替代法训练时，通常推荐使用逐层传播。在正确构建网络的情况下，逐层传播的并行度更大，速度更快
    
- 在内存受限时使用逐步传播，例如ANN2SNN任务中需要用到非常大的 `T`。因为在逐层传播模式下，对无状态的层而言，真正的 batch size 是 `TN` 而不是 `N` (参见下一个教程)，当 `T` 太大时内存消耗极大


# 包装器
SpikingJelly 主要提供了如下几种包装器：
- 函数风格的 `multi_step_forward` 和模块风格的 `MultiStepContainer`
    
- 函数风格的 `seq_to_ann_forward` 和模块风格的 `SeqToANNContainer`
    
- 对单步模块进行包装以进行单步/多步传播的 `StepModeContainer`

对于无状态的ANN网络层，例如 [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d "（在 PyTorch v2.6）")，其本身要求输入数据的 `shape = [N, *]`，若用于多步模式，则可以用多步的包装器进行包装：
```
import torch
import torch.nn as nn
from spikingjelly.activation_based import functional, layer

with torch.no_grad():
    T = 4
    N = 1
    C = 3
    H = 8
    W = 8
    x_seq = torch.rand([T, N, C, H, W])

    conv = nn.Conv2d(C, 8, kernel_size=3, padding=1, bias=False)
    bn = nn.BatchNorm2d(8)

    y_seq = functional.multi_step_forward(x_seq, (conv, bn))
    # y_seq.shape = [T, N, 8, H, W]

    net = layer.MultiStepContainer(conv, bn)
    z_seq = net(x_seq)
    # z_seq.shape = [T, N, 8, H, W]

    # z_seq is identical to y_seq
```

但是ANN的网络层本身是无状态的，不存在前序依赖，没有必要在时间上串行的计算，可以使用函数风格的 `seq_to_ann_forward` 或模块风格的 `SeqToANNContainer` 进行包装。`seq_to_ann_forward` 将 `shape = [T, N, *]` 的数据首先变换为 `shape = [TN, *]`，再送入无状态的网络层进行计算，输出的结果会被重新变换为 `shape = [T, N, *]`。不同时刻的数据是并行计算的，因而速度更快：

```
import torch
import torch.nn as nn
from spikingjelly.activation_based import functional, layer

with torch.no_grad():
    T = 4
    N = 1
    C = 3
    H = 8
    W = 8
    x_seq = torch.rand([T, N, C, H, W])

    conv = nn.Conv2d(C, 8, kernel_size=3, padding=1, bias=False)
    bn = nn.BatchNorm2d(8)

    y_seq = functional.multi_step_forward(x_seq, (conv, bn))
    # y_seq.shape = [T, N, 8, H, W]

    net = layer.MultiStepContainer(conv, bn)
    z_seq = net(x_seq)
    # z_seq.shape = [T, N, 8, H, W]

    # z_seq is identical to y_seq

    p_seq = functional.seq_to_ann_forward(x_seq, (conv, bn))
    # p_seq.shape = [T, N, 8, H, W]

    net = layer.SeqToANNContainer(conv, bn)
    q_seq = net(x_seq)
    # q_seq.shape = [T, N, 8, H, W]

    # q_seq is identical to p_seq, and also identical to y_seq and z_seq
```

其中 X[t] 是外源输入，例如电压增量；为了避免混淆，我们使用 H[t] 表示神经元充电后、释放脉冲前的膜电位；V[t] 是神经元释放脉冲后的膜电位；f(V[t−1],X[t]) 是神经元的状态更新方程，不同的神经元，区别就在于更新方程不同。
![[Pasted image 20250424131834.png]]
## 自定义神经元
如前所述，SpikingJelly使用充电、放电、重置三个方程来描述脉冲神经元，在 [`BaseNode`](https://spikingjelly.readthedocs.io/zh-cn/latest/sub_module/spikingjelly.activation_based.neuron.html#spikingjelly.activation_based.neuron.BaseNode "spikingjelly.activation_based.neuron.BaseNode") 中可以找到对应的代码，单步模式下的前向传播 `single_step_forward` 函数即是由这3个过程组成：
```
# spikingjelly.activation_based.neuron.BaseNode
def single_step_forward(self, x: torch.Tensor):
    self.neuronal_charge(x)
    spike = self.neuronal_fire()
    self.neuronal_reset(spike)
    return spike
```

其中 `neuronal_fire` 和 `neuronal_reset` 对绝大多数神经元都是相同的，因而在 `BaseNode` 中就已经定义了。不同的神经元主要是构造函数和充电方程 `neuronal_charge` 不同。因此，若想实现新的神经元，则只需要更改构造函数和充电方程即可。

假设我们构造一种平方积分发放神经元，其充电方程为：
实现方式如下：

使用平方积分发放神经元进行单步或多步传播：
```
import torch
from spikingjelly.activation_based import neuron

class SquareIFNode(neuron.BaseNode):

    def neuronal_charge(self, x: torch.Tensor):
        self.v = self.v + x ** 2

sif_layer = SquareIFNode()

T = 4
N = 1
x_seq = torch.rand([T, N])
print(f'x_seq={x_seq}')

for t in range(T):
    yt = sif_layer(x_seq[t])
    print(f'sif_layer.v[{t}]={sif_layer.v}')

sif_layer.reset()
sif_layer.step_mode = 'm'
y_seq = sif_layer(x_seq)
print(f'y_seq={y_seq}')
sif_layer.reset()
```

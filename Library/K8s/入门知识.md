**这，就是K8S要做的事情：自动化运维管理Docker（容器化）程序**。

![[Pasted image 20250402194134.png]]
分为 Master Node 和 Worker Node

首先来看 Master Node 都有哪些组件：
- API Server：k8s 的请求入口服务，实际可以部署多个实例，以增大请求吞吐。
- etcd：k8s 的存储服务，etcd 存储了k8s 的关键配置和用户配置，k8s 中仅 API server 才具备读写权限，其他组件必须通过 API server 的接口才能读写数据。

Worker Node：
- Kubelet。worker Node 的监视器，以及与 Master Node 的通讯器。
- kube-proxy：网络代理，负责 node 在 k8s 的网络通讯，以及对外部网络流量的负载均衡。
- Logging Layer。K8S的监控状态收集器。私以为称呼为Monitor可能更合适？Logging Layer负责采集Node上所有服务的CPU、内存、磁盘、网络等监控项信息。
- Add-Ons。K8S管理运维Worker Node的插件组件。有些文章认为Worker Node只有三大组件，不包含Add-On，但笔者认为K8S系统提供了Add-On机制，让用户可以扩展更多定制化功能，是很不错的亮点。


总结来看，K8S的 Master Node具备：请求入口管理（API Server），Worker Node调度（Scheduler），监控和自动调节（Controller Manager），以及存储功能（etcd）；而K8S的Worker Node具备：状态和监控收集（Kubelet），网络和负载均衡（Kube-Proxy）、保障容器化运行环境（Container Runtime）、以及定制化功能（Add-Ons）。


# Pod
Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。Pod就是 K8S 中一个服务的闭包。==Pod 可以被理解成一群可以共享网络、存储和计算资源的容器化服务的集合==。

笔者总结Pod如下图，可以看到：同一个Pod之间的Container可以通过localhost互相访问，并且可以挂载Pod内所有的数据卷；但是不同的Pod之间的Container不能用localhost访问，也不能挂载其他Pod的数据卷。

![[Pasted image 20250402195056.png]]

k8s 中所有的对象都通过 yaml 来表示，最简单的 pod 的 yaml：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-ctr
    image: polinux/stress
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
    volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
```
- `apiVersion`记录K8S的API Server版本，现在看到的都是`v1`，用户不用管。
- `kind`记录该yaml的对象，比如这是一份Pod的yaml配置文件，那么值内容就是`Pod`。
- `metadata`记录了Pod自身的元数据，比如这个Pod的名字、这个Pod属于哪个namespace（命名空间的概念，后文会详述，暂时理解为“同一个命名空间内的对象互相可见”）。
- `spec`记录了Pod内部所有的资源的详细信息，看懂这个很重要：

- `containers`记录了Pod内的容器信息，`containers`包括了：`name`容器名，`image`容器的镜像地址，`resources`容器需要的CPU、内存、GPU等资源，`command`容器的入口命令，`args`容器的入口参数，`volumeMounts`容器要挂载的Pod数据卷等。可以看到，**上述这些信息都是启动容器的必要和必需的信息**。
- `volumes`记录了Pod内的数据卷信息，后文会详细介绍Pod的数据卷。

## Volume 数据卷
K8S支持很多类型的volume数据卷挂载，前文就“如何理解volume”提到：“**需要手动mount的磁盘**”，此外，有一点可以帮助理解：**数据卷volume是Pod内部的磁盘资源**。

# Deployment
**Deployment的作用是管理和控制Pod和ReplicaSet，管控它们运行在用户期望的状态中**。哎，打个形象的比喻，**Deployment就是包工头**，主要负责监督底下的工人Pod干活，确保每时每刻有用户要求数量的Pod在工作。如果一旦发现某个工人Pod不行了，就赶紧新拉一个Pod过来替换它。

新的问题又来了：那什么是ReplicaSets呢？
ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。 因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。

再来翻译下：ReplicaSet的作用就是管理和控制Pod，管控他们好好干活。但是，ReplicaSet受控于Deployment。形象来说，**ReplicaSet就是总包工头手下的小包工头**。

笔者总结得到下面这幅图，希望能帮助理解：
![[Pasted image 20250402195809.png]]

新的问题又来了：如果都是为了管控Pod好好干活，为什么要设置Deployment和ReplicaSet两个层级呢，直接让Deployment来管理不可以吗？

引入这样子的层级管理制度是为了实现前后两个版本平滑升级、平滑回滚等高级功能。笔者画了一副Deployment平滑升级（其中一种策略，实际可选有很多策略，本文不展开），或许可以帮助理解。如下：

![[Pasted image 20250402195841.png]]

因此，建议 k8s 用户，使用 deployment 来部署服务，而不要直接使用 ReplicaSet。当Deployment被部署的时候，K8S会自动生成要求的ReplicaSet和Pod。

>This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section.  
这实际上意味着您可能永远不需要操作ReplicaSet对象：直接使用Deployments并在规范部分定义应用程序。

补充说明：在K8S中还有一个对象 --- **ReplicationController（简称RC）**
>_ReplicationController_ 确保在任何时候都有特定数量的 Pod 副本处于运行状态。 换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。

“ReplicationController是ReplicaSet的前身”，官方也推荐用Deployment取代ReplicationController来部署服务。

# Service, Ingress 和 lstio
前文介绍的Deployment、ReplicationController和ReplicaSet主要管控Pod程序服务；那么，Service和Ingress则负责管控Pod网络服务。

Service：
> 将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。
> 使用 Kubernetes，您无需修改应用程序即可使用不熟悉的服务发现机制。 Kubernetes 为 Pods 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名， 并且可以在它们之间进行负载均衡。

翻译下：K8S中的服务（Service）并不是我们常说的“服务”的含义，而更像是网关层，是若干个Pod的流量入口、流量均衡器。

那么，为什么要Service呢？

>Kubernetes [Pod](https://link.zhihu.com/?target=https%3A//kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 是有生命周期的。 它们可以被创建，而且销毁之后不会再启动。 如果您使用 [Deployment](https://link.zhihu.com/?target=https%3A//kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/) 来运行您的应用程序，则它可以动态创建和销毁 Pod。  
每个 Pod 都有自己的 IP 地址，但是在 Deployment 中，在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同。  
这导致了一个问题： 如果一组 Pod（称为“后端”）为群集内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用工作量的后端部分？

此外，笔者推荐[k8s外网如何访问业务应用](https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/50b930fa7ca3)对于Service的介绍，不过对于新手而言，推荐阅读前半部分对于service的介绍即可，后半部分就太复杂了。我这里做了简单的总结：

**Service是K8S服务的核心，屏蔽了服务细节，统一对外暴露服务接口，真正做到了“微服务”**。举个例子，我们的一个服务A，部署了3个备份，也就是3个Pod；对于用户来说，只需要关注一个Service的入口就可以，而不需要操心究竟应该请求哪一个Pod。优势非常明显：**一方面外部用户不需要感知因为Pod上服务的意外崩溃、K8S重新拉起Pod而造成的IP变更，外部用户也不需要感知因升级、变更服务带来的Pod替换而造成的IP变化，另一方面，Service还可以做流量负载均衡**。

K8S的Service有3种类型（最新出了一种新的ClusterName，可以直接提供DNS Name），总结如下：

- **ClusterIP**：只能提供K8S集群内可以访问的IP和Port。
- **NodePort**：在上面的基础上，提供K8S集群外部可以访问的Port，IP则是集群内任意一个NodeIP。
- **LoadBlancer**：在上面的基础上，提供K8S集群外可以访问的IP和Port。需要注意的是，在LoadBlancer中可以设置关闭NodePort。还有一点需要注意，LoadBlanacer依赖集群底座的能力，并不是所有的K8S都能使用LoadBlancer。

**因此，我们可以看到，Service实际上已经能够满足基础通讯要求，包括K8S集群内部和内外。那么，还需要Ingress和Istio吗？**

官方文档中的对 Ingress 解释是：
>Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。 Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。

翻译一下：Ingress是整个K8S集群的接入层，复杂集群内外通讯。

这里对于Ingress和Istio不做深入介绍，感兴趣的读者欢迎关注后续文章。Ingress是官方团队推出的K8S集群接入层，Istio则是社区开发和运维的适配K8S的接入层组件。在实际项目中，我们往往需要功能更为复杂的网关层，比如流量转发，熔断等。而Service提供的功能太过简单，流量只能导入到单点或者有限服务上，无法满足实际生产需求，因此Ingress和Istio应运而生。其中，Istio又发展最好，功能和生态最丰富，是目前主流采用的网关组件。




![[Pasted image 20250224211822.png]]

_Pods_ are the smallest deployable units of computing that you can create and manage in Kubernetes.

A _Pod_ (as in a pod of whales or pea pod) is a group of one or more [containers](https://kubernetes.io/docs/concepts/containers/), with shared storage and network resources, and a specification for how to run the containers.

# Kubernetes 集群
**Kubernetes 协调一个高可用计算机集群，每个计算机作为独立单元互相连接工作。**

一个 Kubernetes 集群包含两种类型的资源：

- **控制面**调度整个集群
- **节点（Nodes）负责运行应用

**控制面负责管理整个集群。** 控制面协调集群中的所有活动，例如调度应用、维护应用的所需状态、应用扩容以及推出新的更新。

**节点是一个虚拟机或者物理机，它在 Kubernetes 集群中充当工作机器的角色。** 每个节点都有 Kubelet，它管理节点而且是节点与控制面通信的代理。 节点还应该具有用于​​处理容器操作的工具，例如 [containerd](https://containerd.io/docs/) 或 [CRI-O](https://cri-o.io/#what-is-cri-o)。

Minikube 是一种轻量级的 Kubernetes 实现，可在本地计算机上创建 VM 并部署仅包含一个节点的简单集群。 Minikube 可用于 Linux、macOS 和 Windows 系统。Minikube CLI 提供了用于引导集群工作的多种操作， 包括启动、停止、查看状态和删除。


Kubernetes [**Pod**](https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/) 是由一个或多个为了管理和联网而绑定在一起的容器构成的组。本教程中的 Pod 只有一个容器。 Kubernetes [**Deployment**](https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/) 检查 Pod 的健康状况，并在 Pod 中的容
器终止的情况下重新启动新的容器。 Deployment 是管理 Pod 创建和扩展的推荐方法。

Once you have a running Kubernetes cluster, you can deploy your containerized applications on top of it. To do so, you create a Kubernetes **Deployment**. The Deployment instructs Kubernetes how to create and update instances of your application. Once you've created a Deployment, the Kubernetes control plane schedules the application instances included in that Deployment to run on individual Nodes in the cluster.

Once the application instances are created, a Kubernetes Deployment controller continuously monitors those instances. If the Node hosting an instance goes down or is deleted, the Deployment controller replaces the instance with an instance on another Node in the cluster. **This provides a self-healing mechanism to address machine failure or maintenance.**

![[Pasted image 20250224230113.png]]
# Deploying your first app on Kubernetes
You can create and manage a Deployment by using the Kubernetes command line interface, **kubectl**.

When you create a Deployment, you'll need to specify the container image for your application and the number of replicas that you want to run. You can change that information later by updating your Deployment;

For your first Deployment, you'll use a hello-node application packaged in a Docker container that uses NGINX to echo back all the requests.

## kubectl basics
The common format of a kubectl command is: `kubectl action resource`

To view the nodes in the cluster, run the **`kubectl get nodes`** command.

You see the available nodes. Later, Kubernetes will choose where to deploy our application based on Node available resources.

## Deploy an app
Let’s deploy our first app on Kubernetes with the `kubectl create deployment` command. We need to provide the deployment name and app image location (include the full repository url for images hosted outside Docker Hub).

**`kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1`**

You just deployed your first application by creating a deployment. This performed a few things for you:
- searched for a suitable node where an instance of the application could be run (we have only 1 available node)
- scheduled the application to run on that Node
- configured the cluster to reschedule the instance on a new Node when needed

## View the app
Pods that are running inside Kubernetes are running on a private, isolated network. By default they are visible from other pods and services within the same Kubernetes cluster, but not outside that network. When we use `kubectl`, we're interacting through an API endpoint to communicate with our application.

**You need to open a second terminal window to run the proxy.**

**`kubectl proxy`**

The `kubectl proxy` command can create a proxy that will forward communications into the cluster-wide, private network. The proxy can be terminated by pressing control-C and won't show any output while it's running.

We now have a connection between our host (the terminal) and the Kubernetes cluster. The proxy enables direct access to the API from these terminals.

You can see all those APIs hosted through the proxy endpoint. For example, we can query the version directly through the API using the `curl` command:

**`curl http://localhost:8001/version`**

The API server will automatically create an endpoint for each pod, based on the pod name, that is also accessible through the proxy.

First we need to get the Pod name, and we'll store it in the environment variable POD_NAME:

**`export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')`**  
**`echo Name of the Pod: $POD_NAME`**

You can access the Pod through the proxied API, by running:

**`curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME:8080/proxy/`**

In order for the new Deployment to be accessible without using the proxy, a Service is required which will be explained in [Module 4](https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/).

Once you're ready, move on to [Viewing Pods and Nodes](https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/ "Viewing Pods and Nodes").



# Viewing Pods and Nodes
A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker), and some shared resources for those containers. Those resources include:
- Shared storage, as Volumes
- Networking, as a unique cluster IP address
- Information about how to run each container, such as the container image version or specific ports to use

Pods are the atomic unit on the Kubernetes platform. When we create a Deployment on Kubernetes, that Deployment creates Pods with containers inside them (as opposed to creating containers directly). Each Pod is tied to the Node where it is scheduled, and remains there until termination (according to restart policy) or deletion. In case of a Node failure, identical Pods are scheduled on other available Nodes in the cluster.

![[Pasted image 20250225093930.png]]
## Nodes
A Pod always runs on a **Node**. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the control plane. A Node can have multiple pods, and the Kubernetes control plane automatically handles scheduling the pods across the Nodes in the cluster. The control plane's automatic scheduling takes into account the available resources on each Node.

Every Kubernetes Node runs at least:
- Kubelet, a process responsible for communication between the Kubernetes control plane and the Node; it manages the Pods and the containers running on a machine.
- A container runtime (like Docker) responsible for pulling the container image from a registry, unpacking the container, and running the application.
![[Pasted image 20250225094307.png]]
A node is a worker machine in K8s and may be a VM or physical machine, depending on the cluster. Multiple Pods can run on one Node.

## Troubleshooting with kubectl
**`kubectl describe pods`**

We see here details about the Pod’s container: IP address, the ports used and a list of events related to the lifecycle of the Pod.

## Show the app in the terminal

## View the container logs
Anything that the application would normally send to standard output becomes logs for the container within the Pod. We can retrieve these logs using the `kubectl logs` command:

**kubectl logs "$POD_NAME"**

_**Note:** We don't need to specify the container name, because we only have one container inside the pod._

## Executing command on the container
We can execute commands directly on the container once the Pod is up and running. For this, we use the `exec` subcommand and use the name of the Pod as a parameter. Let’s list the environment variables:
`kubectl exec "$POD_NAME" -- env`

Next let’s start a bash session in the Pod’s container:

`kubectl exec -ti $POD_NAME -- bash`

We have now an open console on the container where we run our NodeJS application. The source code of the app is in the server.js file:

`cat server.js`

You can check that the application is up by running a curl command:

`curl http://localhost:8080`

容器内部的 localhost:8080 端口

# Using a Service to Expose Your App
## Overview of K8s Services
K8s Pods are mortal. Pods have a lifecycle. When a worker node dies, the Pods running on the Node are also lost. A ReplicaSet might then dynamically drive the cluster back to the desired state via the creation of new Pods to keep your application running. That said, each Pod in a Kubernetes cluster has a unique IP address, even Pods on the same Node, so there needs to be a way of automatically reconciling changes among Pods so that your applications continue to function.

A Service in k8s is an abstraction which defines a logical set of Pods and a policy by which to access them. Services enable a loose coupling between dependent Pods. A Service is defined using YAML or JSON, like all Kubernetes object manifests. The set of Pods targeted by a Service is usually determined by a _label selector_ (see below for why you might want a Service without including a `selector` in the spec).

A K8s Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.

Although each Pod has a unique IP address, those IPs are not exposed outside the cluster without a Service. Services allow your applications to receive traffic. Services can be exposed in different ways by specifying a `type` in the spec of the Service:
- _ClusterIP_ (default) - Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster.
- _NodePort_ - Exposes the Service on the same port of each selected Node in the cluster using NAT. Makes a Service accessible from outside the cluster using `<NodeIP>:<NodePort>`. Superset of ClusterIP.
- _LoadBalancer_ - Creates an external load balancer in the current cloud (if supported) and assigns a fixed, external IP to the Service. Superset of NodePort.
- _ExternalName_ - Maps the Service to the contents of the `externalName` field (e.g. `foo.bar.example.com`), by returning a `CNAME` record with its value. No proxying of any kind is set up. This type requires v1.7 or higher of `kube-dns`, or CoreDNS version 0.0.8 or higher.

这段话解释了 Kubernetes 中 **Service（服务）** 的几种暴露方式，以便应用程序能够接收流量。尽管每个 Pod 都有一个唯一的 IP 地址，但这个 IP 只能在 Kubernetes 集群内部使用，外部无法直接访问。因此，**Service** 充当了连接 Pod 与外部世界的桥梁。

## **Service 的几种类型**

1. **ClusterIP（默认类型）**
    
    - 作用：在集群内部创建一个 IP 地址，只有集群内部的组件才能访问这个 Service。
        
    - 适用场景：集群内的微服务之间进行通信。
        
    - 访问方式：只能通过 **集群内部的 IP** 访问，不能从外部直接访问。
        
2. **NodePort**
    
    - 作用：在集群中 **每个 Node（节点）** 的相同端口上暴露服务，并通过 NAT（网络地址转换）将请求转发给 Pod。
        
    - 适用场景：需要外部访问但不想依赖云提供商的负载均衡（如开发测试环境）。
        
    - 访问方式：外部可以通过 `<NodeIP>:<NodePort>` 访问。
        
    - 端口范围：默认在 `30000-32767` 之间。
        
3. **LoadBalancer**
    
    - 作用：在 **云环境（如 AWS、GCP、Azure）** 下，会自动创建一个外部负载均衡器，并分配一个固定的外部 IP 地址。
        
    - 适用场景：需要一个稳定的外部 IP 地址，适用于生产环境。
        
    - 访问方式：外部可以通过负载均衡器提供的 **外部 IP 地址** 访问。
        
    - **特点**：是 `NodePort` 的超集，包含 `NodePort` 的功能。
        
4. **ExternalName**
    
    - 作用：将 Service 解析为 **外部域名**（CNAME 记录），不会进行代理转发。
        
    - 适用场景：当你想让 Kubernetes 内部的 Pod 访问外部服务（如 `foo.bar.example.com`）时。
        
    - 访问方式：集群内的服务查询该 Service 时，DNS 解析会返回一个 **CNAME**，指向 `externalName` 指定的外部域名。
        

---

### **总结**

|Service 类型|是否可被外部访问|访问方式|
|---|---|---|
|ClusterIP|❌ 仅集群内部|`ClusterIP:Port`|
|NodePort|✅ 通过节点 IP|`<NodeIP>:<NodePort>`|
|LoadBalancer|✅ 通过外部负载均衡器|`外部 IP`|
|ExternalName|✅ 但只返回域名|`CNAME 解析，不走代理`|

如果你想从外部访问集群中的应用：

- **内网访问**：使用 `ClusterIP`
    
- **简单外部访问**：使用 `NodePort`
    
- **生产环境，云上负载均衡**：使用 `LoadBalancer`
    
- **仅解析域名**：使用 `ExternalName`
    


## Services and Labels
A Service routes traffic across a set of Pods. Services are the abstraction that allows pods to die and replicate in Kubernetes without impacting your application. Discovery and routing among dependent Pods (such as the frontend and backend components in an application) are handled by Kubernetes Services.

Services match a set of Pods using [labels and selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels), a grouping primitive that allows logical operation on objects in Kubernetes. Labels are key/value pairs attached to objects and can be used in any number of ways:
- Designate objects for development, test, and production
- Embed version tags
- Classify an object using tags
![[Pasted image 20250225104400.png]]
### Step 1: Creating a new Service
We have a Service called kubernetes that is created by default when minikube starts the cluster. To create a new service and expose it to external traffic we'll use the expose command with NodePort as parameter.

**kubectl expose deployment/kubernetes-bootcamp --type="NodePort" --port 8080**

To find out what port was opened externally (for the type: NodePort Service) we’ll run the `describe service` subcommand:

**kubectl describe services/kubernetes-bootcamp**

Create an environment variable called NODE_PORT that has the value of the Node port assigned:

**export NODE_PORT="$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')"**

**echo "NODE_PORT=$NODE_PORT"**

Now we can test that the app is exposed outside of the cluster using `curl`, the IP address of the Node and the externally exposed port:

**curl http://"$(minikube ip):$NODE_PORT"**

> Note:
> 
If you're running minikube with Docker Desktop as the container driver, a minikube tunnel is needed. This is because containers inside Docker Desktop are isolated from your host computer.

In a separate terminal window, execute:  
`minikube service kubernetes-bootcamp --url`

The output looks like this:

**http://127.0.0.1:51082  
!  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.**

Then use the given URL to access the app:  
`curl 127.0.0.1:51082`

And we get a response from the server. The Service is exposed.

### Step 2: Using labels
The Deployment created automatically a label for our Pod. With the `describe deployment` subcommand you can see the name (the _key_) of that label:

**kubectl describe deployment**

Let’s use this label to query our list of Pods. We’ll use the `kubectl get pods` command with -l as a parameter, followed by the label values:

**kubectl get pods -l app=kubernetes-bootcamp**


# Running Multiple Instances of Your App
## Scaling an application
Exposed it publicly via a Service. The Deployment created only one Pod for running our application. When traffic increases, we will need to scale the application to keep up with user demand.

_Scaling_ is accomplished by changing the number of replicas in a Deployment

You can create from the start a Deployment with multiple instances using the --replicas parameter for the kubectl create deployment command.

## Scaling overview
Scaling out a Deployment will ensure new Pods are created and scheduled to Nodes with available resources. Scaling will increase the number of Pods to the new desired state. Kubernetes also supports [autoscaling](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) of Pods. Scaling to zero is also possible, and it will terminate all Pods of the specified Deployment.

Running multiple instances of an application will require a way to distribute the traffic to all of them. Services have an integrated load-balancer that will distribute network traffic to all Pods of an exposed Deployment. Services will monitor continuously the running Pods using endpoints, to ensure the traffic is sent only to available Pods.

Once you have multiple instances of an application running, you would be able to do Rolling updates without downtime.

## Scaling a Deployment
To list your Deployments, use the `get deployments` subcommand:

**kubectl get deployments**

The output should be similar to:
![[Pasted image 20250225113834.png]]
This shows:
- _NAME_ lists the names of the Deployments in the cluster.
- _READY_ shows the ratio of CURRENT/DESIRED replicas
- _UP-TO-DATE_ displays the number of replicas that have been updated to achieve the desired state.
- _AVAILABLE_ displays how many replicas of the application are available to your users.
- _AGE_ displays the amount of time that the application has been running.

To see the ReplicaSet created by the Deployment, run:

**kubectl get rs**

Notice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-[RANDOM-STRING]. The random string is randomly generated and uses the _pod-template-hash_ as a seed.

![[Pasted image 20250402181541.png]]

Two important columns of this output are:
- _DESIRED_ displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state.
- _CURRENT_ displays how many replicas are currently running.

Next, let’s scale the Deployment to 4 replicas. We’ll use the `kubectl scale` command, followed by the Deployment type, name and desired number of instances:

**kubectl scale deployments/kubernetes-bootcamp --replicas=4**

The change was applied, and we have 4 instances of the application available. Next, let’s check if the number of Pods changed:

**kubectl get pods -o wide**

There are 4 Pods now, with different IP addresses. The change was registered in the Deployment events log. To check that, use the describe subcommand:

**kubectl describe deployments/kubernetes-bootcamp**

## Load Balancing
Let's check that the Service is load-balancing the traffic. To find out the exposed IP and Port we can use the describe service as we learned in the previous part of the tutorial:

**kubectl describe services/kubernetes-bootcamp**

Next, we’ll do a `curl` to the exposed IP address and port. Execute the command multiple times:

**curl http://"$(minikube ip):$NODE_PORT"**

We hit a different Pod with every request. This demonstrates that the load-balancing is working.

The output should be similar to:
![[Pasted image 20250225115532.png]]

# Performing a Rolling Update
## Updating an application
Users expect applications to be available all the time, and developers are expected to deploy new versions of them several times a day. In Kubernetes this is done with rolling updates.
 A **rolling update** allows a Deployment update to take place with zero downtime. It does this by incrementally replacing the current Pods with new ones. The new Pods are scheduled on Nodes with available resources, and Kubernetes waits for those new Pods to start before removing the old Pods.

Similar to application Scaling, if a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update. An available Pod is an instance that is available to the users of the application.

Rolling updates allow the following actions:

- Promote an application from one environment to another (via container image updates)
- Rollback to previous versions
- Continuous Integration and Continuous Delivery of applications with zero downtime

In the following interactive tutorial, we'll update our application to a new version, and also perform a rollback.

## Update the version of the app
To view the current image version of the app, run the `describe pods` subcommand and look for the `Image` field:

**kubectl describe pods**

To update the image of the application to version 2, use the `set image` subcommand, followed by the deployment name and the new image version:

**kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2

The command notified the Deployment to use a different image for your app and initiated a rolling update. Check the status of the new Pods, and view the old one terminating with the `get pods` subcommand:


## Verify an update
Next, do a `curl` to the exposed IP and port:

Every time you run the `curl` command, you will hit a different Pod. Notice that all Pods are now running the latest version (v2).

You can also confirm the update by running the `rollout status` subcommand:

**kubectl rollout status deployments/kubernetes-bootcamp**

To view the current image version of the app, run the `describe pods` subcommand:

`kubectl describe pods`

In the `Image` field of the output, verify that you are running the latest image version (v2).

## Roll back an update
Let’s perform another update, and try to deploy an image tagged with `v10`:

**kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=gcr.io/google-samples/kubernetes-bootcamp:v10**

Use `get deployments` to see the status of the deployment:

**kubectl get deployments**

Notice that the output doesn't list the desired number of available Pods. Run the `get pods` subcommand to list all Pods:

**kubectl get pods**

Notice that some of the Pods have a status of ImagePullBackOff.

To get more insight into the problem, run the `describe pods` subcommand:

**kubectl describe pods**

In the `Events` section of the output for the affected Pods, notice that the `v10` image version did not exist in the repository.

To roll back the deployment to your last working version, use the `rollout undo` subcommand:

**kubectl rollout undo deployments/kubernetes-bootcamp**

The `rollout undo` command reverts the deployment to the previous known state (v2 of the image). Updates are versioned and you can revert to any previously known state of a Deployment.

The Deployment is once again using a stable version of the app (v2). The rollback was successful.

Remember to clean up your local cluster

**kubectl delete deployments/kubernetes-bootcamp services/kubernetes-bootcamp**


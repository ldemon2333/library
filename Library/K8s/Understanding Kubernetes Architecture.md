https://devopscube.com/kubernetes-architecture-explained/

The followingÂ **Kubernetes architecture diagram**Â shows all the components of the Kubernetes cluster and how external systems connect to the Kubernetes cluster.
![[Pasted image 20250615222626.png]]

# HTTP REST API
æˆ‘ä»¬å¯ä»¥åˆ†å‡ å—æ¥è¯´æ¸…æ¥š **â€œHTTP REST APIâ€** è¿™ä¸ªè¯´æ³•ã€‚

---

### ğŸ”¹ä»€ä¹ˆæ˜¯ APIï¼Ÿ

APIï¼ˆApplication Programming Interfaceï¼Œåº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼‰æ˜¯ä¸åŒè½¯ä»¶ä¹‹é—´è¿›è¡Œæ•°æ®äº¤æ¢çš„ä¸€ç§æ–¹å¼ã€‚å¯ä»¥æŠŠå®ƒç†è§£ä¸º**ä¸åŒè½¯ä»¶ä¹‹é—´çš„ä¸€ç§â€œåè®®â€**ï¼Œè¯´å¥½æ€æ ·äº¤æ¢æ•°æ®ã€‚

---

### ğŸ”¹ä»€ä¹ˆæ˜¯ RESTï¼Ÿ

RESTï¼ˆRepresentational State Transferï¼Œè¡¨ç°å±‚çŠ¶æ€è½¬ç§»ï¼‰æ˜¯è®¾è®¡APIæ—¶çš„ä¸€ç§**æ¶æ„é£æ ¼**ï¼Œå®ƒæœ‰ä»¥ä¸‹å‡ ä¸ªä¸»è¦ç‰¹ç‚¹ï¼š

âœ… **æ— çŠ¶æ€ï¼ˆStatelessï¼‰**ï¼šæ¯ä¸€æ¬¡APIè°ƒç”¨æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œä¸ä¹‹å‰å‘ç”Ÿçš„æ“ä½œæ— å…³ï¼Œ**æœåŠ¡å™¨ä¸ä¼šä¿å­˜å®¢æˆ·ç«¯çš„ä¼šè¯**ã€‚

âœ… **èµ„æºï¼ˆResourcesï¼‰**ï¼šRESTAPIå°†æ•°æ®å’ŒåŠŸèƒ½æŠ½è±¡ä¸ºâ€œèµ„æºâ€ï¼Œæ¯ç§èµ„æºéƒ½æœ‰ä¸€ä¸ª**å”¯ä¸€URL**ã€‚

âœ… **è¡¨ç°ï¼ˆRepresentationï¼‰**ï¼šå®¢æˆ·ç«¯å¯ä»¥æŒ‰éœ€è¦è·å–ä¸åŒè¡¨ç°å½¢å¼çš„æ•°æ®ï¼ˆæœ€å¸¸ç”¨çš„å°±æ˜¯JSONæˆ–è€…XMLï¼‰ã€‚

âœ… **ç»Ÿä¸€æ“ä½œ**ï¼šå¯¹èµ„æºè¿›è¡Œæ“ä½œæ—¶ï¼Œä¸»è¦ä¾èµ–**HTTPæä¾›çš„ä¸€è‡´æ€§çš„æ“ä½œ**ï¼ˆGETã€POSTã€PUTã€DELETE ç­‰ï¼‰ï¼Œè€Œä¸æ˜¯è‡ªå·±å®šä¹‰å¾ˆå¤šä¸åŒçš„â€œåŠ¨ä½œâ€ã€‚

---

### ğŸ”¹ä»€ä¹ˆæ˜¯ HTTP REST APIï¼Ÿ

**HTTP REST API** å°±æ˜¯**æŒ‰RESTæ¶æ„é£æ ¼è®¾è®¡å‡ºçš„API**ï¼Œå®ƒ**é€šè¿‡HTTPåè®®è¿›è¡Œæ•°æ®äº¤æ¢**ã€‚

ç®€è€Œè¨€ä¹‹ï¼š  
âœ… **API**ï¼šå¯¹æ•°æ®æä¾›æ“ä½œå…¥å£  
âœ… **REST**ï¼šå®šä¹‰APIå¦‚ä½•è¿›è¡Œè®¾è®¡ï¼ˆèµ„æº + æ— çŠ¶æ€ + ä¸€è‡´æ“ä½œï¼‰  
âœ… **HTTP**ï¼šå…·ä½“çš„æ•°æ®ä¼ è¾“åè®®ï¼ˆGETã€POSTã€PUTã€DELETEï¼‰

---

### ğŸ”¹ä¸¾ä¸ªä¾‹å­ï¼š

å‡å¦‚æˆ‘ä»¬æœ‰ä¸€ä¸ª**â€œç”¨æˆ·â€**èµ„æºï¼ŒAPIå¯ä»¥è¿™æ ·è®¾è®¡ï¼š

||URL|Method|ä½œç”¨|
|---|---|---|---|
|è·å–æ‰€æœ‰ç”¨æˆ·|`/users`|GET|åˆ—å‡ºå…¨éƒ¨ç”¨æˆ·|
|è·å–æŒ‡å®šç”¨æˆ·|`/users/{id}`|GET|è·å–æŒ‡å®š ID çš„ç”¨æˆ·|
|æ–°å¢ç”¨æˆ·|`/users`|POST|åˆ›å»ºä¸€ä¸ªæ–°ç”¨æˆ·|
|æ›´æ–°ç”¨æˆ·|`/users/{id}`|PUT|æ›´æ–°æŒ‡å®š ID çš„ç”¨æˆ·|
|åˆ é™¤ç”¨æˆ·|`/users/{id}`|DELETE|åˆ é™¤æŒ‡å®š ID çš„ç”¨æˆ·|

---

### ğŸ”¹æ•°æ®äº¤æ¢æ ·ä¾‹ï¼ˆJSONï¼‰ï¼š

âœ… **GET /users/123**

**å“åº”**ï¼š

```json
{
  "id": 123,
  "name": "Alice",
  "email": "alice@example.com"
}
```

âœ… **POST /users**

**è¯·æ±‚ä¸»ä½“ï¼ˆPOSTæ—¶å‘é€çš„æ•°æ®ï¼‰**ï¼š

```json
{
  "name": "Charlie",
  "email": "charlie@example.com"
}
```

**å“åº”**ï¼š

```json
{
  "id": 124,
  "name": "Charlie",
  "email": "charlie@example.com"
}
```

---

### ğŸ”¹æ€»ç»“ï¼š

âœ… **API**ï¼šè½¯ä»¶æä¾›çš„æ•°æ®å’Œæ“ä½œå…¥å£  
âœ… **REST**ï¼šæŒ‰èµ„æºè¿›è¡Œç»„ç»‡ï¼Œä¾èµ–HTTPè¿›è¡Œæ“ä½œï¼Œä¿æŒæ— çŠ¶æ€  
âœ… **HTTP**ï¼šå…·ä½“çš„æ•°æ®ä¼ è¾“åè®®ï¼ˆGETã€POSTã€PUTã€DELETEï¼‰

So when you use kubectl to manage the cluster, at the backend you are actually communicating with the API server throughÂ **HTTP REST APIs**. However, the internal cluster components like the scheduler, controller, etc talk to the API server usingÂ [gRPC](https://grpc.io/docs/what-is-grpc/introduction/?ref=devopscube.com).

The communication between the API server and other components in the cluster happens over TLS to prevent unauthorized access to the cluster.

![[Pasted image 20250615223120.png]]

KubernetesÂ **api-server**Â is responsible for the following.

1. **API management**: Exposes the cluster API endpoint and handles all API requests. The API is version and itsupports multiple API versions simultaneously.
2. **Authentication**Â (Using client certificates, bearer tokens, and HTTP Basic Authentication) andÂ **Authorization**Â (ABAC and RBAC evaluation)
3. Processing API requests and validating data for the API objects like pods, services, etc. (Validation and Mutation Admission controllers)
4. api-server coordinates all the processes between the control plane and worker node components.
5. API server also contianes anÂ [aggreagation layer](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/?ref=devopscube.com)Â which allows you to extend Kubernetes API to create custom APIs resources and controllers.
6. The only component that theÂ **kube-apiserver initiates a connection to**Â is theÂ **etcd**Â component. All the other components connect to the API server.
7. The API server alsoÂ **supports watching resources**Â for changes. For example, clients can establish a watch on specific resources and receive real-time notifications when those resources are created, modified, or deleted.
8. Each component (Kubelet, scheduler, controllers)Â **independently watches the API server**Â to figure out what it needs to do.


# 2 etcd
etcd usesÂ [raft consensus algorithm](https://raft.github.io/?ref=devopscube.com)Â for strong consistency and availability. It works in a leader-member fashion for high availability and to withstand node failures.


ä¸‹é¢æ˜¯å¯¹ **Raft ä¸€è‡´æ€§ç®—æ³•** çš„ç®€æ´ã€ç›´è§‚ä»‹ç»ï¼š

---

### ğŸ”¹Raft æ˜¯ä»€ä¹ˆï¼Ÿ

Raft æ˜¯ä¸ºäº†**åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­å®ç°æ•°æ®å‰¯æœ¬é«˜åº¦ä¸€è‡´**è€Œè®¾è®¡çš„ä¸€ç§**ä¸€è‡´æ€§ç®—æ³•**ã€‚  
å®ƒå¯ä»¥è®©ä¸€ç»„æœåŠ¡å™¨ï¼ˆé€šå¸¸ä¸º 3 æˆ– 5 å°ï¼‰å…±åŒå¯¹æ“ä½œè¿›è¡Œ**å¤åˆ¶**ï¼ˆReplicationï¼‰ï¼Œä»è€Œåšåˆ°ï¼š  
âœ… å®•æœºæ—¶æ•°æ®ä¸ä¸¢å¤±  
âœ… ç³»ç»Ÿå¯¹å®¢æˆ·ç«¯è¡¨ç°ä¸ºä¸€ä¸ªæ•´ä½“ï¼ˆå¯¹å¤–æ˜¯ä¸€ä¸ªâ€œé¢†å¯¼è€…â€ï¼‰

---

### ğŸ”¹Raft çš„æ ¸å¿ƒæ€è·¯ï¼š

â¥ **é¢†å¯¼è€…ï¼ˆLeaderï¼‰**  
æ¯æ—¶åˆ»æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªé¢†å¯¼è€…ï¼Œè´Ÿè´£å¤„ç†å®¢æˆ·ç«¯å†™æ“ä½œï¼Œç¡®ä¿**æ•°æ®æŒ‰åºå¤åˆ¶åˆ°å¤šæ•°å‰¯æœ¬**ã€‚

â¥ **è·Ÿéšè€…ï¼ˆFollowerï¼‰**  
è¢«é¢†å¯¼è€…å¤åˆ¶æ•°æ®ï¼Œçº¯è¢«åŠ¨åœ°è¿›è¡Œå“åº”ã€‚

â¥ **å€™é€‰è€…ï¼ˆCandidateï¼‰**  
è‹¥è·Ÿéšè€…é•¿æ—¶é—´æ”¶ä¸åˆ°é¢†å¯¼è€…å¿ƒè·³ï¼Œä¼šè‡ªå·±è½¬ä¸ºå€™é€‰è€…ï¼Œå‘èµ·**é€‰ä¸¾**ã€‚

---

### ğŸ”¹Raft ä¸€è‡´æ€§è¿‡ç¨‹ç®€è¿°ï¼š

1ï¸âƒ£ **é¢†å¯¼è€…é€‰ä¸¾ï¼ˆLeader Electionï¼‰**

- æœåŠ¡å™¨è¶…æ—¶æœªæ”¶åˆ°é¢†å¯¼è€…å¿ƒè·³æ—¶ï¼Œä¼šè½¬ä¸º Candidateï¼Œå¼€å§‹é€‰ä¸¾ï¼Œè‡ªå·±ç»™è‡ªå·±æŠ•ä¸€ç¥¨ï¼Œç„¶åå‘å…¶ä»–èŠ‚ç‚¹æ±‚ç¥¨ã€‚
    
- ä¸€æ—¦æœ‰**å¤šæ•°**ï¼ˆ> 50%ï¼‰èŠ‚ç‚¹æŠ•ç¥¨ç»™å®ƒï¼Œå®ƒå°±æˆä¸ºé¢†å¯¼è€…ã€‚
    

2ï¸âƒ£ **æ—¥å¿—å¤åˆ¶ï¼ˆLog Replicationï¼‰**

- ç”±é¢†å¯¼è€…æ¥æ”¶å®¢æˆ·ç«¯å†™æ“ä½œï¼Œå†™åˆ°è‡ªå·±çš„æ“ä½œæ—¥å¿—ä¸­ã€‚
    
- é¢†å¯¼è€…å¼‚æ­¥åœ°å°†æ­¤æ¡æ—¥å¿—å¤åˆ¶åˆ°æ¯ä¸€ä¸ªè·Ÿéšè€…ã€‚
    
- ä¸€æ—¦æœ‰**å¤šæ•°**è·Ÿéšè€…ç¡®è®¤å†™å…¥ï¼Œé¢†å¯¼è€…å°±å¯ä»¥â€œæäº¤â€æ­¤æ¡æ“ä½œï¼Œå¹¶å°†ç»“æœåº”ç”¨åˆ°è‡ªå·±çš„â€œçŠ¶æ€æœºâ€ä¸­ã€‚
    

---

### ğŸ”¹Raft çš„ä¼˜ç‚¹ï¼š

âœ… ç®€æ´ï¼šç»“æ„å’Œå®ç°ç›´è§‚ï¼Œå®¹æ˜“ç†è§£  
âœ… å®¹é”™ï¼šåªè¦**å¤§å¤šæ•°**èŠ‚ç‚¹æ˜¯æ­£å¸¸çš„ï¼Œæ•´ä½“å°±èƒ½æä¾›æœåŠ¡  
âœ… ä¸€è‡´ï¼šé€šè¿‡é¢†å¯¼è€…è¿›è¡Œå†™æ“ä½œåºåˆ—åŒ–ï¼Œä¿æŒæ•°æ®å‰¯æœ¬é«˜åº¦ä¸€è‡´


In a nutshell, here is what you need to know about etcd.
1. etcd stores all configurations, states, and metadata of Kubernetes objects (pods, secrets, daemonsets,Â [deployments](https://devopscube.com/kubernetes-deployment-tutorial/), configmaps, statefulsets, etc).
2. `etcd`Â allows a client to subscribe to events usingÂ `Watch()`Â API . Kubernetes api-server uses the etcdâ€™s watch functionality to track the change in the state of an object.
3. etcd exposes key-value APIÂ [using gRPC](https://etcd.io/docs/v3.5/learning/api/?ref=devopscube.com). Also, theÂ [gRPC gateway](https://etcd.io/docs/v3.3/dev-guide/api_grpc_gateway/?ref=devopscube.com)Â is a RESTful proxy that translates all the HTTP API calls into gRPC messages. This makes it an ideal database for Kubernetes.
4. etcd stores all objects under theÂ **/registry**Â directory key in key-value format. For example, information on a pod named Nginx in the default namespace can be found underÂ **/registry/pods/default/nginx**

![[Pasted image 20250615224159.png]]

Also, etcd it is the onlyÂ **Statefulset**Â component in the control plane.

The number of nodes in an etcd cluster directly affects its fault tolerance. Here's how it breaks down:

1. **3 nodes**: Can tolerate 1 node failure (quorum = 2)
2. **5 nodes**: Can tolerate 2 node failures (quorum = 3)
3. **7 nodes**: Can tolerate 3 node failures (quorum = 4)

And so on. The general formula for the number of node failures a cluster can tolerate is:

```
fault tolerance = (n - 1) / 2
```

WhereÂ `n`Â is the total number of nodes.

# 3 kube-scheduler
The kube-scheduler is responsible forÂ **scheduling Kubernetes pods on worker nodes**.

The following image shows a high-level overview ofÂ **how the scheduler works**.
![[Pasted image 20250615224428.png]]

In a Kubernetes cluster, there will be more than one worker node. So how does the scheduler select the node out of all worker nodes?

Here is how the scheduler works.

1. To choose the best node, the Kube-scheduler usesÂ **filtering and scoring**Â operations.
2. InÂ **filtering**, the scheduler finds the best-suited nodes where the pod can be scheduled. For example, if there are five worker nodes with resource availability to run the pod, it selects all five nodes. If there are no nodes, then the pod is unschedulable and moved to the scheduling queue. If It is a large cluster, let's say 100 worker nodes, and the scheduler doesn't iterate over all the nodes. There is a scheduler configuration parameter calledÂ **`percentageOfNodesToScore`**. The default value is typicallyÂ **50%**. So it tries to iterate over 50% of nodes in a round-robin fashion. If the worker nodes are spread across multiple zones, then the scheduler iterates over nodes in different zones. For very large clusters the defaultÂ **`percentageOfNodesToScore`**Â is 5%.
3. In theÂ **scoring phase**, the scheduler ranks the nodes by assigning a score to the filtered worker nodes. The scheduler makes the scoring by calling multipleÂ [scheduling plugins](https://kubernetes.io/docs/reference/scheduling/config/?ref=devopscube.com#scheduling-plugins). Finally, the worker node with the highest rank will be selected for scheduling the pod. If all the nodes have the same rank, a node will be selected at random.
4. Once the node is selected, the scheduler creates a binding event in the API server. Meaning an event to bind a pod and node.
![[Pasted image 20250615224900.png]]

Here is that you need to know about a scheduler.

1. It is a controller that listens to pod creation events in the API server.
2. The scheduler has two phases.Â **Scheduling cycle**Â and theÂ **Binding cycle**. Together it is called the scheduling context.Â TheÂ scheduling cycle selects a worker node and the binding cycle applies that change to the cluster.
3. The scheduler always places the high-priority pods ahead of the low-priority pods for scheduling. Also, in some cases, after the pod starts running in the selected node, the pod might get evicted or moved to other nodes. If you want to understand more, read theÂ [Kubernetes pod priority guide](https://devopscube.com/pod-priorityclass-preemption/)
4. You can create custom schedulers and run multiple schedulers in a cluster along with the native scheduler. When you deploy a pod you can specify the custom scheduler in the pod manifest. So the scheduling decisions will be taken based on the custom scheduler logic.
5. The scheduler has aÂ [pluggable scheduling framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/?ref=devopscube.com). Meaning, that you can add your custom plugin to the scheduling workflow.

# 4 kube Controller Manager
**Kube controller manager**Â is a component that manages all the Kubernetes controllers. Kubernetes resources/objects like pods, namespaces, jobs, replicaset are managed by respective controllers. Also, the Kube scheduler is also a controller managed by the Kube controller manager.
![[Pasted image 20250615225317.png]]

# 5 Cloud Controller Manager (CCM)
When kubernetes is deployed in cloud environments, the cloud controller manager acts as a bridge between Cloud Platform APIs and the Kubernetes cluster.

This way the core kubernetes core components can work independently and allow the cloud providers to integrate with kubernetes using plugins. (For example, an interface between kubernetes cluster and AWS cloud API)

[Cloud controller integration](https://devopscube.com/aws-cloud-controller-manager/)Â allows Kubernetes cluster to provision cloud resources like instances (for nodes), Load Balancers (for services), and Storage Volumes (for persistent volumes).
![[Pasted image 20250615225635.png]]

Cloud Controller Manager contains a set ofÂ **cloud platform-specific controllers**Â that ensure the desired state of cloud-specific components (nodes,Â [Loadbalancers](https://devopscube.com/aws-load-balancers/), storage, etc). Following are the three main controllers that are part of the cloud controller manager.
1. **Node controller:**Â This controller updates node-related information by talking to the cloud provider API. For example, node labeling & annotation, getting hostname, CPU & memory availability, nodes health, etc.
2. **Route controller:**Â It is responsible for configuring networking routes on a cloud platform. So that pods in different nodes can talk to each other.
3. **Service controller**: It takes care of deploying load balancers for kubernetes services, assigning IP addresses, etc.

Following are some of the classic examples of cloud controller manager.

1. Deploying Kubernetes Service of type Load balancer. Here Kubernetes provisions a Cloud-specific Loadbalancer and integrates with Kubernetes Service.
2. Provisioning storage volumes (PV) for pods backed by cloud storage solutions.

Other than PodSpecs from the API server, kubelet can accept podSpec from a file, HTTP endpoint, and HTTP server. A good example of â€œpodSpec from a fileâ€ is Kubernetes static pods.

Static pods are controlled by kubelet, not the API servers.

Here is a real-world example use case of the static pod.

While bootstrapping the control plane, kubelet starts the api-server, scheduler, and controller manager as static pods from podSpecs located atÂ `/etc/kubernetes/manifests`

Following are some of the key things about kubelet.

1. Kubelet uses the CRI (container runtime interface) gRPC interface to talk to the container runtime.
2. It also exposes an HTTP endpoint to stream logs and provides exec sessions for clients.
3. Uses the CSI (container storage interface) gRPC to configure block volumes.
4. It uses the CNI plugin configured in the cluster to allocate the pod IP address and set up any necessary network routes and firewall rules for the pod.

![[Pasted image 20250615230310.png]]

# 2 Kube proxy
To understand Kube proxy, you need to have a basic knowledge of Kubernetes Service & endpoint objects.

Service in Kubernetes is a way to expose a set of pods internally or to external traffic. When you create the service object, it gets a virtual IP assigned to it. It is called clusterIP. It is only accessible within the Kubernetes cluster.

The Endpoint object contains all the IP addresses and ports of pod groups under a Service object. The endpoints controller is responsible for maintaining a list of pod IP addresses (endpoints). The service controller is responsible for configuring endpoints to a service.

You cannot ping the ClusterIP because it is only used for service discovery, unlike pod IPs which are pingable.

Now let's understand Kube Proxy.

Kube-proxy is a daemon that runs on every node as aÂ [daemonset](https://devopscube.com/kubernetes-daemonset/). It is a proxy component that implements the Kubernetes Services concept for pods. (single DNS for a set of pods with load balancing). It primarily proxies UDP, TCP, and SCTP and does not understand HTTP.

When you expose pods using a Service (ClusterIP), Kube-proxy creates network rules to send traffic to the backend pods (endpoints) grouped under the Service object. Meaning, all the load balancing, and service discovery are handled by the Kube proxy.

Kube proxy talks to the API server to get the details about the Service (ClusterIP) and respective pod IPs & ports (endpoints). It also monitors for changes in service and endpoints.

Kube-proxy then uses any one of the following modes to create/update rules for routing traffic to pods behind a Service

1. **[IPTables](https://wiki.centos.org/HowTos/Network/IPTables?ref=devopscube.com)**: It is the default mode. In IPTables mode, the traffic is handled by IPtable rules. This means that for each service, IPtable rules are created. These rules capture the traffic coming to the ClusterIP and then forward it to the backend pods. Also, In this mode, kube-proxy chooses the backend pod random for load balancing. Once the connection is established, the requests go to the same pod until the connection is terminated.
2. **[IPVS](https://en.wikipedia.org/wiki/IP_Virtual_Server?ref=devopscube.com):**Â For clusters with services exceeding 1000, IPVS offers performance improvement. It supports the following load-balancing algorithms for the backend.
    1. `rr`: round-robin : It is the default mode.
    2. `lc`: least connection (smallest number of open connections)
    3. `dh`: destination hashing
    4. `sh`: source hashing
    5. `sed`: shortest expected delay
    6. `nq`: never queue
3. **Userspace**Â (legacy & not recommended)
4. **Kernelspace**: This mode is only for Windows systems.

![[Pasted image 20250615230835.png]]
If you would like to understand the performance difference between kube-proxy IPtables and IPVS mode,Â [read this article](https://www.tigera.io/blog/comparing-kube-proxy-modes-iptables-or-ipvs/?ref=devopscube.com).

Also, you can run a Kubernetes cluster without kube-proxy by replacing it withÂ [Cilium](https://docs.cilium.io/en/v1.9/gettingstarted/kubeproxy-free/?ref=devopscube.com).

How does the CNI Plugin work with Kubernetes?

1. The Kube-controller-manager is responsible for assigning pod CIDR to each node. Each pod gets a unique IP address from the pod CIDR.
2. Kubelet interacts with container runtime to launch the scheduled pod. The CRI plugin which is part of the Container runtime interacts with the CNI plugin to configure the pod network.
3. CNI Plugin enables networking between pods spread across the same or different nodes using an overlay network.

![[Pasted image 20250615231832.png]]

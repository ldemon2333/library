# $A=LU$
$A \rightarrow E_{21}A \rightarrow\dots\rightarrow U.$ In the two by two case this looks like:

![[Pasted image 20240707145642.png]]

We can convert this to a factorization $A = LU$ by "canceling" the matrix $E_{21}$;multiply by its inverse to get $E_{21}^{-1} E_{21} A=E_{21}^{-1}U$

![[Pasted image 20240707145838.png]]

The matrix $U$ is upper triangular with pivots on the diagonal. The matrix $L$ is lower triangular and has ones on the diagonal. Sometimes we will also want to factor out a diagonal matrix whose entries are the pivots:

![[Pasted image 20240707150044.png]]

In the three dimensional case, if $E_{21}E_{31}E_{21}A = U$ then $A=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1}U=LU$.

For example, suppose $E_{31}$ is the identity matrix and $E_{32}$ and $E_{21}$ are as shown below:

![[Pasted image 20240707150349.png]]

The factorization $A=LU$ is preferable to the statement $EA=U$ because the combination of row subtractions does not have the effect on $L$ that it did on $E$. Here $L=E^{-1}$:

![[Pasted image 20240707150604.png]]

# How expensive is elimination?
When using elimination to find the factorization $A=LU$ we just saw that we can build $L$ as we go by keeping track of row subtractions. We have to remember $L$ and (the matrix which will become) $U$; we don't have to store $A$ or $E_{ij}$ in the computer's memory.

How many operations does the computer perform during the elimination process for an $n \times n$ matrix? A typical operation is to multiply one row and then subtract it from another, which requires on the order of $n$ operations.
There are $n$ rows, so the total number of operations used in eliminating entries in the first column is about $n^2$. The second row and column are shorter; that product costs about $(n-1)^2$ operations, and son on. The total number of operations needed to factor $A$ into $LU$ is on the order of $n^3$:
$$
n^2+(n-1)^2+ \cdots+2^2+1^2 = \sum_{i=1}^ni^2 \approx \int_{0}^n x^2 \,dx = \frac{1}{3}n^3.
$$

While we're factoring $A$ we're also operating on $\mathbf{b}$. That costs about $n^2$ operations, which is hardly worth counting compared to $\frac{1}{3}n^3$.

# Row exchanges 
What happens if there's a zero in a pivot position?

To swap two rows, we multiply on the left by a permutation matrix. For example,
$$
P_{12}=\left[
\begin{array}{ccc}
0 &1 &0\\
1&0&0\\
0&0&1

\end{array}
\right]
$$

swaps the first and second rows of a $3\times 3$ matrix. The inverse of any permutation matrix $P$ is $P^{-1} = P^T$.




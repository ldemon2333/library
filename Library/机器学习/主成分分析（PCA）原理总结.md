# 1.PCA的思想
PCA就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是n维的，共有m个数据。我们希望将这m个数据的维度从n维降到n'维，希望这m个n'维的数据集尽可能的代表原始数据集。我们知道数据从n维降到n'维肯定会有损失，但是我们希望损失尽可能的小。那么如何让这n'维的数据尽可能表示原来的数据呢？

我们先看看最简单的情况，也就是n=2，n'=1也就是将数据从二维降维到一维。数据如下图所示。我们希望找到某一个维度方向，它可以代表这两个维度的数据。图中列了两个向量方向，$u_1$和$u_2$，从直观上也可以看出，$u_1$比$u_2$好。

![[Pasted image 20241007201743.png]]

为什么$u_1$比$u_2$好呢？可以有两种解释，第一种解释是样本点到这个直线的距离足够近，第二种解释是样本点在这个直线上的投影能尽可能的分开。
　
假如我们把n'从1维推广到任意维，则我们的希望降维的标准为：样本点到这个超平面的距离足够近,或者说样本点在这个超平面上的投影能尽可能的分开。

基于上面的两种标准，我们可以得到PCA的两种等价推导。

# 2.PCA的推导：基于最小投影距离
我们首先看第一种解释的推动，即样本点到这个超平面的距离足够近。

假设m个n维数据$(x^{(1)}, x^{(2)},...,x^{(m)})$都已经进行了中心化，即$\sum_{i=1}^m x^{(i)}=0$。经过投影变换后得到的新坐标系为$\{w_1,w_2,...,w_n\}$，其中$w$是标准正交基，即$||w||_2=1$

![[Pasted image 20241007202504.png]]
![[Pasted image 20241007202615.png]]

# 3.PCA的推导：基于最大投影方差
![[Pasted image 20241007202713.png]]

# 4.PCA算法流程
从上面两节我们可以看出，求样本$x^{(i)}$的n'维的主成分其实就是求样本集的协方差矩阵$XX^T$的前n'个特征值对应特征向量矩阵W，然后对于每个样本$x^{(i)}$，做如下变换$z^{(i)}=W^Tx^{(i)}$，即达到降维PCA目的。

下面我们看看具体的算法流程

![[Pasted image 20241007203832.png]]

# 5.PCA实例
![[Pasted image 20241007204113.png]]

# 6.核主成分分析KPCA介绍
![[Pasted image 20241007204215.png]]

# 7.PCA算法总结
这里对PCA算法做一个总结。作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如第六节的为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。

PCA算法的主要优点有：

1）仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　

2）各主成分之间正交，可消除原始数据成分间的相互影响的因素。

3）计算方法简单，主要运算是特征值分解，易于实现。

PCA算法的主要缺点有：

1）主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。

2）方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

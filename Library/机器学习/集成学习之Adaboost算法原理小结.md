历史上，Kearns和Valiant首先提出了“强可学习（strongly learnable）”和“弱可学习（weakly learnable）”的概念。指出：在概率近似正确（probably approximately correct, PAC）学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。非常有趣的是Schapire后来证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

对提升（boosting）方法来说，有两个问题需要回答：一是在每一轮如何改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。关于第一个问题，Adaboost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。至于第二个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器权值，使其在表决中起较小的作用。

集成学习按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，另一类是个体学习器之间不存在强依赖关系。前者的代表算法就是boosting系列算法。在boosting系列算法中，Adaboost是最著名的算法之一。Adaboost既可以用作分类，也可以用作回归。

Adaboost弱学习器的基本上是深度为1的决策树（决策树桩）。

# 1.回顾boosting算法的基本原理

![[Pasted image 20240920193859.png]]

从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

但是有四个问题：
1. 如何计算学习误差率e？
2. 如何得到弱学习器权重系数$\alpha$?
3. 如何更新样本权重D？
4. 使用何种结合策略？

只要是boosting大家族的算法，都要解决这4个问题。那么Adaboost是怎么解决的呢？

# 2. Adaboost算法的基本思路
假设训练集样本是
$$
T=\{(x_1,y_1),(x_2,y_2), ...(x_m,y_m)\}
$$
训练集在第k个弱学习器的输出权重为
$$
D(k) = (w_{k1}, w_{k2}, ...w_{km}) ;\;\; w_{ki}=\frac{1}{m};\;\; i =1,2...m
$$
## Adaboost分类问题
![[Pasted image 20240920214429.png]]
有M个弱分类器
![[Pasted image 20240920214447.png]]
![[Pasted image 20240920214558.png]]
![[Pasted image 20240920214618.png]]
![[Pasted image 20240920214948.png]]


## 例子
![[Pasted image 20240920215340.png]]
![[Pasted image 20240920232630.png]]
![[Pasted image 20240920232657.png]]
![[Pasted image 20240920232838.png]]


分类问题的误差率很好理解和计算。由于多元分类是二元分类的推广，这里假设我们是二元分类问题，输出为$\{-1,1\}$，则第k个弱分类器 $G_k(x)$在训练集上的加权误差率为
$$
e_k=P(G_k(x_i) \neq y_i)=\sum\limits_{i=1}^{m}w_{ki}I(G_k(x_i) \neq y_i)
$$
接着我们看弱学习器权重系数，对于二元分类问题，第k个弱学习器$G_k(x)$的权重系数为
$$
\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k}
$$
从上式可以看出，如果分类误差率$e_k$越大，则对应的弱分类器权重系数$\alpha_k$越小。也就是说，误差率小的弱分类器权重系数越大。

对于Adaboost多元分类算法，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数
$$
\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k} + log(R-1)
$$
其中R为类别数。从上式可以看出，如果是二元分类，R=2，则上式和我们的二元分类算法中的弱分类器的系数一致。

ps：如果针对二分类，$e_k$要<1/2，要比随机的好，否则弃之。

第三个问题，更新样本权重D。假设第k个弱分类器的样本权重系数为$D(k) = (w_{k1}, w_{k2}, ...w_{km})$，则对应的第k+1个弱分类器的样本权重系数为
$$
w_{k+1,i} = \frac{w_{ki}}{Z_K}exp(-\alpha_ky_iG_k(x_i))
$$
这里$Z_k$是规范化因子
$$
Z_k = \sum\limits_{i=1}^{m}w_{ki}exp(-\alpha_ky_iG_k(x_i))
$$
从$w_{k+1,i}$计算公式可以看出，如果第i个样本分类错误，则$y_iG_k(x_i) < 0$，导致样本的权重在第k+1个弱分类器中增大，如果分类正确，则权重在第k+1个弱分类器中减少。

最后一个问题是集合策略。Adaboost分类采用的是加权表决法，最终的强分类器为
$$
f(x) = sign(\sum\limits_{k=1}^{K}\alpha_kG_k(x))
$$

## Adaboost 回归问题
这里以Adaboost R2算法为准。

回归问题的误差率问题，对于第k个弱学习器，计算他在训练集上的最大误差
$$
E_k= max|y_i - G_k(x_i)|\;i=1,2...m
$$
然后计算每个样本的相对误差
$$
e_{ki}= \frac{|y_i - G_k(x_i)|}{E_k}
$$
这里是误差损失为线性时的情况，如果我们用平法误差，则$e_{ki}= \frac{(y_i - G_k(x_i))^2}{E_k^2}$，如果我们用的是指数误差，则$e_{ki}= 1 - exp（\frac{-|y_i - G_k(x_i)|)}{E_k})$

最终得到第k个弱学习器的误差率为
$$
e_k =  \sum\limits_{i=1}^{m}w_{ki}e_{ki}
$$
我们再来看看如何得到弱学习器权重系数$\alpha$，这里有：
$$
\alpha_k =\frac{e_k}{1-e_k}
$$
对于更新样本权重D，第k+1个弱学习器的样本权重系数为
$$
w_{k+1,i} = \frac{w_{ki}}{Z_k}\alpha_k^{1-e_{ki}}
$$
最后是结合策略，和分类问题稍有不同，采用的是对加权的弱学习器权重中位数对应的弱学习器作为强学习器的方法，最终的强回归器为
$$
f(x) =G_{k^*}(x)
$$
其中，$G_{k^*}(x)$是所有$ln\frac{1}{\alpha_k}, k=1,2,....K$的中为数值对应序号$k^*$对应的弱学习器。

# 3.AdaBoost分类问题的损失函数优化
刚才上一节我们讲到了分类Adaboost的弱学习器权重系数公式和样本权重更新公式。但是没有解释选择这个公式的原因，让人觉得是魔法公式一样。其实它可以从Adaboost的损失函数推导出来。

从另一个角度讲，Adaboost是模型为加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题。

模型为加法模型好理解，我们的最终的强分类器是若干个弱分类器加权平均而得到的。

前向分布学习算法也好理解，我们的算法是通过一轮轮的弱学习器学习，利用前一个强学习器的结果和当前弱学习器来更新当前的强学习器的模型。也就是说，第k-1轮的强学习器为
$$
f_{k-1}(x) = \sum\limits_{i=1}^{k-1}\alpha_iG_{i}(x)
$$
而第k轮的强学习器为
$$
f_{k}(x) = \sum\limits_{i=1}^{k}\alpha_iG_{i}(x)
$$
上两式比较可以得到
$$
f_{k}(x) = f_{k-1}(x) + \alpha_kG_k(x)
$$
可见强学习器的确是通过前向分布学习算法一步步而得到的。

Adaboost损失函数为指数函数，即定义损失函数为
$$
\underbrace{arg\;min\;}_{\alpha, G} \sum\limits_{i=1}^{m}exp(-y_if_{k}(x))
$$
利用前向分布学习算法的关系可以得到损失函数为
$$
(\alpha_k, G_k(x)) = \underbrace{arg\;min\;}_{\alpha, G}\sum\limits_{i=1}^{m}exp[(-y_i) (f_{k-1}(x) + \alpha G(x))]
$$
令$w_{ki}^{’} = exp(-y_if_{k-1}(x))$，它的值不依赖与$\alpha,G$，因此与最小化无关，仅仅依赖于$f_{k-1}(x)$，随着每一轮迭代而改变。

损失函数转化为
$$
(\alpha_k, G_k(x)) = \underbrace{arg\;min\;}_{\alpha, G}\sum\limits_{i=1}^{m}w_{ki}^{’}exp[-y_i\alpha G(x)]
$$
首先，我们求$G_k(x)$
$$
\begin{align} \sum\limits_{i=1}^mw_{ki}^{'}exp(-y_i\alpha G(x_i)) &= \sum\limits_{y_i =G_k(x_i)}w_{ki}^{'}e^{-\alpha} + \sum\limits_{y_i \ne G_k(x_i)}w_{ki}^{'}e^{\alpha} \tag{1}\\& = (e^{\alpha} - e^{-\alpha})\sum\limits_{i=1}^mw_{ki}^{'}I(y_i \ne G_k(x_i)) + e^{-\alpha}\sum\limits_{i=1}^mw_{ki}^{'} \tag{2}\end{align}
$$
![[Pasted image 20240920235318.png]]
基于上式，可以得到
$$
G_k(x) = \underbrace{arg\;min\;}_{G}\sum\limits_{i=1}^{m}w_{ki}^{’}I(y_i \neq G(x_i))
$$
将$G_k(x)$带入损失函数，并对$\alpha$求导，使其等于0，则就得到了
$$
\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k}
$$
其中，$e_k$即为我们前面的分类误差率。
$$
e_k = \frac{\sum\limits_{i=1}^{m}w_{ki}^{’}I(y_i \neq G(x_i))}{\sum\limits_{i=1}^{m}w_{ki}^{’}} = \sum\limits_{i=1}^{m}w_{ki}I(y_i \neq G(x_i))
$$
最后看样本权重的更新。利用$f_{k}(x) = f_{k-1}(x) + \alpha_kG_k(x)$和$w_{ki}^{’} = exp(-y_if_{k-1}(x))$，即可得到：
$$
w_{k+1,i}^{’} = w_{ki}^{’}exp[-y_i\alpha_kG_k(x)]
$$
# 4. Adaboost回归问题的算法流程
这里我们对AdaBoost回归问题算法流程做一个总结。AdaBoost回归算法变种很多，下面的算法为Adaboost R2回归算法过程。

输入：样本集$T=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$，弱学习器算法，弱学习器迭代次数K.

输出：最终的强学习器$f(x)$

![[Pasted image 20240920234516.png]]
![[Pasted image 20240920234531.png]]

# 5.Adaboost算法的正则化
为了防止Adaboost过拟合，我们通常也会加入正则化项，这个正则化项我们通常称为步长(learning rate)。定义为$\nu$,对于前面的弱学习器的迭代
$$
f_{k}(x) = f_{k-1}(x) + \alpha_kG_k(x)
$$
如果我们加上了正则化项，则有
$$
f_{k}(x) = f_{k-1}(x) + \nu\alpha_kG_k(x)
$$
$\nu$的取值范围为$0 < \nu \leq 1$.对于同样的训练集学习效果，较小的$\nu$意味着更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。

# 6.小结
到这里Adaboost就写完了，前面有一个没有提到，就是弱学习器的类型。理论上任何学习器都可以用于Adaboost.但一般来说，使用最广泛的Adaboost弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。

这里对Adaboost算法的优缺点做一个总结。

Adaboost的主要优点有：

1）Adaboost作为分类器时，分类精度很高

2）在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。

3）作为简单的二元分类器时，构造简单，结果可理解。

4）不容易发生过拟合

Adaboost的主要缺点有：

1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。
# 1.scikit-learn PCA类介绍
在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。最常用的PCA类就是sklearn.decomposition.PCA，我们下面主要也会讲解基于这个类的使用的方法。

除了PCA类以外，最常用的PCA相关类还有KernelPCA类，它主要用于非线性数据的降维，需要用到核技巧。因此在使用的时候需要选择合适的核函数并对核函数的参数进行调参。

另一个常用的PCA相关类是IncrementalPCA类，它主要是为了解决单机内存限制的。有时候样本量可能是上百万+，维度可能也是上千，直接去拟合数据可能会让内存爆掉， 此时我们可以用IncrementalPCA类来解决这个问题。IncrementalPCA先将数据分成多个batch，然后对每个batch依次递增调用partial_fit函数，这样一步步的得到最终的样本最优降维。

此外还有SparsePCA和MiniSparsePCA。他们和上面讲到的PCA类的区别主要是使用了L1的正则化，这样可以将很多非主要成分的影响度降为0，这样在PCA降维的时候我们仅仅需要对那些相对比较主要的成分进行PCA降维，避免了一些噪声之类的因素对PCA降维的影响。SparsePCA和MiniSparsePCA之间的区别则是MiniBatchSparsePCA通过使用一部分样本特征和给定的迭代次数来进行PCA降维，以解决在大样本时特征分解过慢的问题，当然，代价就是PCA降维的精确度可能会降低。使用SparsePCA和MiniSparsePCA需要对L1正则化参数进行调参。

# 2.sklearn.decomposition.PCA参数介绍
下面我们主要基于sklearn.decomposition.PCA来讲解如何使用scikit-learn进行PCA降维。PCA类基本不需要调参，一般来说，我们只需要指定我们需要降维到的维度，或者我们希望降维后的主成分的方差和占原始维度所有特征方差和的比例阈值就可以了。

1）**n_components**：这个参数可以帮我们指定希望PCA降维后的特征维度数目。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于等于1的整数。当然，我们也可以指定主成分的方差和所占的最小比例阈值，让PCA类自己去根据样本特征方差来决定降维到的维度数，此时n_components是一个（0，1]之间的数。当然，我们还可以将参数设置为"mle", 此时PCA类会用MLE算法根据特征的方差分布情况自己去选择一定数量的主成分特征来降维。我们也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)。

2）**whiten**：判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化，让方差都为1.对于PCA降维本身来说，一般不需要白化。如果你PCA降维后有后续的数据处理动作，可以考虑白化。默认值是False，即不进行白化。


3）**svd_solver**：即指定奇异值分解SVD的方法，由于特征分解是奇异值分解SVD的一个特例，一般的PCA库都是基于SVD实现的。有4个可以选择的值：{‘auto’, ‘full’, ‘arpack’, ‘randomized’}。randomized一般适用于数据量大，数据维度多同时主成分数目比例又较低的PCA降维，它使用了一些加快SVD的随机算法。 full则是传统意义上的SVD，使用了scipy库对应的实现。arpack和randomized的适用场景类似，区别是randomized使用的是scikit-learn自己的SVD实现，而arpack直接使用了scipy库的sparse SVD实现。默认是auto，即PCA类会自己去在前面讲到的三种算法里面去权衡，选择一个合适的SVD算法来降维。一般来说，使用默认值就够了。

除了这些输入参数外，有两个PCA类的成员值得关注。第一个是**explained_variance_**，它代表降维后的各主成分的方差值。方差值越大，则说明越是重要的主成分。第二个是**explained_variance_ratio_**，它代表降维后的各主成分的方差值占总方差值的比例，这个比例越大，则越是重要的主成分。


在执行主成分分析（PCA）之前进行**中心化处理**，即将每个特征的均值归零，是非常重要的一步。这是因为中心化对 PCA 的计算和结果有以下几个关键影响：

1. **使协方差矩阵反映真实的特征变化**：
   - PCA 的核心是计算数据的协方差矩阵。协方差矩阵衡量的是特征之间的线性相关性。协方差的计算公式依赖于每个特征的均值，如果不进行中心化处理，协方差矩阵会被数据的均值值偏移，影响主成分的提取结果。中心化可以消除均值的影响，使得协方差仅反映变量之间的真实波动关系。

2. **保证主成分的方向性**：
   - PCA 的目标是找到数据的主成分，即最大化方差的方向。如果不进行中心化，第一主成分将会受到原始数据的均值的影响，偏向高均值特征，从而可能无法正确找出数据的主要变化方向。

3. **使结果更具可解释性**：
   - 如果数据未经中心化，主成分可能会偏向那些数值较大的特征，而不是体现真实的方差情况。通过将数据中心化，每个特征在相同的参考框架下，主成分的方向和方差大小变得更具解释性。

### 举例
假设有两个特征的数据集，一个特征的数值范围是1000左右，另一个特征的数值范围是0.5左右。如果不做中心化，PCA 可能过多地偏向于解释第一个特征的数据，因为它的均值较大，但这并不意味着它的重要性高。

因此，**中心化处理**可以去除特征中的偏移，使得 PCA 更加专注于特征间的相对方差，而不是绝对数值的大小。


**协方差矩阵**（Covariance Matrix）是数据的一个统计度量，用来表示多维变量之间的协方差关系。对于多维数据集，协方差矩阵反映了每对特征之间的线性相关性。它是 PCA 和很多其他多变量统计分析的重要基础。

### 协方差的定义
协方差反映了两个变量如何共同变化。对于两个变量 $X$ 和 $Y$，它们的协方差定义为：
$$

\text{Cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})

$$
其中：
- $n$ 是样本数量，
- $X_i$ 和 $Y_i$ 是变量 $X$ 和 $Y$ 的第 $i$ 个样本值，
- $\bar{X}$ 和 $\bar{Y}$ 分别是变量 $X$ 和 $Y$ 的均值。

如果协方差为正，说明两个变量呈正相关；如果为负，说明它们呈负相关；如果接近 0，则说明没有明显的线性相关性。

### 协方差矩阵的构建
对于一个有 $d$ 个特征的多维数据集（假设有 $n$ 个样本，每个样本有 $d$ 个特征），协方差矩阵是一个 $d \times d$ 的对称矩阵，定义为：
$$
\mathbf{Cov}(X) = \begin{bmatrix}
\text{Cov}(X_1, X_1) & \text{Cov}(X_1, X_2) & \cdots & \text{Cov}(X_1, X_d) \\
\text{Cov}(X_2, X_1) & \text{Cov}(X_2, X_2) & \cdots & \text{Cov}(X_2, X_d) \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}(X_d, X_1) & \text{Cov}(X_d, X_2) & \cdots & \text{Cov}(X_d, X_d)
\end{bmatrix}
$$
其中：
- $\text{Cov}(X_i, X_j)$ 表示第 $i$ 个特征和第 $j$ 个特征的协方差。
- 对角线上的值是每个特征的方差，即 $\text{Cov}(X_i, X_i) = \text{Var}(X_i)$。
- 矩阵是对称的，因为 $\text{Cov}(X_i, X_j) = \text{Cov}(X_j, X_i)$。

### 协方差矩阵的计算
给定一个数据集 $X$（大小为 $n \times d$，$n$ 为样本数，$d$ 为特征数），协方差矩阵可以通过以下公式计算：
$$
\mathbf{Cov}(X) = \frac{1}{n-1} (X - \bar{X})^T (X - \bar{X})
$$
其中：
- $X - \bar{X}$ 表示将数据矩阵 $X$ 的每个特征减去它的均值，完成中心化操作。

### Python 计算协方差矩阵的示例
你可以使用 `numpy` 中的 `np.cov` 函数来计算协方差矩阵，示例如下：

```python
import numpy as np

# 生成一个示例数据集
X = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9],
              [10, 11, 12]])

# 计算协方差矩阵
cov_matrix = np.cov(X, rowvar=False)
print(cov_matrix)
```

在这个例子中，`rowvar=False` 意味着每一列是一个特征，而每一行是一个样本。


**Pearson 系数**（Pearson Correlation Coefficient）是一个用于衡量两个变量之间线性相关性的统计量，其值范围从 -1 到 1。它的计算公式为：
$$
r = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}

$$
其中：
- $r$ 是 Pearson 系数。
- $\text{Cov}(X, Y)$ 是变量 $X$ 和 $Y$ 的协方差。
- $\sigma_X$ 和 $\sigma_Y$ 是变量 $X$ 和 $Y$ 的标准差。

### 解读
- **$r = 1$**：完全正相关，两个变量呈完美的线性关系。
- **$r = -1$**：完全负相关，两个变量呈完美的反向线性关系。
- **$r = 0$**：没有线性相关性，但可能存在非线性关系。

### 特点
- Pearson 系数仅衡量线性关系，对于非线性关系不敏感。
- 受异常值影响较大，因此在使用前应进行数据清理。

### Python 示例
可以使用 `numpy` 或 `scipy` 库来计算 Pearson 系数，示例如下：

```python
import numpy as np
from scipy.stats import pearsonr

# 示例数据
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 3, 4, 5, 6])

# 计算 Pearson 系数
r, _ = pearsonr(x, y)
print("Pearson 系数:", r)
```


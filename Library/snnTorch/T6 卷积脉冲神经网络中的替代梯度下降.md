在本教程中，你将：
1）学习如何使用代理梯度下降来客服死神经元问题
2）构建并训练卷积脉冲神经网络
3）使用顺序容器nn.Sequential来简化模型构造
4）使用snn.backprop模块，以减少设计神经网络所需的时间

# 1.代理梯度下降（Surrogate Gradient Descent）
教程5提出了死神经元问题。这是因为尖峰电位的不可微性：
$$S[t] = \Theta(U[t] - U_{\rm thr}) \tag{1}$$
$$\frac{\partial S}{\partial U} = \delta(U - U_{\rm thr}) \in \{0, \infty\} \tag{2}$$
其中$\Theta(\cdot)$是Heaviside阶跃函数，$\delta(\cdot)$是Dirac-Delta函数。我们之前使用spike-operator方法克服了这个问题。另一种方法是在向后传递期间平滑Heaviside函数，这相应地平滑了Heaviside函数的梯度。

常见的平滑函数有sigmoid函数或者fast sigmoid函数。sigmoid函数也必须进行平滑，使其以阈值$U_{\rm thr}$为中心。定义膜电位的过载为$U_{OD} = U - U_{\rm thr}$：
$$\tilde{S} = \frac{U_{OD}}{1+k|U_{OD}|} \tag{3}$$
$$\frac{\partial \tilde{S}}{\partial U} = \frac{1}{(k|U_{OD}|+1)^2}\tag{4}$$
其中$k$调节代理函数的平滑程度，并被视为超参数。随着$k$的增加，近似收敛于（2）的原始导数：
$$\frac{\partial \tilde{S}}{\partial U} \Bigg|_{k \rightarrow \infty} = \delta(U-U_{\rm thr})$$
![[Pasted image 20241012150512.png]]
总结：
* **Forward Pass**

  -  使用 $(1)$的移位Heaviside函数确定$S$

  - 存储 $U$ 以备后向传递时使用 

* **Backward Pass**

  - 将 $U$ 传递到 $(4)$ 中以计算导数项

在Leaky IF 函数中，fast sigmoid 函数的梯度可以覆盖Dirac-Delta 函数

（LIF）神经元模型：
```python3
# Leaky neuron model, overriding the backward pass with a custom function
class LeakySigmoidSurrogate(nn.Module):
  def __init__(self, beta, threshold=1.0, k=25):
      super(Leaky_Surrogate, self).__init__()

      # initialize decay rate beta and threshold
      self.beta = beta
      self.threshold = threshold
      self.surrogate_func = self.FastSigmoid.apply
  
  # the forward function is called each time we call Leaky
  def forward(self, input_, mem):
    spk = self.surrogate_func((mem-self.threshold))  # call the Heaviside function
    reset = (spk - self.threshold).detach()
    mem = self.beta * mem + input_ - reset
    return spk, mem

  # Forward pass: Heaviside function
  # Backward pass: Override Dirac Delta with gradient of fast sigmoid
  @staticmethod
  class FastSigmoid(torch.autograd.Function):  
    @staticmethod
    def forward(ctx, mem, k=25):
        ctx.save_for_backward(mem) # store the membrane potential for use in the backward pass
        ctx.k = k
        out = (mem > 0).float() # Heaviside on the forward pass: Eq(1)
        return out

    @staticmethod
    def backward(ctx, grad_output): 
        (mem,) = ctx.saved_tensors  # retrieve membrane potential
        grad_input = grad_output.clone()
        grad = grad_input / (ctx.k * torch.abs(mem) + 1.0) ** 2  # gradient of fast sigmoid on backward pass: Eq(4)
        return grad, None
```
更好的是，所有这些都可以通过使用snntorch内置模块snn.surrogate 来实现，其中$(4)$中的$k$表示为slope。替代梯度作为参数传递给spike_grad：
```python
spike_grad = surrogate.fast_sigmoid(slope=25)
beta = 0.5
lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)
```

# 2. 设置CSNN
## 2.1 Data Loader
注意，调用`utils.data_subset()`会将数据集的大小缩减10倍以加快训练速度。
```python
# dataloader arguments
batch_size = 128
data_path='/data/mnist'
subset=10

dtype = torch.float
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

# Define a transform
transform = transforms.Compose([
            transforms.Resize((28, 28)),
            transforms.Grayscale(),
            transforms.ToTensor(),
            transforms.Normalize((0,), (1,))])

mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)
mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)

# reduce datasets by 10x to speed up training
utils.data_subset(mnist_train, subset)
utils.data_subset(mnist_test, subset)

# Create DataLoaders
train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)
test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)
```

## 2.2 定义网络
使用的卷积网络架构是：12C5-MP2-64C5-MP2-1024FC10
- 12C5是一个 5 $\times$ 5  带有12个滤波器的卷积核
- MP2是一个2×2最大池化函数
- 1024FC10是一个全连接层，它将1024个神经元映射到10个输出

```text
# attention:这个初始化有问题，不应该每次进入forward进行一次初始化，初始化只在最开始全局初始化。
# neuron and simulation parameters
spike_grad = surrogate.fast_sigmoid(slope=25)
beta = 0.5
num_steps = 50

# Define Network
class Net(nn.Module):
    def __init__(self):
        super().__init__()

        # Initialize layers
        self.conv1 = nn.Conv2d(1, 12, 5)
        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)
        self.conv2 = nn.Conv2d(12, 64, 5)
        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)
        self.fc1 = nn.Linear(64*4*4, 10)
        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)

    def forward(self, x):

        # Initialize hidden states and outputs at t=0
        mem1 = self.lif1.init_leaky()# 有问题
        mem2 = self.lif2.init_leaky() 
        mem3 = self.lif3.init_leaky()

        # Record the final layer
        spk3_rec = []
        mem3_rec = []

        for step in range(num_steps):
            cur1 = F.max_pool2d(self.conv1(x), 2)
            spk1, mem1 = self.lif1(cur1, mem1)
            cur2 = F.max_pool2d(self.conv2(spk1), 2)
            spk2, mem2 = self.lif2(cur2, mem2)
            cur3 = self.fc1(spk2.view(batch_size, -1))
            spk3, mem3 = self.lif3(cur3, mem3)

            spk3_rec.append(spk3)
            mem3_rec.append(mem3)

        return torch.stack(spk3_rec), torch.stack(mem3_rec)
```
在前面的教程中，网络被包装在一个类中，如上所示。

随着网络复杂性的增加，这添加了许多我们希望避免的样板代码。或者，可以使用`nn.Sequential`方法来代替:
```text
#  Initialize Network
net = nn.Sequential(nn.Conv2d(1, 12, 5),
                    nn.MaxPool2d(2),
                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),
                    nn.Conv2d(12, 64, 5),
                    nn.MaxPool2d(2),
                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),
                    nn.Flatten(),
                    nn.Linear(64*4*4, 10),
                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)
                    ).to(device)
```

`init_hidden`参数初始化神经元的隐藏状态(这里是膜电位)。这在后台作为一个实例变量进行。

如果激活`init_hidden`，膜电位不会显式返回给用户，确保只有输出的峰值顺序地通过`nn.Sequential`包裹的层。

要使用最后一层的膜电位来训练模型，设置参数`output=True`。

这使得最后一层同时返回神经元的脉冲和膜电位响应。

## 2.3 前向传播
模拟时长为`num_steps`的正向传递如下所示:

```text
data, targets = next(iter(train_loader))
data = data.to(device)
targets = targets.to(device)

for step in range(num_steps):
    spk_out, mem_out = net(data)
```

将其包装在一个函数中，记录随时间变化的膜电位和脉冲响应:

```text
def forward_pass(net, num_steps, data):
  mem_rec = []
  spk_rec = []
  utils.reset(net)  # resets hidden states for all LIF neurons in net

  for step in range(num_steps):
      spk_out, mem_out = net(data)
      spk_rec.append(spk_out)
      mem_rec.append(mem_out)
  
  return torch.stack(spk_rec), torch.stack(mem_rec)

spk_rec, mem_rec = forward_pass(net, num_steps, data)
```

# 3.训练循环
## 3.1 使用snn.Functional 表示损失
上一篇教程中，我们使用输出神经元的膜电位与目标之间的交叉熵损失来训练网络。这一次，将使用每个神经元的脉冲总数来计算交叉熵。

`snn.Functional`模块中包含各类损失函数，实现上使用了`torch.nn.Functional`模拟尖峰。这些实现了交叉熵和均方误差损失的混合，应用于脉冲和膜电位，以训练频率或延迟编码网络。

下面的方法将交叉熵损失应用于输出脉冲计数，以训练频率编码网络：

```text
# already imported snntorch.functional as SF 
loss_fn = SF.ce_rate_loss()
```

```text
loss_val = loss_fn(spk_rec, targets)

print(f"The loss from an untrained network is {loss_val.item():.3f}")
```

输出如下

```text
The loss from an untrained network is 2.303
```

## 3.2 使用snn.Functional表示正确率
`SF.accuracy_rate()`函数的工作方式类似，预测的输出峰值和实际目标都作为参数提供。`accuracy_rate`假设一个速率代码用于通过检查具有最高脉冲计数的神经元的索引是否与目标索引匹配来解释输出。
```text
acc = SF.accuracy_rate(spk_rec, targets)

print(f"The accuracy of a single batch using an untrained network is {acc*100:.3f}%")
```
上面的函数只返回单个批次数据的精度，下面的函数返回整个DataLoader对象的精度:
```text
def batch_accuracy(train_loader, net, num_steps):
  with torch.no_grad():
    total = 0
    acc = 0
    net.eval()
    
    train_loader = iter(train_loader)
    for data, targets in train_loader:
      data = data.to(device)
      targets = targets.to(device)
      spk_rec, _ = forward_pass(net, num_steps, data)

      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)
      total += spk_rec.size(1)

  return acc/total

test_acc = batch_accuracy(test_loader, net, num_steps)

print(f"The total accuracy on the test set is: {test_acc * 100:.2f}%")
```

## 3.3使用snn.backprop进行自动化训练
即使是简单的网络，训练snn也会变得很困难，即使是一个很简单的网络。因此`snn.backprop`模块就是为了减少这种工作量。

`backprop.BPTT`函数自动执行单个epoch的训练，其中你只需要提供训练参数、数据加载器和其他几个参数。

返回每次迭代的平均损失。参数`time_var`表示输入数据是否时变。因为我们使用MNIST数据集，所以我们明确指定`time_var=False`。

下面的代码块可能需要一段时间才能运行。如果你没有连接到GPU，那么考虑减少`num_epochs`。

```text
optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999))
num_epochs = 10
test_acc_hist = []

# training loop
for epoch in range(num_epochs):

    avg_loss = backprop.BPTT(net, train_loader, optimizer=optimizer, criterion=loss_fn, 
                            num_steps=num_steps, time_var=False, device=device)
    
    print(f"Epoch {epoch}, Train Loss: {avg_loss.item():.2f}")

    # Test set accuracy
    test_acc = batch_accuracy(test_loader, net, num_steps)
    test_acc_hist.append(test_acc)

    print(f"Epoch {epoch}, Test Acc: {test_acc * 100:.2f}%\n")
```


# 神经网络中的Epoch、Iteration、Batchsize
神经网络中epoch与iteration是不相等的

- batchsize：中文翻译为批大小（批尺寸）。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；

- iteration：中文翻译为迭代，1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过

- epoch：迭代次数，1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递

举个例子，训练集有1000个样本，batchsize=10，那么：训练完整个样本集需要：100次iteration，1次epoch。


# CNN本质和优势

局部卷积（提取局部特征）

权值共享（降低训练难度）

Pooling（降维，将低层次组合为高层次的特征）

多层次结构

# 鞍点的定义和特点？
- [ ]  TODO

# 神经网络数据预处理方法有哪些？


# 神经网络怎样进行参数初始化

# 卷积

# 卷积的的反向传播过程

# CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？



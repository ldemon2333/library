在 "Efficient Large-Scale Language Model Training on GPU Clusters" 论文中， NVIDIA 介绍了分布式训练超大规模模型的三种必须的并行技术：

- 数据并行（Data Parallelism）
- 模型并行（Tensor Model Parallelism）
- 流水并行（Pipeline Model Parallelism）


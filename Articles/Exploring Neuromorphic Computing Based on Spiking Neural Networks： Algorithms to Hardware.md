神经形态计算是 20 世纪 80 年代末期开创的一个概念，由于有望降低人工神经网络的计算能耗、延迟和学习复杂性，最近受到了广泛关注。受神经科学的启发，这个跨学科领域通过提供端到端方法来实现机器智能中类似大脑的效率，从而跨设备、电路和算法进行多堆栈优化。一方面，神经形态计算引入了一种新的算法范式，称为脉冲神经网络 (SNN)，这与标准深度学习有很大不同，它以脉冲（“1”或“0”）而不是模拟值的形式传输信息。这开辟了新的算法研究方向，以制定在脉冲序列中表示数据的方法，开发可以随时间处理信息的神经元模型，为事件驱动的动态系统设计学习算法，并设计适合稀疏、异步、事件驱动计算的网络架构，以实现更低的功耗。另一方面，平行研究重点是开发用于新算法的高效计算平台。适用于深度学习工作负载的标准加速器并不特别适合高效处理跨多个时间步的处理。为此，研究人员设计了依赖于事件驱动的稀疏计算以及高效矩阵运算的神经形态硬件。虽然大多数大规模神经形态系统都是基于 CMOS 技术进行探索的，但最近，非易失性存储器 (NVM) 技术有望在单个设备上实现仿生功能。在本文中，我们概述了基于脉冲神经网络 (SNN) 的神经形态计算在最近取得的几项进展，并提出了我们对该领域需要克服的挑战的展望，以使生物可信度路线取得成功。

# 1.介绍
在开创性的著作《计算机与大脑》[241]中，约翰·冯·诺依曼讨论了如何将大脑视为一台计算机。从那时起，有大量的工作试图用类似大脑的架构来实现类似大脑的功能。神经网络，特别是深度学习，为当今无处不在的人工智能时代提供了动力，展示了前所未有的成功，甚至在几项认知任务中超越了人类[116，214]。但代价是什么？

虽然我们最快的并行计算机已经实现了深度学习，但它们的主要限制在于它们在计算和内存之间移动数据的能力，这与大脑的大规模并行、稀疏、事件驱动、分布式处理能力形成鲜明对比。因此，大脑和深度学习架构之间存在巨大的能效差距。随着传感器内分析和物联网 (IoT) 时代的任务复杂性不断增加，以及为此类任务部署的网络规模不断扩大，在功率、能源和​​计算预算有限的边缘设备中实现和训练此类深度神经网络已成为一项艰巨的任务。

神经形态计算的底层计算范式是一门新兴的人工神经网络学科，它试图基于大脑中的神经元“脉冲”或放电事件，在时间和分布式方面模拟神经元和突触功能 [47, 70]。这些网络被称为脉冲神经网络 (SNN) [133]，它们可以实现稀疏、事件驱动的神经元计算和时间编码——这与标准的深度学习网络（称为模拟神经网络 (ANN)）不同，后者处理和传输的是逻辑上模拟的信息，而不是全有或全无的脉冲。

由于 SNN 的独特功能，需要探索新的算法方向，以适应其隐式递归、事件驱动和稀疏计算特性。SNN 的设计是动态递归系统；脉冲神经元的内部状态整合了时间信息并保留了先前输入的历史记录。使用二进制信号训练递归网络加剧了梯度爆炸和消失的问题。虽然 SNN 的事件驱动特性为实现智能硬件的低能耗和低功耗提供了一条有希望的途径，但它也对其学习能力构成了关键限制。将脉冲神经元/突触的时间编码统计数据与标准的基于梯度下降的学习算法（适用于不及时编码信息的 ANN）相结合带来了一些挑战 [123, 265]。很难以端到端的方式全局训练深度 SNN 架构的各层 [181]。生物可信的无监督 [52] 和监督 [99] 学习已被探索为一种可行的解决方案，它允许局部学习，并且已被证明在计算上比基于反向传播的算法更有效。虽然用无监督学习算法训练的 SNN 还不适合具有挑战性的认知任务，但它们在聚类和从图像中提取低级特征以进行识别方面找到了应用。人们也大力发展可扩展的基于梯度的算法，这些算法可以适应 SNN 中事件驱动的稀疏活动 [16, 166]。总的来说，这些算法驱动有望将 SNN 的性能扩展到 ANN 目前提供的水平，同时保留基于稀疏事件的计算的好处。此外，目前的算法没有充分利用典型的“时间”参数，未来的研究可以探索这些机会，同时实施额外的生物可信功能，以进一步提高 SNN 的性能。

并行研究重点是为不断发展的 SNN 算法和工作负载开发高效的计算平台。SNN 工作负载的硬件从神经形态工程的广阔领域中获得了一定的动力。神经形态工程的概念最初由 Carver Mead 在他的开创性著作 [144, 145] 中提出，他在书中探讨了模拟电路的概念，以模拟大脑中复杂的神经元和突触功能。后来，Mahowald 等人 [138] 在实现硅神经元 [139] 和硅视网膜的开创性工作中证明了这一点。因此，已经实现了各种神经形态芯片，例如 ROLLS [187]、Dynap-SEL [154]、Neurogrid [21]、SpiNNaker [171] 等。这些实现旨在通过 CMOS 电路模拟大脑神经元和突触的生物物理学。 SNN 工作负载倾向于使用更简单的神经突触模型，并且可以从上述深入研究在芯片上模拟类似大脑的特征的工作中获得灵感。除了神经突触模型之外，SNN 硬件的另一个关键组成部分是加速 SNN 工作负载中的计算。过去几年，此类 SNN 加速器的要求发生了重大变化，特别是在设计有效利用 SNN 算法关键特性的大型系统方面。旨在更有效地执行标准 ANN 工作负载的加速器，例如通用图形处理单元 (GPGPU) 和张量处理单元 (TPU) [98]，并非旨在优化跨多个时间步的处理。神经形态硬件架构从 SNN 的两个基本原理中汲取灵感：(i) 事件驱动的稀疏计算和 (ii) 高效和并行矩阵运算。虽然 ANN 加速器旨在高效执行矩阵运算，但它们未能利用 SNN 中的时间稀疏性。时间稀疏性启发了各种架构，例如 TrueNorth [6] 和 Loihi [46]，它们部署了异步计算系统以减少计算和数据通信。第二个特征对神经网络来说更为通用，并且已经监督了领域特定加速器的重大发展 [5, 163]，这些加速器具有交织的内存和计算元素，以克服冯诺依曼瓶颈。虽然大多数大型 SNN 加速器都基于 CMOS 技术，但几种非易失性存储器 (NVM) 技术已成为构建仿生设备的有希望的候选者 [31, 37, 269]。此类设备可以一对一地模拟神经元 [172, 206, 234] 和突触功能 [28, 84, 97, 111, 185, 224, 226]，同时实现“内存”计算的新范式，即原位突触计算 [262]，以加速 SNN 工作负载 [11, 216]。因此，需要在多学科研究中共同设计神经形态算法和底层计算架构 [88]。

本文概述了神经形态工程领域中脉冲神经网络领域的最新发展，这些发展推动了算法和硬件的研究方向。我们是否能够在机器智能平台中实现人们一直难以实现的能效，这是一个目前难以回答的问题。然而，我们认为，以统一的硬件-软件视角重新思考脑启发计算对于实现计算效率学习至关重要。结合不同领域的观点——计算神经科学、机器学习、材料、设备、电路和架构——将有助于概述这些不同观点所带来的挑战，并为具有与大脑类似的功率和能效的智能平台提供未来方向。

# 2.算法
在本节中，我们将深入探讨 SNN 的算法基础。神经元建模、输入编码、学习算法和网络架构的重大发展塑造了神经形态计算的进步。由于输入以时间域信息表示，因此需要对目前用于 ANN 的训练算法进行大量重新思考。


## 2.1 神经元模型
已经提出了具有不同程度生物保真度的神经元模型来模拟生物神经元的动态[69]。一些更简单和流行的模型是积分和发射（IF）、泄漏积分和发射（LIF）和尖峰响应模型（SRM），它们广泛用于图像识别任务的深度SNN [52, 96, 199, 209, 213]。更详细和复杂的HodginHuxley模型考虑了在生物神经元中观察到的不同离子通道的动态[83]。IF / LIF和SRM神经元模型可以从Hodgin-Huxley模型中推导出来[69]。虽然Hodgin-Huxley模型在生物学上更现实，但当前的优化算法（例如ANN-to-SNN转换和基于替代梯度的学习）在更简单的IF / LIF神经元模型中表现更好。大多数脉冲神经元模型有几个共同点：它们有一个内部状态，可以积累输入刺激；当内部状态超过阈值时，神经元会产生输出（或触发）；有时触发事件之后会有一个不应期，在此期间神经元处于休眠状态，不会对输入刺激做出反应。在这里，我们讨论了在复杂任务上表现出竞争力的流行 IF/LIF 模型和没有内部状态的随机神经元模型。有关其他脉冲神经元模型的详细信息，请参阅参考文献 [69]。

### 2.1.1 LIF/IF 神经元模型
该模型的动力学模型由微分方程来描述：
![[Pasted image 20241017101108.png]]

该方程表示当膜电位 ($u$) 低于阈值电位 ($v$) 时神经元的行为。膜电位随时间积分输入电流，当电位超过阈值电压时，神经元会触发。IF/LIF 是一个非常简单的模型，并不反映生物神经元的整体复杂动态。然而，它的简单性使其在深度学习框架中具有吸引力。对于 LIF 神经元，在没有输入刺激的情况下，积分是泄漏的，膜电位会随时间衰减（图 1）。触发事件有时会伴随着不应期，在此期间膜电位不会积分输入电流。这避免了特定神经元的过度触发，并允许其他神经元参与学习 [52]。触发后，膜电位会重置，并在不应期后返回其静止值（图 1）。

![[Pasted image 20241017101548.png]]

求解连续时间域微分方程（公式 (1)）可得到迭代更新规则 [254]，其中 $τ$ 通过积分映射到 $λ$，如下所示
![[Pasted image 20241017101706.png]]
其中 $u$ 是膜电位，下标 $i$ 和 $j$ 分别代表后神经元和前神经元，上标 $t$ 是时间步长，$λ$ 是常数（<1），负责膜电位泄漏，$w$ 是连接前神经元和后神经元的权重，$o$ 是输出脉冲，$v$ 是阈值电位。等式 (2) 中的第二项对输入进行积分，最后一项在触发后重置膜电位。重置机制将膜电位降低阈值，而不是将其重置为重置电位。这减少了信息丢失，并在图像分类任务中获得了更好的性能 [77]。大多数 LIF 模型的硬件实现都采用接地重置，但是，很少有模型提供减法重置功能 [4]。

### 2.1.2 随机神经元模型
上面讨论的确定性 IF/LIF 神经元模型在膜电位超过阈值电压时发出尖峰。如果膜电位在任何时间步长低于阈值，则下一个时间步长的计算将从前一个膜电位开始。相反，概率神经元模型随机激发，并且在特定时间激发的概率是加权输入瞬时幅度的非线性函数 [19, 206, 242]。后神经元激发的概率定义为
![[Pasted image 20241017102008.png]]
其中 $o_j$ 是来自前神经元的脉冲输入（1 或 0），而 $w_{ij}$ 是连接前神经元和后神经元的突触权重。在没有任何输入活动和偏差（$\sum_j o_j = 0$）的情况下，触发概率为 0.5。对具有随机神经元的网络进行多次迭代评估，并将层的输出计算为所有迭代的平均脉冲数。然后可以将平均模拟值进行速率编码（有关更多信息，请参见第 2.2.1 节）作为泊松脉冲序列作为下一层输入。这些随机模型很难在硬件中实现。

## 2.2 输入编码
SNN 通过二进制信号（尖峰）计算和传递信息。因此，模拟输入（例如图像像素或实数）需要以二进制信号编码。单个尖峰是一个离散事件，表示为“+1”或“-1”，模拟值则编码为一组尖峰。编码机制决定了用尖峰表示模拟值时的量化或转换误差。模拟值由分布在一段时间内的一个或多个尖峰表示。流行的编码方法是频率编码 [52, 209]、时间编码 [155] 和显式训练编码层 [193, 255]。编码方法如图 2 所示。
![[Pasted image 20241017102424.png]]

### 2.2.1 Rate Coding
在频率编码中，信息以一个时间段内神经元的平均发放率表示。单个脉冲的时间无关紧要。在泊松编码（一种频率编码）中，在每个时间步长上，将归一化的模拟值与随机数进行比较。如果模拟值较大，则神经元发放（输出“1”），否则保持不活动状态（输出“0”）。时间步长1 的数量决定了脉冲序列表示模拟值的离散化误差。这导致采用大量时间步长来获得高精度，但代价是高推理延迟 [209]。由于每个脉冲中的信息内容很少，因此频率编码效率低下。


### 2.2.2 时间编码
与频率编码不同，在时间编码中，单个脉冲的定时至关重要，因为信息是在时间实例中编码的。对数时间编码 [267] 将模拟值编码为具有预定义位数的二进制数。位数充当时间的代理，并为二进制数中的每个活动位生成一个脉冲。对于更稀疏的表示，仅为最高有效位生成脉冲。秩顺序编码 [48] 将信息表示为触发实例的顺序，而不是使用脉冲的精确时间。编码较大模拟值的输入神经元比编码较小值的神经元更早触发。首次脉冲时间 [156, 198] 是秩顺序编码的一种形式，它限制每个神经元只能产生一次脉冲。脉冲的顺序或时间与被编码的模拟值成反比。在时间开关编码 [76] 中，模拟值使用两个脉冲进行编码，脉冲之间的时间差与编码值成正比。它实现了更好的能源效率，因为每个突触最多执行两次内存访问和两次加法计算。虽然时间编码方法可以用较少的脉冲对信息进行编码，但与频率编码的 SNN [198] 相比，时间编码 SNN 缺乏适当的训练算法，导致性能不佳。

### 2.2.3 编码层
前面讨论过的频率和时间编码是基于固定公式的非参数化编码方法。或者，可以将输入编码作为训练过程的一部分，因此编码函数可以具有使用输入数据进行训练的参数。编码函数被建模为一个神经网络，它接收模拟值并生成脉冲序列。在某些情况下，这个网络就像一个卷积层一样简单 [132,193, 199]。编码层附加在 SNN 的前面，并与整个 SNN 进行端到端训练。编码层由 IF/LIF 神经元组成，它整合加权模拟值并生成脉冲序列。每个时间步长的编码层输入都是相同的模拟值。

### 2.2.4 基于事件的传感器
所有上述讨论的编码方法都旨在将标准相机捕获的原始图像帧编码为逐像素时间尖峰，以构建 SNN 的输入。但是，由于原始输入一开始就没有时间信息，因此这种时空表示不是很合理。通常用于捕获图像和视频信息的标准相机在现实世界和边缘应用中存在各种缺点。它们的低且固定的采样率使它们在捕捉高速运动时容易出现运动模糊。它们的低动态范围操作使它们无法在具有挑战性的照明条件下（例如低光和高动态范围 (HDR) 环境）捕获有意义的信息。此外，定期对整个图像帧进行采样会使它们在几乎静态的场景中一遍又一遍地捕获冗余信息，从而导致高功耗。这些缺点促使人们需要专门的传感模式，以便在具有挑战性的现实环境中高效运行，同时显著节能。

基于事件的传感器（例如 DVS128 [129]、DAVIS240 [29]、三星 DVS [217] 等）旨在通过提供对来自环境的视觉信息变化的异步感知来解决这些问题。事件相机也称为生物启发式硅视网膜，它异步且独立地检测每个像素元素的对数强度 ($I$) 变化，并在变化超过阈值 ($C$) 时生成尖峰事件：
![[Pasted image 20241017104858.png]]
这些相机在处理输入强度信号时采用阈值变化不敏感算法，而非标准异步 sigma-delta 调制。这会导致输出空闲，而输入强度没有变化。由于只监测和记录对数强度变化，因此可以实现非常低的功耗。由于与标准相机相比，事件相机的工作原理根本不同，因此事件相机提供极高的时间分辨率（10μs vs. 3ms）、高动态范围（120dB vs. 74dB）和低功耗（∼10mW vs.3W）。与标准相机相比，事件相机在实时人机界面系统、机器人、可穿戴电子产品或基于视觉的边缘设备等场景中具有优势，在这些场景中，在具有挑战性的光照条件下高效运行、低延迟和功耗[130]至关重要。它们还可应用于计算机视觉和机器人任务，如物体检测和跟踪 [151]、手势识别 [10]、光流/深度、自我运动估计 [260、270、272] 等。

基于事件的编码会产生异步且稀疏的时空数据，其中包含体素形式的结构和时间信息。这种类型的输入表示可以由异步事件驱动模型（例如 SNN）自然处理，这将在后面的章节中讨论。尽管如此，大多数使用事件相机的研究都是与传统计算机视觉方法或 ANN 结合进行的。这需要通过在一定时间间隔内累积事件并随后将其视为等同于图像帧来构建事件帧来代替图像帧（如标准相机）。例如，参考文献 [260, 270, 272] 等作品利用这些事件帧作为 ANN 的输入通道。这导致在累积间隔内丢失基本时间信息以及各个帧的时间顺序。虽然与使用标准相机的方法相比，它们显示出有希望的潜力，但它们仍然没有完全利用事件相机的基本优势。这是因为事件相机数据的异步和离散特性使其无法与依赖基于帧的信息的传统 ANN 原生形式兼容。此外，基于 ANN 的方法通常是为基于像素的图像设计的，遵循光一致性假设（物体的颜色和亮度在图像序列中保持不变）。某些作品（如参考文献 [22] 和 [23]）探索直接使用事件来估计视觉流，但它们在可扩展性方面受到限制，无法解决更复杂的问题。鉴于此，受生物神经元模型启发的 SNN 可以直接处理事件数据，提供异步计算，同时还利用丰富的时空动态和固有的输入稀疏性。此外，使用事件数据和 SNN 消除了基于 ANN 的方法所需的任何复杂尖峰编码阶段的需要。脉冲神经元的工作原理自然地为分布在 SNN 层上的基于事件的异步处理提供了兼容性，并且与 IBM 的 TrueNorth [6] 或英特尔的 Loihi [46] 等专用神经形态硬件上的计算相结合，提供了高能效。第 3.4 节讨论了一些将事件相机与 SNN 相结合的工作以及与之相关的好处和挑战。


## 2.3 前馈神经网络
前馈网络由多个卷积层和/或全连接层组成，其中信息从输入层流向输出层，神经元之间的连接不形成循环。在本节中，我们讨论了前馈 SNN 的各种无监督、监督和生物可信的局部学习规则。

### 2.3.1 无监督学习
无监督学习是指从未标记的数据中识别模式的算法。数据由输入（图像、音频、文本等）组成，没有相应的标签或目标。无监督学习适合在原始和未知数据中找到聚类。

脉冲时间依赖性可塑性：脉冲时间依赖性可塑性 (STDP) [25, 69, 101] 是一种无监督学习技术，是 SNN 中突触学习的一种生物学上可行的机制。基于 STDP 的学习规则 [218] 根据各自脉冲时间之间的相关程度修改连接一对突触前和突触后神经元的突触的权重，具体如下所述：
![[Pasted image 20241017105427.png]]
其中 $t_{pre}$ 和 $t_{post}$ 是一对前脉冲和后脉冲的时间瞬间，$A_+$、$A_−$、$τ_+$ 和 $τ_−$ 是控制突触权重变化 $Δw$ 的学习率和时间常数。STDP 规则的另一种变体基于 $Δt$ 的正值执行增强和抑制（图 3(d)）[52, 192]。权重更新计算如下
![[Pasted image 20241017105625.png]]

![[Pasted image 20241017105610.png]]
其中 $Δw$ 是权重变化，$η$ 是学习率，$t_{pre}$ 和 $t_{post}$ 是突触前和突触后脉冲的时间瞬间，$τ$ 是时间常数，$o f f set$ 是用于抑制的常数，$w_{max}$ 是对突触权重施加的最大约束，$w$ 是先前的权重值，$μ$ 是控制对先前权重值的指数依赖性的常数。如果后神经元在前神经元之后立即出现脉冲，则权重更新为正（增强），如果脉冲相距较远，则权重更新为负（抑制）（图 3(d)）。响应脉冲对刺激，STDP 曲线有许多其他变体，包括脉冲三重峰和四重峰 [73]。STDP 独立调节每个突触的强度；虽然这非常强大，但也带来了稳定性问题。因此，参考文献 [3] 探讨了在整个网络中保持适当分布式活动水平的机制。在 STDP 中，有效突触得到加强，无效突触得到削弱，从而形成正反馈回路并导致稳定性问题。

除了非易失性 STDP（由于在没有尖峰刺激的情况下不存在任何遗忘机制，因此无法适应不同的学习场景）之外，突触学习机制（例如短期可塑性 (STP) 和长期增强 (LTP) [140, 274]）与短期和长期记忆的概念相似 [13, 113]）也已被用于在线学习，可以使突触权重适应动态变化的环境。灾难性遗忘是在持续学习中观察到的一种现象，在学习新任务时会丢失有关旧任务的知识，这是神经网络中普遍存在的问题，使用 STDP 训练的 SNN 也会受到此问题的影响 [173]。自适应权重衰减机制 [173] 可以逐渐忘记不太重要的权重并学习新信息，从而有效地学习新数据并避免灾难性遗忘。

由于 SNN 能够使用基于 STDP 的局部学习规则学习输入表示，因此人们正在积极探索 SNN 在无监督模式识别中的应用。SNN 的无监督特征学习能力最初是使用两层 SNN 来展示的，该两层 SNN 由一个完全连接到兴奋层（或输出层）神经元的输入层和一个抑制层组成（图 3）[52, 74]。然而，这种两层 SNN 的性能仍然仅限于手写数字识别等简单任务。卷积 SNN 的架构与深度学习 SNN [107] 类似，旨在解决两层 SNN 可扩展性有限的问题 [63, 103, 121, 141, 221, 223, 230, 232]。卷积层可以使用 STDP 进行训练，以无监督的方式学习分层输入表示，然后将其输入到使用监督学习规则训练的分类器进行推理。虽然 STDP 训练的卷积 SNN 比完全连接的两层 SNN 有所改进，但准确率仍然低于流行基准数据集上的最新性能。STDP 训练的卷积 SNN 面临的挑战是双重的。首先，这些 SNN 可以扩展到多深还有待观察，因为当前的网络仅限于少数卷积层。其次，仅凭 STDP 是否能够使更深的层学习复杂的输入表示尚不确定。STDP 在聚类和从图像中提取低级特征方面功能强大，但无法泛化组成高级特征，因此两层或更多层的 STDP 学习不会带来太大好处 [63]。在具有卷积层的分层学习中，需要将学习到的特征组合成可以执行识别的高阶特征。一些可以补充 STDP 特征提取的算法可以解决这些缺点。然而，如果这些巨大的挑战得到解决，那么 STDP 将实现新一代低成本（面积/功率/资源要求）、本地和无监督的学习框架，而不是依赖全局和监督学习技术的非脉冲对应物（ANN）。优点很多；无监督学习使网络能够适应不断变化的环境，而局部学习有助于实现低功耗学习电路原语。局部学习将消除对监督学习必不可少的全局误差分布的需求，而监督学习又需要昂贵的硬件电路。

随机STDP：前面描述的 STDP 算法适用于具有全精度权重的浅层网络（2-3 层）。然而，低精度网络（例如二进制权重）需要概率学习​​规则才能有效训练并避免在允许的级别之间快速切换权重。后神经元和前神经元的尖峰时间之差（$t_{post} −t_{pre}$）被映射到连接二进制权重的切换概率（图 4）[222]。如果时间差为正且低于某个阈值，则突触权重以恒定概率从低状态切换到高状态（赫布增强）。如果时间差为负且高于固定阈值，则权重以恒定概率从高状态切换到低状态（赫布抑制）。此外，如果时间差为正且高于某个值，则由于因果关系低，突触会被抑制（反赫布抑制）。参考文献 [222] 中的作者提到，反赫布学习使突触能够忘记位于接受域之外的特征，例如图像中的噪声背景。随机 STDP 学习规则支持使用二进制权重训练 SNN [188]，并且可以实现与使用 STDP 训练的全精度 SNN [52] 类似的精度，但内存要求较低 [222]。此外，参考文献 [119] 提出了一种使用 STDP 的半监督训练机制。使用以无监督方式训练的预训练 STDP 权重初始化网络。接下来，采用基于梯度的监督学习来微调权重并以更快的收敛速度提高精度。尽管基于 STDP 的局部学习规则尚不适用于大规模问题（如 ImageNet），但它们在节能聚类任务中表现相对较好。

![[Pasted image 20241017110330.png]]


### 2.3.2 监督学习
与不需要标记示例进行训练的无监督学习不同，监督学习的优势在于大量标记示例的语料库。在监督学习中，网络接收输入（图像、文本或音频）并为输入可能属于的所有标签（即数据集中的类别数）生成预测分数。将预测与真实标签（one-hot 向量，正确类别为“1”，其他类别为“0”）进行比较以确定误差/损失，并根据损失函数相对于参数的梯度更新网络参数（权重、偏差等）。通常，在 ANN 中，如果有大量标记样本可用，监督方法会表现得非常好。然而，由于脉冲神经元的不连续性和不可微性，在 SNN 中直接实施基于梯度的方法具有挑战性。为了解决这个问题，研究人员提出了将训练好的 ANN 转换为 SNN 进行推理的方法 [34, 53, 199, 209]。此外，使用将不连续导数近似为连续函数的替代梯度来训练具有端到端反向传播的 SNN [16, 166, 213]。最近，使用 ANN 到 SNN 的转换和基于替代梯度的学习相结合来训练深度 SNN 以完成图像识别任务 [193, 194]。此外，还提出了从突触中存在的局部信息中学习的生物合理方法，以实现高效训练 [165, 202]。

**ANN-to-SNN 转换**：尽管使用局部学习规则训练的 SNN 对硬件友好且节能，但它们在具有挑战性的数据集上的准确度不够理想 [63,174]；而使用转换框架训练的 SNN 可实现与 ANN 相似的准确度 [34, 53, 85, 199, 209]。在 ANN 到 SNN 的转换中，首先使用最先进的监督算法训练具有 ReLU 神经元的 ANN，并施加一些限制（无偏差、平均池化、无批量归一化），尽管一些研究表明可以放宽某些限制 [199]。接下来，使用训练后的 ANN 的权重初始化具有 IF 神经元和 iso 架构的 SNN 作为 ANN。该过程的基本原理是可以将 ReLU 神经元映射到 IF 神经元，同时将转换损失降至最低。映射是通过调整 IF 神经元的触发阈值来执行的，因此其平均触发率与 ReLU 神经元的激活相似（图 5）。该方法的主要瓶颈是确定可以平衡准确度-延迟权衡的 IF 神经元的触发阈值。通常，阈值被计算为 IF 神经元的最大预激活，从而以高推理延迟（约 1,000 个时间步）为代价获得高推理准确度 [209]。在最近的研究中，作者表明，与使用最大预激活相比，预激活分布的某个百分位数可以减少推理延迟（60-80 个时间步），同时准确度下降最小 [132]。参考文献 [199, 209] 中的研究人员已经展示了在标准深度学习架构（例如 VGG [215]、ResNet [79] 和 Inception [228]）上使用转换方法训练的深度 SNN，在 ImageNet [200] 等复杂数据集上表现出最先进的性能。提出了一种在脉冲事件时保留残余膜电位的“软复位”机制（第 2.1 节），以进一步减少 ANN 到 SNN 转换框架中的转换损失 [77]。与 ANN 相比，具有稀疏、事件驱动计算的深度架构可以潜在地降低能耗，因为神经元脉冲稀疏性会随着网络深度的增加而急剧增加 [209]。虽然转换框架确立了基于脉冲的推理的有效性，并作为概念证明表明 SNN 具有与 ANN 相当的计算能力，但它有一个主要缺点：缺乏时间信息。转换过程中未使用典型参数“时间”，这会导致更高的推理延迟。下面介绍的基于脉冲的梯度下降方法使用时间反向传播 (BPTT) 执行权重分配，并通过结合时间信息实现更低的推理延迟。

**基于脉冲反向传播**：ANN 主要使用基于梯度下降的方法进行训练，该方法基于损失函数相对于参数 ($∂L/∂W$) 的偏导数来更新网络参数（权重、偏差等）。在 SNN 中，IF/LIF 神经元的脉冲激活函数没有连续导数。脉冲函数 (Dirac-delta) 的导数在脉冲时未定义，否则为“0”。为了解决这个问题，提出了脉冲函数的替代梯度或伪导数（图 6），将真实梯度近似为连续函数 [16, 166, 213]。由于 SNN 在前向传递中计算多个时间步，因此梯度是通过及时展开网络并执行 BPTT 来计算的。最近的几项研究已将基于尖峰的梯度下降学习应用于 SNN 以执行各种分类任务 [16, 96,119, 120, 123, 155, 165, 166, 254]。与 ANN-to-SNN 转换网络相比，使用基于尖峰的梯度下降训练的 SNN 可以实现更低的推理延迟（图 7）。主要原因是训练中整合了时间信息，而 ANN-to-SNN 转换过程中则缺乏这些信息。因此，基于梯度下降的训练实现了更好的延迟，但需要更多的训练工作（计算和内存）。ANN 中的单个前馈传递对应于 SNN 中的多个前向传递，其与时间步数成比例。此外，后向传递要求将梯度积分到总时间步数上，这增加了计算和内存复杂性。多次迭代训练工作和爆炸式增长的内存需求限制了基于脉冲的反向传播方法在简单的几层卷积架构上对小型数据集（如 MNIST 和 CIFAR10）的适用性。
![[Pasted image 20241018132655.png]]
![[Pasted image 20241018133135.png]]

参考文献 [16] 中的研究人员表明，抑制替代梯度可以提高较大时间跨度内梯度下降的性能。此外，参考文献 [166] 还提供了一项比较研究，研究了在 SNN 中使用各种替代梯度作为不连续实梯度的近似值来执行梯度下降。

**Hybrid learning(混合学习)**：到目前为止，梯度下降算法的有效性已在MNIST [117] 和 CIFAR 数据集 [106] 上得到验证，适用于层数较少的 SNN。监督算法的可扩展性及其实现更深的 SNN 训练收敛的能力仍然是一个挑战（因为这需要误差反向传播，其中神经元以时间脉冲序列的形式产生输出）。此外，与转换框架相比，在 SNN 中执行梯度下降的内存要求更高，因为计算次数随时间线性增加。为了解决梯度下降方法的可扩展性问题，参考文献 [194] 提出了一种混合训练机制（图 8），该机制解决了高延迟（转换框架）和高训练成本（SNN 中的梯度下降）的问题。他们首先使用参考文献 [209] 中的框架执行 ANN 到 SNN 的转换，然后在 SNN 中使用替代梯度进行基于梯度下降的训练（算法 1）。转换框架起到了良好的初始化作用，梯度下降在几个时期内收敛到一个良好的设置。通过训练泄漏和阈值以及网络权重，可以提高图像分类任务的推理延迟和准确性[193]。
![[Pasted image 20241018134356.png]]

**生物可信的局部学习**：前面描述的基于脉冲的梯度方法实现了良好的精度，但全局损失从输出层反向传播到输入层会导致显着的内存开销。然而，基于 STDP 的无监督学习规则（第 2.3.1 节）具有计算和内存效率，但没有达到具有竞争力的精度。此外，局部学习规则与事件驱动的神经形态硬件更兼容（第 4 节将对此进行更多介绍）。因此，研究人员探索了基于梯度的方法，这些方法不需要端到端反向传播并根据局部信息执行权重更新。通常，全局误差 ($E$) 是最终层 ($O_L$) 的输出和目标 ($Y$) 的函数
$$
E = f(O_L,Y)\tag{8}
$$
隐藏层$W_l$的梯度计算为：
![[Pasted image 20241018133804.png]]
其中$U_l$是膜电位。术语$∂E/∂O_l$ 是反向传播误差，由所有下游突触权重计算得出；其他两个术语基于局部信息。基于生物可信局部学习的工作消除了对这个反向传播项的依赖，以执行权重更新。赫布三因素学习规则[66]描述了一种基于突触前活动、突触后变量和神经调节剂局部更新参数的方法。在SNN的背景下，神经调节剂被局部误差信号取代[17,99,157,264]。这些学习规则可以通过为每个突触设计资格轨迹在硬件中实现[49]。DECOLLE是一个配备局部误差函数来执行深度连续局部学习的SNN，就是这样一个例子[99]。网络的每一层都附加了一个随机读出层，并在读出层上定义一个辅助成本函数。通过将激活与随机固定矩阵相乘可获得随机读出。训练过程不是最小化全局目标函数，而是最小化许多局部成本函数 [157]。每层的权重更新仅取决于本地可用的信息，因此网络中的所有层都可以并行训练。

## 2.4 循环网络
在前面讨论过的前馈网络中，一个神经元总是连接到另一个神经元，并且没有任何自连接或循环。此外，同一层中的神经元之间没有连接；一层中的神经元连接到其他层中的神经元。相反，循环网络具有反馈连接，其中神经元的输出被路由回输入，并带有时间延迟。因此，输出是当前输入和神经元先前状态的函数。SNN 隐含地具有这种关系，因为膜电位取决于输入和前一时间步的电位（等式 (2)）。因此，SNN 通过其内部状态具有隐式递归 [166, 252]，如图 9 所示。但请注意，在本节中，我们讨论了脉冲神经元的循环网络 (RSNN)，它们在内在的循环动力学之上具有显式的循环连接。在 ANN 中，循环连接对于学习和生成具有长程结构的序列（例如文本、视频或音频数据）尤为重要。RNN 的连接结构中包含反馈回路，这使网络能够记住先前的计算或输入的历史记录 [57, 116, 179, 227]。RNN 中的计算是通过在离散时间步长上展开网络来完成的。展开后，RNN 可以被视为具有共享权重的深度前馈网络，能够建立长程时间依赖性。然而，由于梯度爆炸或消失会降低整体学习效果，因此训练 RNN 比训练深度前馈网络要困难得多 [20, 81]。如今，循环神经网络 (RNN)，特别是长短期记忆 (LSTM) 网络 [82]，被广泛用于处理语言或语音等序列输入。
![[Pasted image 20241018134225.png]]

众所周知，生物大脑由数百万个神经元组成的网络，这些神经元通过数十亿个循环突触连接而连接在一起。这些神经元连接不是随机确定的，而是针对特定任务进行了优化，并且经过了长期进化而发展起来的。RSNN 试图实现这种大脑神经元模型。这包括探索模型中的连通性以及学习突触权重。RSNN 旨在提供比 RNN 更高的能效，因为计算本质上是稀疏和离散的，以尖峰的形式出现。然而，由于与训练相关的挑战，该领域的研究受到极大限制。与经过同等训练的 RNN 相比，RSNN 的性能通常不是最优的，因此它们的应用仅限于简单的任务。

多年来，开发用于实现 RSNN 中生物可信学习的算法的研究一直是一个悬而未决的问题，但收效甚微。过去几年，有几项研究旨在以生物可信的方式训练 RSNN 模型 [7, 71, 148, 235]。这些研究旨在通过基于反馈的局部学习规则学习 RSNN 的非线性动力学。参考文献 [71] 中的作者提出了 FOLLOW（基于反馈的在线局部权重学习），这是一种局部学习算法，其中权重的变化取决于突触前活动和突触后神经元的投影误差。参考文献 [235] 建议除了突触前和突触后活动之外，还加入第三个局部树突电位因素来调节可塑性。他们利用一种功能规则来尽量减少体细胞放电和局部树突电位之间的差异。然而，参考文献 [148] 引入了一种称为深度反馈控制 (DFC) 的学习方法，该方法使用反馈控制器驱动神经网络朝着所需的目标输出方向前进，同时其控制信号用于信用分配。参考文献 [51] 中的研究人员探索使用适当的“发放率”模型来训练 RSNN，以利用连续变量网络识别训练目标来生成复杂的时间输出。此外，参考文献 [100, 149, 150] 中的作者探索了与视觉运动感知和场景理解的神经电路设计相关的基于事件的解决方案。

与基于替代梯度的 BPTT 相比，参考文献 [18] 中的作者提出了“e-prop”，这是一种生物学上现实的方法，基于最小化尖峰相关损失函数 (E)，该函数测量实际神经元输出与目标输出之间的差异。结果表明，在合适的伪导数的帮助下，关于突触权重的损失导数可以表示为 RSNN 计算时间步长的乘积之和，从而提供足够强大的函数来实现 RSNN 模型中的学习。结果表明，与 A3C [152] 算法相比，“eprop”在表型识别和 Atari 游戏强化学习等任务上的学习效果几乎与基于 BPTT 的方法一样好

### 2.4.1 Spiking LSTMs
与传统 LSTM 一样，脉冲 LSTM 也可用于序列建模任务，例如自然语言和语音处理、时间序列预测等。然而，训练这些脉冲 LSTM 再次遭遇与 SNN 相关的缺点，如前所述。为了缓解这些问题并实现训练，需要开发巧妙的方法。参考文献 [131] 中的作者探索了近似损失函数来计算梯度并在脉冲 LSTM 中实现时间反向传播 (BPTT)。他们评估了 MNIST [117] 和扩展 MNIST 数据集的顺序版本的性能，同时与其他前馈 SNN 进行了比较。然而，参考文献 [182] 中的作者提出了一种混合模拟和脉冲 LSTM，其中 LSTM 的计算密集型部分被转换为 SNN，以提高边缘设备的能效。结果显示，在 MNIST [117] 数据集上的序列图像分类和 IWSLT14 [36] 数据集上的序列到序列转换的性能下降可以忽略不计，同时能耗显著降低。

### 2.4.2 Liquid State Machines
液态机 (LSM) 已被探索为一种轻量级架构，用于以生物合理的方式处理时空输入 [134, 136]。LSM 是一种 RSNN，由兴奋性和抑制性脉冲神经元的稀疏连接储存器（液体）组成。突触连接及其权重是随机初始化的，并先验固定。这导致了一个简单的轻量级结构，同时仍然固有地捕获时空输入信息。液体本质上将输入投射到更高维空间，同时还通过循环连接保留时间信息。给定一个具有随机和稀疏互连的足够大的储存器，可以使用完全连接的读出层对生成的高维表示进行线性分类。Maass 和 Markram 在 2004 年的工作 [135] 研究了 LSM 在实时计算中的计算能力。图 10(a) 显示了由具有随机连接的脉冲神经元（兴奋性和抑制性）组成的 vanilla LSM。一些研究已经成功地将 LSM 用于各种应用，包括手势识别 [175]、视频活动识别 [219]、强化学习 [124, 183] 等，而且计算成本较低。LSM 的主要挑战在于，如果不大幅增加储存器的大小，它就无法很好地扩展到现实生活中的复杂计算任务。人们已经做出了各种努力来提高 LSM 的学习能力，而不会显著增加其尺寸。一种方法是探索训练液体连接的机制，以提高应用准确性，但代价是增加复杂性。参考文献 [258] 中的作者探索使用具有不同行为和液体兴奋程度的异质神经元来帮助学习。参考文献 [175] 采用驱动/自主模型方法 [2]，结合递归最小二乘 (RLS) 规则和 FORCE 训练 [167] 来训练液体连接。这种方法由两种不同类型的突触连接组成，即固定快速连接（τf ast）和可学习慢速连接（τslow = 10 ∗ τf ast）。如图 10(b) 所示。其他努力则侧重于优化网络架构本身。参考文献 [220] 中的作者建议采用“分而学”策略，利用多种小液体，每种液体学习对应于一段输入模式的特征模式。此外，输入到集合的连接使用基于 STDP 的学习规则进行训练。使用全局分类器将液体集合的中间输出组合起来，生成最终输出。这种方法如图 10(c) 所示。然而，参考文献 [219] 中的作者提出了一种涉及多层液体的架构，以形成深层分层 LSM。隐藏层（液体）使用尖峰赢家通吃编码器连接，以提取和传播时间特征。不同隐藏层生成的表示（液体）在读出时使用注意函数进行压缩，然后进行最终分类。图 10(d) 演示了这种方法。与单个大液体相比，这些方法提供了具有竞争力的准确性和更快的训练时间。与这些类似，参考文献 [162] 提出了一种有效的分区方法，用于在可重构神经形态硬件上对大型 SNN 进行分层映射。

因此，脉冲 LSTM 和 LSM 形式的脉冲神经元循环网络有望在资源受限的边缘设备上实现节能的现实应用。然而，与 ANN 相比，该领域的进步并不快，因为它们的学习能力有限。

## 2.5 神经形态APIs 和库
涉及不同脉冲神经元模型和架构的神经形态系统要求对最初针对 ANN 进行优化的标准软件和库进行范式转变，例如 Pytorch、Tensorflow、Caffe 等。这是因为 SNN 架构所需的异步事件驱动处理不能直接使用传统图形处理单元 (GPU) 上的上述软件 API 实现。这不仅需要在硬件端进行彻底改造以开发神经形态硬件，例如英特尔的 Loihi [46]、IBM 的 TrueNorth [6]、SpiNNaker [171] 等，如第 4 节将更详细讨论的那样，还需要开发能够处理这些事件驱动计算的专用软件。朝着这个方向，英特尔在推出其全新升级的 Loihi-2 的同时，还推出了其 LAVA 软件框架 [45]。 LAVA 满足了对通用神经形态软件框架的需求，允许研究人员和开发人员利用一套通用的工具、方法和库，并在传统和神经形态硬件的异构架构上无缝运行神经网络模型。此外，它还支持在不访问专门的神经形态硬件的情况下开发应用程序。接下来将讨论这些神经形态系统所针对的应用程序类型。

# 3. 应用
由于 SNN 具有固有的重复性，因此非常适合处理静态和顺序数据。此外，SNN 可以自然地处理来自事件传感器的离散时空数据。在本节中，我们将讨论 SNN 在图像分类、手势识别、情绪分析、生物医学应用和运动估计中的应用。此外，我们还回顾了 SNN 作为对抗攻击防御机制的相关性。

## 3.1 图像分类
表 1 比较了各种 SNN 模型在基于帧的图像数据集（MNIST [117]、CIFAR10 [106]、ImageNet [50]）以及神经形态数据集（N-MNIST [170]、CIFAR10-DVS [127]）的图像分类任务上的性能。基于像素的图像根据第 2.2 节中讨论的编码方法转换为脉冲训练。学习算法是第 2.3 节中描述的变体之一。在 SNN 中，挑战在于以最少的时间步长实现具有竞争力的准确度，从而实现更好的能效。为此，直接使用像素值作为输入并使用基于梯度的反向传播方法进行训练的网络可实现最佳的整体性能。神经形态-MNIST (N-MNIST) [170] 和 CIFAR10-DVS [127] 分别是 MNIST 和 CIFAR10 数据集的脉冲版本，用动态视觉传感器记录。

## 3.2 手势识别和情绪分析
顺序分类任务的输入之间存在一些时间依赖性。因此，它们需要能够在生成预测时考虑这种依赖性的模型。SNN 固有捕获时间信息的能力使它们与此类时间输入直接兼容，并适用于序列分类任务。参考文献 [10, 213, 256] 等作品探索在 IBM DVS 手势数据集 [10] 上使用深度 SNN 进行手势识别。他们的结果表明，SNN 表现出与相应的最新 ANN 实现相当的性能。参考文献 [213] 还在TIDIGITS 数据集 [125] 上执行音频分类。然而，参考文献 [4] 对来自 IMDB 数据集的电影评论执行情绪分析任务。该实现表明，当评论以序列形式呈现给SNN 时，神经元膜电位会跟踪情绪的积极/消极性质。

## 3.3 生物医学应用
现实世界中的生物信号和模式通常是随时间变化的信号，非常适合基于 SNN 的处理。为此，研究人员提出了几种分析和分类这些生物模式的解决方案。这些包括分析和解码脑电图 (EEG) [54, 105, 109, 168, 229]、心电图 (ECG) [259, 261]、肌电图 (EMG) [55, 68] 等信号的工作。此外，参考文献 [30, 67, 275] 中的作者表明，皮质脑电图 (ECoG) 记录中致痫组织产生的高频振荡 (HFO) 也可以使用 SNN 进行有效处理和分析。

## 3.4 运动估计
如前所述（第 2.2.4 节），基于事件的传感器的出现为 SNN 带来了有希望的机会，因为它具有固有的输入兼容性。边缘设备（例如小型地面和飞行机器人）在感知和规划任务方面从基于事件的处理中获得了巨大的好处。参考文献 [44] 中的作者证明了使用事件摄像机有效完成任务的重要性，例如在高速移动时检测和避障以及基于意识的环境感知。虽然有一些研究利用 ANN 和事件摄像机来完成此类任务 [270]，但它们在保留事件所包含的丰富时间信息方面效率低下。相比之下，SNN 通过自然捕捉事件摄像机数据的时间特性表现出固有的兼容性。参考文献 [169] 中的作者通过考虑生成运动敏感受体场时的突触延迟来演示使用 SNN 进行视觉运动估计。参考文献 [75] 介绍了基于 IBM TrueNorth [6] 的实时模型光流估计，用于简单的旋转螺旋和管道图案。此外，参考文献 [177] 中的作者使用基于 STDP 的学习训练的卷积 SNN 进行了光流估计。这些工作的主要局限性涉及使用小规模学习，对于动态和复杂的输入，这种学习不能很好地扩展。使用端到端反向传播训练深度 SNN 带来了新的挑战。深度 SNN 受到“尖峰消失”现象的影响，传播到后续层的尖峰数量急剧减少，阻碍了学习并导致性能不佳。为此，混合 SNN-ANN 架构似乎是一个有前途的替代方案，其中 SNN 层能够轻松处理事件数据，而 ANN 层能够进行端到端学习并保持应用程序性能。 Spike-FlowNet [118] 中的作者利用这种基于 U-Net [196] 模型的深度编码器-解码器架构对多车辆立体事件相机 (MVSEC) [271] 数据集进行光流估计。该数据集由使用一对立体事件相机记录的各种序列 (indoor_f lying1/2/3、outdoor_day1/2) 组成。该模型使用基于替代梯度的反向传播时间 (BPTT) 进行端到端训练，如算法 2 中突出显示的那样。图 11 突出显示了输入表示、混合 SNN-ANN 架构和定性结果，而表 2 将平均端点误差 (AEE) 与最先进的 ANN 实现 Ev-Flownet [270] 进行了比较。与 ANN 中的乘法-累积 (MAC) 运算相比，由于 SNN 中的累积 (AC) 运算，使用 SNN 可降低能耗。这些工作将 SNN 确立为现实世界感知和规划任务的主要候选者，从而使 SNN 和基于事件相机的计算机视觉成为一个活跃的研究领域。
![[Pasted image 20241020142016.png]]
![[Pasted image 20241020142024.png]]

然而，传统的基于计算机视觉的工作对于小规模输入同样非常高效，但对于较大的输入却不能提供类似的性能或良好的扩展性。最近的一项这样的工作是参考文献[142]，其中作者提出通过将来自事件传感器的时间事件建模为由像素地址和时间戳参数化的三维概率分布来估计光流。每个参数值的 Fisher-Rao 矩阵用于计算由对应于最小特征值的特征向量给出的光流。与 Ev-FlowNet [270] 和 SpikeFlownet [118] 相比，这项工作表现出了可观的性能，如表 2 所示。这些表明，受传统计算机视觉领域基本概念启发的基于学习的方法似乎在实现高性能感知和规划的同时保持效率方面具有很大的潜力。

## 3.5 对抗鲁棒性


# 4. 神经硬件
我们已经描述了神经形态计算如何呈现一种具有多种神经元功能以及突触学习算法的新范式。在本节中，我们将深入研究神经形态硬件的设计 [41, 231]，该硬件可以忠实地模拟算法功能并利用 SNN 提供的固有计算效率。

## 4.1神经形态硬件设计的动机
神经形态硬件设计最初的灵感来自于构建与人脑相当的电子系统，以模仿其计算能力 [143]。1991 年，Mahowald 和 Douglas 证明了这一点 [139]，他们利用硅神经元模拟集成电路和用于立体视觉的硅视网膜 [137] 实现了这一点。

近年来，人们对人工智能和机器学习系统的兴趣日益浓厚，由于工作负载的数据密集型特性，导致 GPU、TPU 等系统中此类工作负载的特定领域加速。然而，设计高效的神经形态硬件提出了需要解决的进一步挑战。神经形态计算工作负载（如 SNN）本质上是时间性的，即它在多个时间步内评估神经网络模型。此外，由于脉冲神经元的时间依赖性处理，几乎没有可以利用的时间并行性，而不会破坏处理流水线。因此，数据级并行性被限制在单个处理时间步内，当然可以通过 GPU 或 TPU 加速来利用。然而，SNN 工作负载往往表现出事件驱动的特性，即神经元异步处理数据，导致其在给定时间的激活相当稀疏。此外，在深层 SNN 中，神经元的脉冲活动可能会在更深的层中减少。GPU 和 TPU 等硬件系统旨在有效利用数据并行性，但它们未能利用 SNN 中激活的高时间稀疏性和空间稀疏性。此外，SNN 具有内存密集型数据结构（例如膜电位），需要跨时间步进行处理，而当今的数字加速器无法完全缓解这一开销。已经有一些关于使用模拟电容器作为神经元 [89] 中的存储元件进行“状态”处理的提议，这可以作为混合信号 SNN 加速器的良好设计选择。最后，SNN 的训练算法可能涉及跨时间的全局和局部权重更新，如第 2 节所述，这可能会进一步阻碍此类操作的有效执行。考虑到旨在加速机器学习工作负载的硬件系统的局限性，研究人员探索了从设备和电路到架构解决方案的多种设计方法 [6、11、21、31、46、89、208、231、262]，以应对神经形态计算工作负载（如 SNN）带来的独特挑战。在下一小节中，我们将讨论基本计算原语的智能设计如何形成加速 SNN 工作负载的平台。

## 4.2 神经形态架构
设计神经形态架构的关键方面涉及有效利用 SNN 中的时间和空间激活稀疏性的技术，以及优化基本计算架构，例如构建高效的功能原语和协调各种数据结构的通信。第一个挑战可以通过稀疏性驱动的优化来解决，其中包括内存和处理单元的条件激活以及异步通信 [6, 46]。

### 4.2.1 稀疏驱动的解决方案—异步系统
我们在第 2 节中讨论了 SNN 在神经元活动中如何具有丰富的时间和空间稀疏性。利用这种稀疏性的一种常见方法是使用各种研究人员采用的分层网格架构和异步通信 [6, 46, 154]。图 12(a) 显示了在 Loihi 芯片中实现的示例网格操作。它是一个完全异步的多核芯片，每个核心都有自己的频率和时间感。核心之间的交互也是异步的，而不是定时的，并且系统中一个神经元的操作完全独立于另一个神经元。系统中核心的操作从本地时间 t 开始，通过迭代每个核心中的神经元隔间，监视触发事件。在触发事件中，片上网络 (NOC) 仅向包含触发核心的突触扇出的核心广播尖峰消息。一旦最慢的内核完成处理，相邻内核之间的同步机制将确保在进入时间步 t +1 之前安全地传送和接收所有脉冲。Loihi 的一个独特属性是它部署了一个完全异步的系统，除了通过网状网络的事件驱动通信系统之外，核心微架构内的不同操作模式可以在不同的频率下运行。作者认为，这种本地数据流控制足以满足脉冲神经元过程对不同工作负载相关时间尺度的需求，并有助于后端时序收敛

![[Pasted image 20241020142523.png]]

TrueNorth 采用了另一种实现异步系统的方法，其中计算核心本质上是同步的，而通过 NOC 路由器的核心之间的通信协议是异步的。这确保所有核心并行运行，并且异步控制电路仅在需要突触整合和膜电位更新时启用核心。本地同步数字电路设计有助于实现复杂神经元功能的低复杂度设计。

一些基于网格的架构的扩展涉及分层混合模式路由系统，其中部署了各种不同级别的路由器。例如，在参考文献 [154] 中，部署了三级路由器：一个负责本地流量，第二个负责附近核心的非本地事件，而第三级路由器负责长距离通信。该芯片也称为 DYNAP，展示了新型数字通信方案的有效性以及在 CMOS 模拟电路上模拟神经突触功能。

事件驱动系统也可以使用连接到通用数据包路由器的二维处理阵列来整合，正如 SpiNNaker 项目 [171] 所采用的那样，如图 12(b) 所示。通过维护事件源的路由表来传达事件。处理元素只是以数据包的形式发出一个带有脉冲神经元地址的脉冲事件，路由器将数据包传送到任何其他处理核心，其中所选的路由由路由表确定。

除了上述构建大规模神经形态系统的方法外，研究人员还探索了使用模拟电路忠实实现复杂神经元和突触功能的可扩展实现。例如，BrainScaleS 系统 [203] 对 HICANN ASIC 的多个实例进行晶圆级集成，该 ASIC 使用模拟电路以及能够执行基于 STDP 的学习的突触来实现自适应指数积分和激发模型。Neurogrid [21] 是一个混合信号大规模系统，用于执行大脑模拟和可视化。它结合了高度生物可信的神经和突触亚阈值模拟电路以及对体细胞、树突树、轴突等的详细建模。多个核心使用数字地址事件表示 (AER) 协议相互通信。具有 AER 架构的神经形态系统的基本框架包含一个集成发射阵列收发器 (IFAT)、一个用于存储突触连接的查找表 (LUT) 以及用于神经元之间通信的 AER 协议，也是一种流行的方法。已经使用不同类型的神经电路探索了 IFAT 系统的变体 [239, 240]。已经提出了一种称为 ROLLS 的可重构混合信号设计，旨在模拟复杂的神经元和突触学习功能以及图像分类任务 [187]。虽然这种设计的灵感来自各种先前的设计，但它执行整体集成并以完全片上格式执行复杂任务。

人们已经探索了大规模可重构系统 [244]，以克服神经形态硬件的李比希定律，该定律的性能可能受到供应最短的组件的限制。这种可重构系统由相同组件的阵列组成，这些组件可以配置为 LIF 神经元、具有基于 STDP 规则的学习突触以及具有可训练延迟的轴突。这种系统的功效已使用基于现场可编程门阵列 (FPGA) 的设计进行了原型设计，随后在硅片上进行了演示。

### 4.2.2 高效计算架构
然而，研究人员也探索了平铺架构以减少网络上的数据移动。 SPARE [5] 和 RESPARC [11] 分别是使用 CMOS 和后 CMOS 原语构建的平铺架构的示例，它们显著提高了 SNN 执行的效率。 如图 12(c) 所示，SPARE 中的每个平铺都由 ROM 嵌入式 RAM 阵列组成，其中 RAM 存储 SNN 小分区的权重，ROM 存储用于执行可执行复杂神经元功能的超越运算的查找表。 在 SNN 执行期间，每个平铺对其权重数据部分执行乘法和累积和超越运算（权重保持静止），从而实现近内存计算。 SPARE 上的典型 SNN 执行以时间复用的方式执行，其中每个层都一次映射到平铺架构上。 当前映射的层在下一层映射之前计算其输出。

RESPARC 进一步扩展了平铺架构，利用了忆阻式交叉开关的高存储密度来实现大量片上平铺。然而，昂贵的交叉开关写入限制了时间复用架构的适用性，在这种架构中，通过重新编程权重矩阵并执行相应的点积运算，可以跨层重用交叉开关。因此，将整个 SNN 的权重数据固定到位于多个平铺上的交叉开关上的空间架构效率更高，因为它充分利用了高存储密度的优势，同时减轻了昂贵的写入。图 13 说明了使用后 CMOS 原语（忆阻式交叉开关）构建的平铺架构。值得注意的是，虽然这种空间架构对于 SNN 推理很有效，但在权重数据频繁更新的 SNN 训练中，它仍可能受到昂贵写入的影响。最近，通过将网络划分为非脉冲和脉冲对应部分，对混合 ANN-SNN 架构的探索也显示出有望缓解 SNN 的算法推理延迟缺点 [216]。
![[Pasted image 20241020143640.png]]

或者，SNN 加速器（例如 Spinal −Flow [163]）已经探索了解决在多个时间步骤上更新膜电位和权重读取的迭代内存访问开销的技术。他们提出了一种新颖的数据流，利用了 SNN 工作负载中的时间重用模式。通过创建一个紧凑的尖峰排序列表，Spinal − Flow 顺序遍历一层中所有时间步骤的尖峰，从而由于激活稀疏性而产生显着的加速。该架构使用输出固定数据流将神经元映射到所有时间步骤中的处理元素。这允许膜电位在进入下一层之前在一层的所有时间步骤中积累输入，从而消除了膜电位数据结构的额外存储和数据移动成本。这种数据流最大限度地提高了膜电位的重用，与 ANN 加速器中遵循的数据流相反，后者旨在最大限度地重复使用数据结构，例如输入、权重和输出。

## 4.3 异步通信
大型神经形态芯片中不同神经元之间的通信需要高速、时分复用的异步电路。通常，此类电路由收发器（用于并行读取和写入脉冲）和片上路由器（用于传输神经元事件或脉冲）提供服务。最常用的异步通信协议是地址事件表示 (AER) [27, 137]。AER 协议中的信号使用数字介质以“脉冲间间隔”的形式携带模拟信息，其中负责事件的节点的地址以二进制表示。在最原始的情况下，维护一个包含源和目标地址对的查找表来确定不同节点之间的连接。

互连 SNN 硬件实现的主要障碍是存储网络连接信息的查找表对内存的要求很高。为了适应 n 个神经元之间的任意连接，路由表需要 O(n) 个条目，这些条目可能大部分是冗余的。相反，如果神经元与本地相邻神经元具有优先连接性，则可以设计路由协议以在网络配置灵活性和路由内存之间进行权衡。为此，人们探索了多阶段 [153] 和分层 [35] 路由方案，以减少实现可重构大规模 SNN 的内存要求。

除了路由方案之外，大规模神经形态系统中的异步通信还需要高效的片上路由器。通常，神经形态系统的片上路由器需要具有独特的功能，例如低延迟多播路由以支持高连接性。研究人员提出了基于树的路由器 [147]，它使用递归分支将数据包广播到相应的子树。通过减少要导航的节点数量，这种方法减少了内存查找以及数据包头的大小。

上述异步通信系统中的延迟变得至关重要，因为神经形态系统使用加速时间尺度，其中电路的运行速度比生物系统快得多。因此，任何基于 AER 的神经形态系统的延迟和性能从根本上都受到通信延迟及其内存要求的限制。

## 4.4 基于 CMOS 的神经形态计算原语
高效神经形态系统的计算原语有两个设计方面：（a）实现复杂的神经元 [90] 和突触学习功能 [15] 和（b）部署低功耗突触整合功能。我们首先描述这些设计方面是如何在 CMOS 技术中实现的，CMOS 技术一直是神经形态电路设计研究的热门方向。接下来，我们将描述如何有效地使用新型内存技术来加速上述计算并以紧凑的方式实现基本处理单元。

### 4.4.1 神经回路
在硅上表示神经元电路的经典案例是基于生物神经元中的离子传输与晶体管中的电子传输之间的等效性。研究人员已经证明，生物神经元中的离子通道传输可以用极少的晶体管来建模，在亚阈值域中运行[61]。这种亚阈值晶体管还被用于使用门控变量的可编程动力学实现基于霍奇金-赫胥黎的神经元模型[263]。

然而，更广泛流行的神经元功能是 IF 或 LIF，我们已在第 2 节中详细描述过。这种神经元的抽象视图主要由一个电容单元（用于保存和更新膜电位）、一个比较单元和一个阈值单元组成。最原始的 IF 神经元形式是在 80 年代后期概念化的 [143]，它有一个简单的反馈电路，可以产生固定宽度、固定高度的电压脉冲，其中脉冲的速率与输入注入电流（来自突触）成正比，脉冲的时间特性代表输入电流波形的形状。随后在模拟域中设计的 IF 神经元电路涉及合并比较器单元，如图 14(a) 所示 [145, 236]。在这个设计中，注入的电流使用膜电容 $C_{mem}$ 进行积分，然后馈送到比较器电路以将得到的 $V_{mem}$与阈值电压 $V_{thr}$ 进行比较。两个电路中的电容反馈 $C_{f b}$ 确保 $V_{mem}$ 在 $V_{thr}$ 附近的小波动不会影响放电活动。更新后的电路还可以控制神经元的不应期。最初，在放电后，$V_{mem}$ 线性下降，但一旦它降至 $V_{thr}$ 以下，第一个反相器的输出就会设置为高，从而导致电容器 $C_r$ 以受控速率使用 $V_{rf r}$ 放电。这确保只要 $C_r$ 处的电压高于某个值，$V_{mem}$ 就不会开始增加。

![[Pasted image 20241020144117.png]]

模拟 IF 神经元电路随着时间的推移而发生了显著的变化，它融合了更多功能，例如复位、泄漏以及尖峰频率自适应等。图 14(b) 显示了相当复杂和通用的 IF 神经元电路 [89]。这种神经元电路由输入差分对积分器 (DPI) 组成。积分由膜电容器 $C_{mem}$ 执行，尖峰生成方案使用带正反馈的反相放大器实现。复位行为是使用晶体管 M13 实现的，与晶体管 M21 一起实现不应期行为。晶体管 M5-M10 产生与神经元放电率成比例的电流，从而导致尖峰频率自适应机制。实施的 LIF 神经元的修改方程为：
![[Pasted image 20241020144404.png]]
这个广义神经元实现了自适应的指数 IF 神经元。

基于 CMOS 的其他类型的模拟神经元实现也已被探索，例如对数域 LPF 神经元 [12]，它实现了可重构 IF 电路。另一种类型的 IF 神经元电路称为“Tau-Cell 神经元”[191, 237]，它使用电流模式电路，并且膜电位表示为电流。已经提出了使用超阈值晶体管的更紧凑的 IF 神经元电路，例如二次 IF 神经元 [249]。这种神经元大致基于 Izhikevich 神经元模型 [92]，其中两个状态变量由两个独立的电容器而不是一个电容器维持。泄漏 IF 神经元也已使用开关电容电路实现，其中开关用于实现膜电位和静息电位之间的泄漏行为 [64]。开关电容技术激发了更多受数字启发的神经元设计。其中一种设计涉及由二进制编码数字权重激活的加权电流镜电路 [211]。之后，神经元根据突触的兴奋/抑制性质产生正和负脉冲。

受先前神经元设计的数字启发，IF 神经元也已在全数字模式下进行了探索 [33]。数字加法器和累加器以及比较器电路可用于实现 IF 神经元的积分和尖峰产生行为，如图 15 所示。这种神经元中的泄漏由全局时钟驱动的固定权重突触实现。

![[Pasted image 20241020144515.png]]

### 4.4.2  基于 CMOS 的突触电路
突触电路最初是由 Carver Mead [145] 设想的，是一种脉冲电流源，晶体管在亚阈值域内工作，如图 16 所示。电路的输出只是一个突触电流 $I_{syn}$，它是一个脉冲，其宽度与输入尖峰的宽度成比例。上述方案的扩展涉及使用节点 $V_{syn}$ 的充电和放电机制使突触后尖峰呈指数衰减 [12, 115]。当施加输入脉冲时，节点 $V_{syn}$ 线性减小，其中减小率由分别使用 $V_{tau}$ 和 $V_w$ 偏置的晶体管中的电流差控制。因此，突触电流 $I_{syn}$ 呈指数增加。当没有输入尖峰时，$I_{syn}$ 以由使用 $V_{tau}$ 偏置的晶体管中的电流控制的速率呈指数放电。可以使用差分对（通常称为差分对积分器 (DPI) 电路 [89]）来实现替代的突触电路。在充电阶段，流过差分对一个分支的电流表示突触的输入电流。该 DPI 突触对突触电压节点的充电和放电时间常数具有更独立的控制。在突触中实现短期和长期可塑性需要额外的电路。无监督学习需要根据内部信号自动更新权重，而不是为单个突触提供外部更新信号。图 17 显示了一个兴奋性突触电路 [89]，它实现了短期可塑性 (STP) 和长期可塑性 (STDP)，同时使用电流镜积分器电路 (CMI) 来实现突触的积分行为。CMI 电路的工作原理与我们之前讨论过的积分电路类似。当尖峰到达“预”节点时，CMI 的积分电容器就会充电，而当没有尖峰时，电荷会通过二极管连接的晶体管衰减。

图 17 中的 STP 作用于突触权重电压 $V_{w0}$。当将尖峰施加到“前”节点时，突触权重会以由偏置电压控制的速率减小。因此，突触权重会经历短期抑制，即在尖峰开始时达到最大值，并在“前”节点连续施加尖峰时逐渐减小。第 2 节中描述的 STDP 机制是使用图 17 所示的 STDP 电路实现的。具体而言，该电路根据前尖峰和后尖峰时间之间的相对差异来调制模拟电压 $V_{w0}$。基于突触前和突触后脉冲分别生成两个波形 $V_{pot}$ 和 $V_{dep}$ 。前尖峰和后尖峰激活两个晶体管，控制导致 $V_{w0}$ 节点增加和减少的电流。偏置电压 $V_p$ 和 $V_d$ 设置了节点 $V_{w0}$ 处电容的电流注入和移除的限制。中间支路中承载电流 $I_{pot}$ 和 $I_{dep}$ 的晶体管在亚阈值区域工作，以实现 STDP 所需的指数关系。图 17 所示的双稳态电路需要保持突触权重的模拟值，因为 CMOS 电容器容易发生电荷泄漏。在没有尖峰活动的情况下，双稳态电路会产生恒定的漏电流，将权重节点 $V_{w0}$ 拉向两个稳定状态之一。馈送到比较器的电压 $V_{thr}$ 由外部设置。如果 STDP 电路导致突触权重低于阈值，则双稳态电路允许负电流流动以驱动权重朝向代表抑制状态的模拟值。当突触权重增加到阈值以上时，正电流流动以将权重驱动到高状态。其他研究也采用了类似的方法，研究了不同变体的塑性突触。例如，一种名为 ROLLS [187] 的芯片使用单独的突触阵列来实现短期可塑性和长期可塑性，同时使用基于前面描述的自适应指数 IF 神经元电路变体的定制神经元电路。

![[Pasted image 20241020144818.png]]

## 4.5 ROM 嵌入式 RAM 作为神经元功能存储


## 4.6 基于非易失性存储器的计算原语


## 4.7 硬件-软件协同设计方法
在本节中，我们描述了在硬件堆栈的各个级别（例如架构、通信以及最终的计算原语）设计自定义 SNN 硬件的不同方法。研究人员还探索了有效的硬件-软件协同优化技术，以有效地将 SNN 映射到硬件系统。其中一种方法包括贝叶斯超参数优化 [178]，其中作者对特定于硬件的超参数进行了网格搜索。参考文献 [163] 的作者探索了我们之前描述的一种架构解决方案，该解决方案要求在进入下一层之前，通过所有时间步长计算 SNN 模型的每一层——这是实现计算效率的硬件感知优化 SNN 模型的一个主要示例。还探索了转换 SNN 模型以更好地适应神经形态硬件约束 [95]，以实现高资源利用率。参考文献 [11] 探讨了在基于非易失性存储器的 SNN 加速器上有效映射 SNN 模型。硬件-软件协同优化在神经形态领域处于相当早期的阶段。研究人员可以从相应的深度学习硬件-软件协同优化技术中获得灵感，例如利用修剪、量化以及探索能够实现参数重用的模型。此外，随着我​​们扩展 SNN 模型，我们还应该继续解决神经形态系统面临的独特挑战，例如膜电位存储的额外开销以及重用和量化在这方面如何提供帮助。

# 5 讨论
人工智能已在各个领域无处不在，并正在改变我们周围的世界。当前强大的机器学习模型大多部署在大型云计算系统上。为了在物联网设备上大规模采用边缘智能或 TinyML，需要重新考虑现行的解决方案。为此，ANN 中的模型压缩、修剪和量化等技术已显示出巨大的前景，但要实现类似大脑的效率，这可能还不够。受大脑启发的神经形态计算，特别是 SNN，可以帮助弥合能量差距。SNN 作为动态系统运行，具有随时间演变的量，这些量定义了神经元和突触的动态。虽然最近的基于梯度的方法考虑了参数更新的尖峰时间，但时间反向传播是内存密集型的。因此，需要进一步研究以开发能够有效利用丰富的时间信息的学习算法，以更少的资源实现更快的学习。

如第 2.4 节和图 9 所述，由于 SNN 具有固有的循环性，因此可以像 RNN 一样工作。这为将 SNN 用于静态和顺序任务提供了独特的机会。到目前为止，研究主要集中在使用 SNN 进行静态图像分类。其他任务，例如语言建模、语音识别、机器翻译等，RNN 在这些任务中表现良好，也可以探索 SNN。在第 2.1 节中，我们讨论了 IF/LIF 神经元模型，其简单性是其优势，但需要进一步研究神经元建模、结构可塑性和树突在有效学习中的作用，以模拟大脑的复杂动态。此外，神经元建模和学习算法的单独进步可能无法产生兼容的解决方案。重点应该放在共同设计神经元模型和学习算法上，以实现复杂性和可训练性之间的最佳权衡。类似地，基于反向传播的算法可能不太适合深度 SNN，而各种硬件友好的局部学习方法可能更适合在边缘执行计算。基于无监督 STDP 的方法在浅层网络上执行简单任务时效果很好，但无法优化深度网络。可以探索 STDP 学习的其他变体与稳态和基于局部梯度的技术相结合，以发现更好的学习机制。批量归一化 [91] 已被证明是训练深度 ANN 的成功技术，但其在 SNN 中的应用有限，并未带来显着的改进。此外，边缘应用程序需要实时在线学习（批量大小 = 1），这引发了一个问题：是否可以进行批量归一化。SNN 在处理来自事件传感器的时空数据方面非常成功。它们在运动估计和从神经形态数据集中对图像进行分类等任务上的表现相对优于 ANN。因此，需要进一步研究以确定更多这样的任务，或有效地将其他任务转换为更适合 SNN 的离散形式。

在第 4 节中，我们描述了构建神经形态模拟器和加速器的基本要求以及最近的探索。神经形态硬件领域的进步缩小了算法空间与当今通用和特定领域加速器中的适应性之间的差距。此外，神经形态硬件的研究利用了 SNN 工作负载的独特功能，这些功能可以潜在地降低计算复杂度并提高能源效率。尽管神经形态硬件领域取得了发展，但要真正实现 SNN 工作负载提供的能源效率潜力，还需要进行重大改进。神经形态算法在不断发展，因为新功能被纳入工作负载中，这需要神经形态硬件也不断发展。在过去的二十年里，我们见证了在 CMOS 中构建模拟神经形态电路以及在单个 NVM 设备中直接映射此类功能的巨大飞跃。构建高效神经形态系统的主要挑战是可扩展性。我们深入研究了大规模 CMOS 神经形态系统，其中包括晶圆级集成以及多芯片模块。然而，扩展基于 NVM 的神经形态系统仍然是一个挑战。首先，设备的可变性和可靠性对使用基于 NVM 的神经原语实现可扩展系统提出了巨大挑战。其次，基于 NVM 的计算原语在模拟域中执行突触计算。基于 NVM 的原语 [37] 中的模拟计算本质上是错误的，需要建模和足够的缓解 [210] 才能实现合理准确的操作。已经探索了各种算法策略来缓解和潜在地利用设备不匹配和可变性。例如，突触 [222] 和神经元 [197] 中的随机性已被证明有利于 SNN 中的泛化。

尽管在 CMOS 中构建大规模神经形态系统方面付出了巨大努力，但此类系统的一个关键方面是使用模拟电路实现神经突触功能。模拟电路容易受到晶体管阈值电压变化的影响。尽管可以使用各种电气工程技术来最大限度地减少设备变化，但硅神经元和突触的模拟设计需要额外的电路来结合各种适应和反馈机制。人们一直在探索设计技术来抵消模拟电路中的这种变化[164]。研究人员还认为，一定程度的不精确性通常对神经计算有益，这与生物大脑中不精确和多样化的计算模式类似[136]。此外，设备不匹配也已被证明[149]可以在循环 SNN 中实现稳定的接受场和平衡的网络活动。尽管这些论点具有重要价值，但也需要设计工程解决方案来规避大规模神经形态系统中设备变化的影响，特别是当 CMOS 技术已经缩小到 10 纳米以下时。

最后，设计神经形态硬件时必须考虑与工作负载相关的开销。虽然在设计事件驱动硬件方面取得了重大进展，但与 ANN 工作负载不同，由于额外的数据结构（即膜电位），SNN 工作负载与额外的数据移动和存储相关。探索基于非易失性存储器的神经元的架构还可以允许原位膜电位存储和更新 [216]，从而减少膜电位的移动。可以考虑进一步优化，以尽量减少在每个时间步获取和更新膜电位的成本。一种这样的技术可以涉及将膜电位量化到低于全精度。这将减少存储要求以及数据移动成本。或者，可以探索输入、权重、部分和膜电位之间的平稳性选项，并设计一个减少最关键数据结构移动的数据流。总体而言，减少膜电位开销仍然是未来神经形态硬件的一个关键设计挑战。

自 20 世纪 90 年代诞生以来，神经形态计算在神经科学和半导体技术的发展推动下取得了长足的发展。凭借我们目前对生物大脑功能的理解，我们利用了一些关键特性，例如基于事件的计算和通信、局部学习以及内存和处理能力的共置。此外，我们从人工智能算法的发展中汲取灵感，扩大网络规模，在最先进的识别任务中实现与传统 ANN 相当的性能，同时潜在地降低功耗。随着重大进展，神经形态计算给我们带来了上述众多挑战，这些挑战将为未来的研究指明方向。在传感器、硬件和算法堆栈方面的这些努力确实有助于实现基于神经形态计算的高效智能系统。
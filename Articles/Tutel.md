MOE 中的动态路由问题，每个 token 被路由到哪个专家是不确定的。这里每个专家分配到的token不一样，计算量也就不平衡。为了强行把所有专家计算量拉平，有了专家容量（Expert Capacity）概念，即每个Expert计算的token数。它的计算方法是，将一个Batch的token总数除以专家数，然后再根据一个容量因子进行扩充。公式如下：
![[Pasted image 20250408130617.png]]
超过专家容量的 token 直接被丢弃或者标记为路由失败

## 路由失败怎么办？

1. **丢弃（Drop tokens）**：最简单粗暴的方法，直接不处理；
    
2. **重定向**：让 token 找到下一个排名高的可用专家（比如 top-2）；
    
3. **辅助损失（Load balancing loss）**：引入平衡损失，鼓励不同专家负载均衡；
    
4. **Token padding**：如果 expert 没满，可以填充，方便并行计算。

# 传统 MoE 实现的问题
动态路由限制让每个专家计算量一致引入了模型质量和硬件效率之间的权衡，因为用户必须决定是丢弃token损失精度还是zero-padding浪费计算和内存资源。这个决策通常是通过超参数调整来进行的，这增加了使用MoEs的复杂性。Megablocks就是想打破这种限制，让没有token drop且没有zero-padding开销，从而摈弃了loss balance和expert capacity限制。


# Abstract
稀疏门控混合专家 (MoE) 已被广泛采用，用于将深度学习模型扩展到具有固定计算成本的万亿以上参数。MoE 的算法性能依赖于其令牌路由机制，该机制将每个输入令牌转发到正确的子模型或专家。虽然令牌路由动态地确定运行时专家的工作量，但现有系统由于其静态执行（即静态并行和流水线）而导致计算效率低下，无法适应动态工作负载。
	 
我们提出了 TUTEL，这是一种具有动态自适应并行和流水线的 MoE 高度可扩展的堆栈设计和实现。TUTEL 设计了一个相同的布局来分配 MoE 模型参数和输入数据，可以通过可切换的并行性和动态流水线方法利用它，而无需数学不等式或张量迁移开销。这使得在运行时以零成本实现自适应并行/流水线优化成为可能。基于这一关键设计，TUTEL 还实现了各种 MoE 加速技术，包括灵活的 All-to-All、二维分层 (2DH) All-to-All、快速编码/解码等。综合所有技术，TUTEL 最终在 16 个和 2,048 个 A100 GPU 上分别实现了单个 MoE 层 4.96 倍和 5.75 倍的加速，比之前最先进的技术更快。

我们的评估表明，TUTEL 高效且有效地运行了基于现实世界 MoE 的模型 SwinV2-MoE，该模型基于最先进的计算机视觉架构 Swin Transformer V2 构建。在效率方面，TUTEL 加速了 SwinV2-MoE，在 Fairseq 的训练和推理中分别实现了高达 1.55 倍和 2.11 倍的加速。在有效性方面，SwinV2-MoE 模型在预训练和下游计算机视觉任务（例如 COCO 对象检测）中都实现了比对应的密集模型更高的准确率，这表明 TUTEL 已准备好进行端到端的真实世界模型训练和推理。

# Introduction
近年来，社区发现，加入更多模型参数是提高深度学习 (DL) 算法性能的最直接但不太复杂的方法之一 (Kaplan 等人，2020 年)。然而，模型容量通常受到计算资源和能源成本的限制 (Sharir 等人，2020 年)。为了解决这个问题，稀疏门控混合专家 (MoE) (Shazeer 等人，2017 年) 通过使用多个称为专家的并行子模型引入了一种稀疏架构，其中每个输入仅根据智能门控函数转发给少数专家。与密集层不同，这种方法以仅次线性增加计算成本的方式扩大了模型容量。如今，MoE 是最流行的方法之一，已被证明可以将 DNN 扩展到万亿以上的参数 (Fedus 等人，2022 年)，为能够学习更多信息的模型铺平了道路。

虽然基于 MoE 的算法开辟了巨大的扩展机会，但 MoE 的动态特性带来了以前大多数 DL 算法和系统中从未见过的基本系统端挑战。具体来说，每个 MoE 层由一定数量的并行专家组成，这些专家分布在加速器（本研究中为 GPU）上，每个 GPU 根据智能门控函数将每个输入数据分派给几个最佳拟合专家，并获取相应的输出以将它们组合起来。这意味着专家的工作量从根本上是不确定的——它取决于输入数据和门控函数。在实践中，它们在每次迭代中都会发生变化。在我们的实验中（见图 1），单次训练中的工作量变化高达 4.38 倍，不同层的工作量也不同。
![[Pasted image 20250407135047.png]]
图 1. 在端到端训练 Swin Transformer V2 的 MoE 版本（Liu 等人，2021；2022）薄型（左）和基础（右）模型期间，MoE 层的工作负载动态变化。y 轴是运行时所需的专家容量，表示工作负载量（请参阅第 2.1 节中的详细信息）。为了清晰起见，模型中总共 10 个 MoE 层，仅显示第 1、第 4 和第 10 层。

以前的 DL 系统，包括最新的 MoE 框架（Lepikhin 等人，2021 年；Ott 等人，2019 年；Rajbhandari 等人，2022 年；He 等人，2022 年），大多基于静态运行时执行，不符合动态 MoE 特性。主要的缺陷在于专家往往无法利用性能最佳的并行性，因为最佳并行性因动态工作负载而异。在运行时动态调整并行性并非易事，因为它通常会在现有系统中产生大量的重新分配开销或 GPU 内存消耗。其他方法（例如负载平衡损失（Fedus 等人，2022 年））试图通过操纵 MoE 算法来解决这个问题，但它往往会损害我们实验中的模型准确性（参见第 2.1 节）。

本文介绍了 TUTEL，这是一种通过专门针对动态 MoE 工作负载的自适应方法在任何规模上彻底优化 MoE 的系统。关键机制是自适应并行切换，它在每次迭代时动态切换并行策略，而无需任何额外的切换开销。具体而言，与现有系统对不同的并行策略使用不同的张量布局不同，我们只利用一个涵盖所有可能的最佳策略的分布布局。当我们切换并行策略时，这使系统无需重新格式化输入数据或权重，因此实现了零成本切换。基于我们对所有类型并行的通信成本分析，我们确保自适应并行不会损害最佳并行策略。

TUTEL 是一个完全实现的框架，适用于大规模的各种 MoE 算法。通过自适应并行切换，它提供了多种优化技术，以实现高效和自适应的 MoE，包括自适应流水线、二维分层 (2DH) All-to-All 算法、GPU 上稀疏计算的快速编码/解码等。TUTEL 已在 GitHub1 上开源，并已集成到 Fairseq (Ott 等人，2019 年) 和 DeepSpeed (微软，2023 年) 中。我们对 Azure A100 集群 (Azure, 2023) 进行的大量实验表明，与使用原始 Fairseq 相比，使用 128 个 GPU，TUTEL 可将 MoE 层加速提高 3.11 倍，将真实世界模型 (SwinV2-MoE) 的端到端训练/推理速度提高 1.55 倍/2.11 倍。对于 2,048 个 GPU，MoE 层加速进一步提高到 5.75 倍。

我们的主要贡献如下：
• 对 MoE 的动态特性和现有框架中的以下挑战进行详细分析。
• 提出自适应并行切换，有效处理 MoE 的动态工作负载，实现单个 MoE 层 1.35× ∼ 14.57× 的加速。
• 聚合所有加速技术，TUTEL 可在任何规模上实现 MoE 的加速：在 16 个和 2,048 个 A100 GPU 上，单个 MoE 层分别实现 4.96× 和 5.75× 的加速。
• TUTEL 已用于在现实世界的计算机视觉问题上实现和运行最先进视觉模型 SwinV2-MoE 的稀疏 MoE 版本。与 Fairseq 等以前的框架相比，训练和推理分别实现高达 1.55× 和 2.11× 的加速。我们还证明了稀疏模型比对应的密集模型具有更高的准确度，这表明 TUTEL 已准备好训练现实世界的 AI 模型。

# 2 
稀疏门控混合专家 (MoE)。MoE 采用多个专家模型，这些模型分别处理各自的专业子任务，以共同解决整个任务。大规模分布式 DNN 模型通过放置一个跨 GPU 层来利用它，该层部分交换来自不同 GPU 的隐藏特征 (Fedus et al.,
2022; Lin et al., 2021; Riquelme et al., 2021)。图 2 说明了一个例子。首先，它运行一个门控函数 (Lewis
et al., 2021; Roller et al., 2021; Yang et al., 2021)，该函数确定以下全对全集体通信 (All-to-All) 中每个输入 token2 的目标 GPU。在 All-to-All（称为调度）之后，每个 GPU 运行自己的专家，即前馈网络层（fflayer），然后进行第二个 All-to-All（称为组合），将每个 token 的相应输出发送到 token 来源的 GPU。门控函数和 fflayer 的细节取决于模型算法。

![[Pasted image 20250407140321.png]]

MoE 是百亿亿级深度学习的关键。

MoE 与现有的 DNN 扩展方法（即增加 DNN 的深度或宽度）的区别在于其高成本效率。具体而言，在 MoE 层中注册更多模型参数（专家）不会增加每个 token 的计算成本。如今，MoE 被认为是超大规模 DL 的关键技术，其在先前的研究中展示了其最先进的结果（Fedus 等人，2022 年；Riquelme 等人，2021 年；Lepikhin 等人，2021 年；Du 等人，2022 年）。目前，许多最先进的框架（例如 DeepSpeed (Microsoft, 2023)、Fairseq (Ott et al., 2019) 等）已经支持 MoE。

MoE 的动态工作负载。

MoE 动态工作负载的根本原因来自其令牌路由机制。

具体而言，MoE 层将每个令牌动态路由到多个专家，其中令牌在专家之间的分布通常不均匀。这使得每个专家的工作量在每次迭代时都会动态变化，如图 1 所示。专家容量是一种常见的做法，用于指示每个专家的工作量，即专家收到的要处理的令牌数量。专家容量取决于每批令牌数量 T、全局专家数量 E、top-k 路由（1 ≤ k ≤ E）和容量因子 f（f ≥ 1），如下所示：

$$
Expert Capacity = k  \times f \times \frac{T}{E} 
$$
f = 1 是最小值，表示 token 分布最均匀。f 值越大，表示 token 路由越不平衡，这意味着专家必须处理更多的 token。

大多数现有的 MoE 框架（Ott 等人，2019 年；Lepikhin 等人，2021 年；Rajbhandari 等人，2022 年；Zheng 等人，2022 年）只是将 f 设置为容量因子 fupper 的静态上限（即 f = fupper），以便不同的迭代始终执行静态计算量。然而，基于 fupper 的静态计算不仅会引入不必要的计算，而且如果 fupper 未设置为足够大的值，还可能会从训练中丢弃过多的 token，这可能会影响模型准确性。为了解决这个问题，在本文中，我们考虑了一个系统（如 TUTEL），它使用所需的最小 f 支持 MoE 训练，既不会像使用 f = fupper 那样产生不必要的计算，也不会丢弃 token。基于此机制，我们探索了 f 在训练步骤中变化时的进一步优化机会。

MoE 框架。
虽然 GShard (Lepikhin 等人，
2021) 提供了一种确保 MoE 算法正确性的计算逻辑，但几种流行的 MoE 框架 (Ott 等人，2019；Rajbhandari 等人，2022) 遵循相同的逻辑，但在大规模上表现不佳。
Fast/FasterMoE (He 等人，2022) 提出了不同的门控算法，这些算法在计算上与 GShard 不等同。此外，它还提出了影子专家和智能调度，当不平衡的代币分布持续很长时间时，它们只会提供有条件的好处，而否则可能会损害吞吐量。另一方面，TUTEL 追求保持与 GShard 相同的计算逻辑，并在任何环境中实现确定性增益，从而将 MoE 框架适应百亿亿次级，而不会损害算法结果。

负载平衡损失。
负载平衡 (LB) 损失通过鼓励门控函数平衡专家的工作量来调节 MoE 层训练 (Shazeer 等人，2017 年；Fedus 等人，2022 年)。LB 损失可能导致 MoE 工作量低且稳定，因为当 token 分布均匀时，容量因子 f 通常会降低（如上一段所述）。然而，LB 损失通常不足以应对 MoE 的动态工作量，因为对 LB 损失赋予较大的权重通常会损害模型准确性。具体而言，对 LB 损失赋予适当的权重可能有助于提高模型准确性，方法是引导门控函数在训练期间注册更多不同的专家参数，但过大的权重可能会损害最终任务的优化目标，并导致无法将 token 转发给知识渊博的专家。表 1 显示，我们对较大 LB 损失权重的实验会损害模型准确性。
此外，根据我们的实证研究结果，LB 损失并不总是会导致专家之间的工作量更加平衡。
例如，我们在图 1 中的实验使用了有助于实现最佳准确度的 LB 损失，但它仍然显示出动态变化的工作量。在本文中，我们仅考虑通常无论 LB 损失如何都适用的系统端解决方案。

## 2.2 Static Parallelism
在 MoE 层的动态特性下，如果我们想用多个 GPU 加速一个专家以获得更高的吞吐量，这将变得具有挑战性。先前的研究已经证明，雇用更多专家通常只能获得快速递减的增量收益，因为专家数量较多（> 256）（Rajbhandari 等人，2022 年；Clark 等人，2022 年；Fedus 等人，2022 年）。因此，在大规模训练中，与 GPU 数量相比，MoE 层通常雇用相对较少的专家，并且将多个 GPU 分配给一个专家以获得更高的吞吐量。

我们考虑了先前研究 (Fedus et al., 2022) 中为 MoE 采用的三种不同的并行方法：

专家并行 (EP，分配专家)、

数据并行 (DP，分配输入数据) 和

模型并行 (MP，拆分和分配单个专家)。

EP、DP 和 MP 可以同时使用。

根据我们的实验，在动态工作负载下，静态地采用某种并行方法并不总是有效。例如，图 3 比较了两种不同的并行方法 EP+DP 和 EP+MP 的性能。如图所示，最佳并行方法取决于工作负载，这两种并行方法之间的性能差距为 7.39%-27.76%。

![[Pasted image 20250407142057.png]]
它比较了不同容量因子 f（即不同工作负载）和不同 top-k 配置下的吞吐量，其中 > 1.0 意味着 EP+MP 优于
EP+DP，反之亦然。模型设置：fflayer 隐藏大小 16K，
fflayer 通道大小 2048，批量大小 4。

不幸的是，在运行时切换不同的并行方法会产生大量开销。具体而言，在现有工作中，基于某种并行性（例如数据并行）的持续训练并非设计为与另一种并行性（例如模型并行）兼容，因为它们对数据分割、权重分割、管理参数梯度动量，甚至启动训练的框架接口都有不同的要求。此外，参数迁移是改变并行性时会产生的另一项昂贵开销，如图 4 所示。这就是现有系统中很少使用并行性切换的原因。

![[Pasted image 20250407142617.png]]

## 2.3 Static Pipelining
图 2 中所示的 MoE 层通常未充分利用 GPU，因为它们按顺序运行 All-to-All 和 fflayer 来调度和组合。由于 All-to-All 主要由非计算密集型的 GPU 间数据副本组成，因此我们可以更好地利用 GPU 的计算能力，通过将其与运行数值计算的 fflayer 进行流水线处理。表 2 显示，通过重叠 All-to-All 和 fflayer 计算，最高可实现 1.86 倍的潜在加速。

![[Pasted image 20250407142928.png]]
然而，我们观察到，调度和组合的静态流水线策略，即静态 All-to-All 算法和流水线度，对于处理动态工作负载效率低下。如图 5 所示，根据不同的 MoE 设置和规模，相应的最佳流水线策略由各种 All-to-All 算法（线性或 2DH3）和流水线度组成。这意味着单一的静态策略在不同的 MoE 设置和规模下无法始终实现最佳性能，并且动态流水线策略在运行时是必要的，以适应不同的设置。
![[Pasted image 20250407155441.png]]
图 5. 各种 MoE 工作负载配置的最佳流水线策略分布。每列表示在 X 轴上描述的策略下表现最佳的配置数量。工作负载配置的详细信息与第 5.1.2 节中描述的相同。

更糟糕的是，如果我们只单独考虑每个方面，计算和通信之间的干扰将使我们很难找到最佳的流水线策略。这是因为在同一 GPU 上同时运行 NCCL 内核和计算内核所导致的速度减慢很难估计。根据我们的大量实验，即使两种不同的 All-to-All 算法具有相似的吞吐量，当引入相同的并发计算内核时，它们的吞吐量通常也会有很大差异，并且每种算法都可能在不同情况下优于另一种算法。这意味着动态调整应该与计算和通信一起进行，以获得最佳的总体吞吐量。

# 3 
TUTEL 是一个全栈 MoE 系统，支持具有自适应优化的完整 MoE 层。由于所有优化对 DNN 模型开发人员来说都是透明的，因此 TUTEL 不会改变 DL 框架的接口，并且可以轻松地与其他框架集成。在以下小节中，我们将详细描述 TUTEL 如何解决上述问题。

## 3.1 Adaptive Parallelism Switching
### 3.1.1  并行切换所需的最小子集是多少？
鉴于 EP、DP 和 MP 得出了 7 种不同的并行方法组合，一种临时方法是为每种方法设计一个执行流程，并使其可与所有其他方法切换。但是，设计最多 7 个执行流程是没有必要的，因为问题可以精确地简化为一个较小但效率相当的问题，正如小节标题中强调的那样。

我们的方法是分析所有并行方法的复杂性，以将它们缩小到我们需要为其设计执行流程的最小子集。请注意，这里只有通信复杂性很重要，因为所有 GPU 都进行相同的计算，因此计算复杂性相同，因此通信复杂性直接决定了一种并行方法相对于其他方法的效率。如表 4 所示，我们分析了所有并行方法的通信复杂性，如果它们 (1) 在任何情况下都不是最佳的，或 (2) 是另一种方法的特殊情况，则将其从我们的考虑中排除。通过一系列比较（如表 4 的注释栏所示），我们得出结论，子集只能包括 DP 和 EP+DP+MP。因此，以下段落设计了相应的并行结构，仅关注 DP 和 EP+DP+MP，无论模型配置如何，它仍然保证覆盖最佳并行方法。

### 3.1.2 零成本可切换并行的执行流程
如第 2.2 节所述，可切换并行性

应保证 MoE 训练的数据布局和执行流程完全相同。我们分别解释 DP 和 EP+DP+MP 的设计如下。零成本意味着切换并行性完全免费，不会因参数/token 迁移而引入任何大于 O(1) 的开销。

可切换 DP（图 6）：它遵循传统的 DP 训练，仅将本地 token 作为输入，但权重参数遵循 ZeRO-DP Stage-3 分区（Ra-jbhandari et al.，2020）机制。具体来说，它让每个设备拥有一个唯一的权重片段，并在前向传递期间执行一次全聚集通信，在后向传递期间执行一次减少散射通信，而不是在后向传递期间执行一次全减少通信的传统训练。两种方式都是复杂度等效的，因为单个全减少自然由减少散射和全聚集组成。在图 8 中，r = 0 代表可切换 DP。
![[Pasted image 20250407193834.png]]
![[Pasted image 20250407160920.png]]

可切换 EP+DP+MP（图 7）：开箱即用，这种并行方法与可切换 DP 的工作原理相同——它们共享相同的读取输入和切片权重格式。在盒子内部，它不仅确保整个计算在数学上等同于 DP，而且还确保所需的计算和网络复杂度在 EP+DP+MP 的预期复杂度范围内，如表 4 中的 7⃝ 所示。我们定义一个控制参数 r，表示将所有 GPU 划分为一个或多个大小为 ⌈(W/E)/r⌉ 的组，以便 DP 将在每个组内执行，MP 将在不同组之间执行。具体而言，它在执行流程开始时以 MP 的风格重复本地标记 r，最后对称地执行本地求和。 DP 仅用于在大小为 ⌈(W/E)/r⌉ 的组内执行全收集。请注意，如果 r 增加并达到 W/E，

组大小将变为 1，因此每个组内的全收集通信将被优化。这就是为什么 7⃝ 中的 r ≥ W/E 的情况消除了额外的 O(P/E/r)。在图 8 中，r 值从 1 到 W/E 代表可切换的 EP+DP+MP，尽管 r = 1 和 r = ⌈W/E⌉ 是两种特殊情况，它们分别与 EP+DP 和 EP+MP 完全等价。

## 3.2 Adaptive Pipelining for Linear & 2DH All-to-All
本节介绍自适应流水线的设计。
由于 All-to-All 通信延迟对最佳流水线度有重大影响，我们的自适应流水线同时联合优化了流水线度和 All-to-All 算法（线性或 2DH）。虽然本节仅解释如何为流水线划分输入令牌，但下面的第 3.3 节描述了我们如何联合搜索最佳流水线度和 All-to-All 通信算法。

多流流水线的令牌分区。
需要对令牌进行适当的分区，以使流能够在更细粒度的数据块上重叠，以便可以在单独的 GPU 流上提交计算和通信并并行运行。传统的分区，如批处理拆分或流水线并行 (Huang et al., 2019)，对层中的所有操作进行分区。这在 MoE 中不起作用，因为它会放大 MoE 调度的不平衡并破坏 ML 特性的正确性，例如批处理优先级路由 (Riquelme et al., 2021)。相反，我们建议只对两个 All-to-Alls 和中间的专家进行分区，而不是对整个 MoE 层进行分区，以避免这些缺点。图 9 给出了 All-to-All-Expert 重叠中数据分区设计的 2-GPU 示例。
![[Pasted image 20250407162228.png]]
在前向传递中，在每个 GPU 上，形状为 (E, Cg, D) 的输入沿维度 C 被分割成两个形状为 (E, Cg/2, D) 的虚拟分区。这两个虚拟分区标记为 C0 和 C1。分割后，每个虚拟分区 Ci 被异步发送以在通信流上按 i 的顺序执行 All-to-All 操作。All-to-All 被定制为接受隔离的数据块作为输入并执行内联数据混洗，生成形状为 (Eg, C/2, D) 的输出。接下来，两个 All-to-All 输出被编程为在它们之前对应的 All-to-All 完成后被发送到计算流上执行专家计算，并且专家计算的输出再次被编程为在之前对应的专家计算完成后被发送到通信流上执行第二个 All-to-All。最后，
在第二次 All-to-Alls 之后设置一个屏障，屏障之后，分区被合并以生成形状为
(E, Cg, D) 的最终输出。

后向传递的工作方式与前向传递类似，不同之处在于输入变为原始输出的梯度，计算变为专家的后向计算，输出变为原始输入的梯度。

请注意，所有分区和重塑操作均由定制操作内联完成，因此与无重叠情况相比，没有额外的数据复制开销。

## 3.3 Dictionary of Optimal Parallelism & Pipelining
TUTEL 管理一个字典，用于记忆各种不同范围的专家容量的最佳并行性和流水线设置。具体来说，我们将字典定义为哈希图：⌊c/R⌋ → {r∗, d∗, a∗}，其中 c 是某次迭代的容量值，R 是将多个相邻的 c 值收敛到同一个键的窗口大小（默认值为 128），{r∗, d∗, a∗} 是最佳设置的元组（分别为自适应：r、流水线度和 All-to-All 算法）。为了预先建立这个字典，我们需要找到每个可能的密钥 (⌊c/R⌋) 的最佳设置，只需要几次试验，其计算方式为：每个密钥的试验次数 = (log3/2⌈W/E⌉ + 2) · 4 · 2。(log3/2⌈W/E⌉+2) 是通过三元搜索 (Wikipedia, 2023) 搜索 r∗ 所需的试验次数，因为 r 在范围 [1, ⌈W/E⌉ − 1] 内确定了一个凸最优分布，再加上 r = 0 和 r = ⌈W/E⌉ 的两次额外试验。“4”是 d∗ 所需的试验次数，因为我们将流水线度的搜索空间限制为 {1, 2, 4, 8}。在我们的实践中，大于 8 的度数几乎不会改善计算和通信之间的重叠，同时会显著增加 All-to-All 的开销。“2”是指 All-to-All 算法的数量（线性或 2DH）。

# 4 implemention
## 4.1 Features
与其他 MoE 框架（包括 DeepSpeed MoE、Fairseq MoE 和 FastMoE）相比，TUTEL 对不同设备、数据类型和 MoE 相关功能的 MoE 模型训练提供了更全面的支持。

动态 Top-ANY MoE 门控。
为了为 MoE 训练启用各种稀疏性选项，TUTEL 支持 top-ANY 路由。k 值也可以按步骤自定义，以启用动态稀疏性更新，这在一个 MoE 层的不同迭代使用其首选的 top-k 设置而不是使用相同的 k 值时很有用。
用户可以利用此功能动态微调 MoE 层的稀疏性。

动态容量因子。
为了在不同的 token 不平衡情况下智能地控制容量上限，TUTEL 支持在每次迭代时动态调整容量因子。如图 10 所示，调整行为由传递给我们的 MoE 层 API 的参数 capacity setting = x 控制。如果 x 为正，则该值直接用作 MoE 层的容量因子。如果 x 为零，TUTEL 会自动将容量因子调整为每次迭代时不会丢弃任何 token 的最小值。如果 x 为负，则其工作原理与 x 为零时相同，只是将 −x 设置为容量因子的上限，即任何超出的值都将调整为 −x。

## 4.2 
灵活的全对全。
我们提出了一种基于传统 MPI/NCCL 全对全接口的抽象，以确保 MoE 专家的高计算吞吐量，无论规模如何，在这种情况下称为灵活的全对全。现有的全对全将张量布局从（E，Cg，D）转换为（W，Eg，Cg，D），其中 Cg 依赖于 W，这会影响专家进行以下矩阵乘法的效率。相反，我们将输出布局转换为（Eg，C，D），以确保在任何规模（W）下进行相同形状的矩阵乘法。图 11 比较了传统全对全和灵活全对全之间的专家计算吞吐量。

# 5 
测试平台。
如未指定，所有实验均使用 Azure Standard ND96amsr A100 v4 VM（Azure，2023 年）。每个 VM 配备 8× NVIDIA A100 SXM 80GB GPU 和 8× 200 Gbps HDR InfiniBand，由 96× 第二代 AMD Epyc CPU 核心和 1.9 TiB 内存支持。GPU 通过第三代 NVLink 和 NVSwitch 在一个 VM 内连接，而不同的 VM 通过具有自适应路由的 1,600 Gbps InfiniBand 无阻塞网络连接。

## 5.1 使用 TUTEL 对自适应 MoE 进行评估
本节评估使用 TUTEL 进行自适应计算的收益。我们比较了最佳并行/流水线策略的吞吐量，并研究了 TUTEL 的自适应性带来的收益。为了与现有框架进行同类比较，在第 5.2 节中，我们仅使用两者都支持的特定并行方法将 TUTEL 与 Fairseq MoE (Ott et al., 2019) 进行比较。

### 5.1.1 Adaptive Parallelism Switching
我们使用单个节点评估了具有各种 MoE 模型设置的自适应并行切换。图 12 比较了使用不同并行选项的归一化吞吐量，其中容量因子 f 从 1.0 到 8.0 不等。我们测试了两种 MoE 配置，Base（样本/步长 = 4K 和 H = 2K）和 Large（样本/步长 = 1K 和 H = 32K），同时共享其他专家设置（E = 16、D = 2K 和总共 64 个 GPU）。如图所示，最佳并行方法因 MoE 专家配置和容量配置而异。例如，当专家容量高时，DP（r = 0）往往更受欢迎，随着容量的降低，趋势逐渐变为 EP+DP（r = 1），然后变为 EP+DP+MP（r > 1）。在相对小规模的 MoE 配置中，最佳并行性选项通常保持在 r = 0 或 r = 1，而在较大规模的配置中，它会在更大范围的 r 值上动态变化。这种多样性证明了 TUTEL 有很大的改进机会，这会导致根据动态变化的 f 产生不同的最佳并行性方法，如第 3.1 节所述。

### 5.1.2 Adaptive Pipelining
我们对不同规模（16 ∼ 256 个 GPU）的 243 个典型 MoE 模型设置上的自适应流水线进行了评估。我们测试了以下范围内的所有 MoE 模型配置组合：例如 ∈
{0.5, 1, 2}、D ∈ {1024, 2048, 4096}、H ∈ {1024, 2048,
4096} 和 tokens/step ∈ {4096, 16384, 65536}。为了进行比较，我们还考虑了不同的程度 {1, 2, 4, 8} 和不同的 All-to-All 算法（线性或 2DH），测量了不同的静态流水线方法。

我们还评估了不同规模下不同动态工作负载下的性能增益。我们使用不同的容量因子 f 来模拟不同训练迭代中的不同工作负载模式。如图 13 所示，自适应流水线始终选择最佳策略，与基线（流水线度 1）相比，当 f = 4 时，它可以实现高达 39% 的改进，当 f = 8 时，它可以实现高达 57% 的改进。

## 5.2 Single MoE Layer Scaling
我们评估了扩展到 2,048 个 GPU 时单个 MoE 层的步进时间。它使用 tokens/step = 16384、f = 1、
D = 2048、H = 2048、Eg = 2、top-k = 2、adaptive:r = 1。
我们一次添加一次 TUTEL 特征来研究主要收益来自哪里，其中 Fairseq (Ott et al., 2019) 用作基线。以下第 5.1 节提供了每个特征的详细实验。

下面按顺序解释图 14 中的每条曲线。

1⃝（红色，圆圈）Fairseq / DeepSpeed MoE 基线。

Fairseq 和 DeepSpeed MoE 的性能相同，因为它们使用了等效的 MoE 层实现。

2⃝（蓝色，菱形）TUTEL 内核（第 4.2 节中的快速编码和解码）+ 线性全对全。TUTEL 内核优化在小规模上提供较大的增益（16 个 GPU 上为 3.52 倍），而大规模增益变小（2,048 个 GPU 上为 1.04 倍）。图 15 显示了使用 TUTEL 内核而不是 Fairseq 的详细增益。3⃝（黄色，三角形）TUTEL 内核 + 2DH 全对全。 2DH All-to-All
在大规模上实现了显著的增益（在 2,048 个 GPU 上实现了 4.25 倍的增益）。4⃝（灰色，正方形）TUTEL 内核 + 2DH All-to-All
+ Flexible All-to-All。Flexible All-to-All 从 256 个 GPU 开始大规模地实现了增益，例如，与不使用相比，在 2,048 个 GPU 上实现了 1.24 倍的增益。5⃝（绿色，十字）TUTEL 内核 + 2DH All-to-All + Flexible All-to-All + Adaptive
Pipelining Degree。5⃝ 显示了优化流水线度与 Linear/2DH All-to-All 算法相结合的增益混合，在 16 个和 2,048 个 GPU 上分别实现了 1.43 倍和 1.04 倍的增益。 5⃝ 在更大规模上变得不那么重要，因为切片令牌的开销对 All-to-All 效率的影响更大。

细分不包括自适应并行切换，因为它静态使用自适应：r = 1，这不仅是因为 Fairseq MoE 正式支持这种并行性，而其他则不支持，而且还要确保 TUTEL 和 Fairseq MoE 所需的 All-to-All 通信大小完全相同，以便公平地比较 All-to-All 的改进。
### 2.1.1 NVIDIA Jetson Xavier at edge
NVIDIA Jetson 是领先的边缘 AI 平台。其高性能、低功耗计算能力使其成为深度学习和计算机视觉应用的理想平台。Xavier 是 NVIDIA Jetson 系列开发套件的最新成员，专为机器人、无人机和其他自主机器而设计。它搭载 512 核 NVIDIA Volta GPU（包含 Tensor 核心）以及 8 核 ARM v8.2 64 位 CPU。NVIDIA 为其 Jetson 系列提供了名为 JetPack SDK 的 SDK。JetPack 为开发套件提供了最新的操作系统镜像、库和 API、示例、文档以及开发工具。

SDK 加载到 Xavier 后，它会启动基于 L4T 的 Ubuntu 18.04（或目前可用的稳定版 Ubuntu 内核），该内核针对 64 位 ARM 架构进行了定制。NVIDIA L4T 提供引导加载程序、Linux 内核、必要的固件、NVIDIA 驱动程序和示例文件系统。它包含 CUDA、TensorRT、cuDNN、OpenCV 以及其他 AI 软件开发所需工具的预构建镜像。该套件尺寸小巧，仅为 100x87x16 毫米，提供 15W 至 30W 的各种功率模式。

## 2.2 Related work
Tao 等人 [6] 对使用虚拟机进行边缘计算的虚拟化框架进行了综述。他们详细讨论了虚拟机框架在辅助边缘计算方面面临的挑战。在研究了 OpenStack、KubeEdge 和 OpenEdge 等工业项目后，他们列出的主要挑战是虚拟机占用大量资源、虚拟机数据量大以及安全问题。他们研究的其他项目包括 Paradrop、AirBox、物联网云中间件、FocusStack、Amino、用于移动边缘计算的轻量级服务复制、SOUL 和 LAVEA。他们阐明了使用虚拟机会增加基于操作系统的开销（例如布局和调度）以及开发人员需要持续关注的安全问题，从而增加复杂性。

Kubernetes 在分布式边缘设备上的应用在研究界是一个鲜为人知的课题。华为的 Xiong 等人提出的 KubeEdge [7] 引入了一种边缘计算环境中的基础设施，主要目的是将云功能扩展到边缘。他们借助 Kubernetes 为云和边缘设备之间的通信提供了一个网络基础设施。然而，它并没有提供流水线架构，其结果也并未显示出比现有架构有任何改进。KubeEdge 是作为 Kubernetes 的一个新发行版构建的，此后，我们陆续推出了多个发行版，例如 microk8s 和 k3s，它们以更好的方式将 Kubernetes 引入了边缘计算。

Vayghan 等人 [8] 提出了一种在私有云中使用 Kubernetes 部署基于微服务的应用程序的架构。该架构利用 Kubernetes 为基于微服务的应用程序实现了高可用性。他们通过该架构在云环境中应对 Pod 故障和节点故障场景。

# 3 Enabling K8s on Edge
## 3.1 Overview
![[Pasted image 20250525140702.png]]
在这种方法中，每个微服务本质上都是 Kubernetes 集群中的一个 Pod。每个 Pod 与一个或多个 Docker 容器关联。这些容器包含与单体应用解耦的软件组件。Kubernetes 允许使用持久存储卷，并使用 DNS 通信协议在 Pod 内部或跨 Pod 的容器之间传输或共享数据。根据系统架构设计，Pod 可以
配置为在单个节点或跨不同节点运行。持久存储卷无法跨节点共享，因为它们绑定到节点的本地网络。因此，对于分布式计算而言，使用通信协议是必需的。由于我们希望提供在边缘节点和服务器之间平衡负载的方法，因此需要在这两个节点上运行 Pod；并且系统中可以有多个工作边缘节点。
由于 DNS 提供的便利，应用程序员无需跟踪每个生产者或消费者的 IP 地址进行数据传输。使用 YAML 文件，架构师可以通过绑定到容器的服务名称来配置跨容器的通信路径。DNS 不仅有助于通信，还有助于将容器部署到工作节点和主节点。同样，可以通过配置 Kubernetes 服务和部署的 YAML 文件将容器绑定到特定节点。这极大地简化了分布式计算，只需在 YAML 文件中配置一个值，就可以将作业部署到集群中的任何设备。本论文围绕使用这种方法在边缘设备上执行分布式 AI 处理展开。

必须建立一个通信协议，以便微服务之间进行通信，其基本要求是兼容 Docker 和 Kubernetes。本文使用了 gRPC，这是 Google 的开源 RPC 框架，使用 HTTP2 [9] 进行传输。Kubernetes 和 Docker 对 gRPC 有着广泛的支持，因此选择它作为通信协议是一个更重要的理由。

如上所述，本论文围绕机器学习应用的流水线式微服务方法展开。二维卷积就是一个这样的机器学习应用示例。该应用可以轻松解耦为卷积、ReLU 和 MaxPool 模块，这些模块也作为不同的小型应用程序（即微服务）运行，并带有一个将图像像素发送到卷积模块的生产者模块。每个模块都将使用 Docker 进行容器化，部署到 Kubernetes 集群中，以便它们通过 gRPC 进行通信。

## 3.2 GRPC for communication between containers
与许多 RPC 系统一样，gRPC 的核心思想是定义服务，指定可远程调用的方法及其参数和返回类型。默认情况下，gRPC 使用协议缓冲区 (Protocol Buffers) 作为接口定义语言 (IDL) 来描述服务接口和有效负载消息的结构。如果需要，也可以使用其他替代方案 [9]。使用协议缓冲区可以实现各种语言，例如 C++、Java、Python 等。这意味着，可以使用 C++ 编写服务器，并使用 Python 和 Java 分别编写客户端。这也意味着我们可以基于不同的技术构建微服务。

GRPC 通过其客户端-服务器模型帮助创建分布式服务，其中客户端可以直接调用同一台或不同机器上的服务器应用程序的过程，就像调用本地对象一样。顾名思义，gRPC 基于定义服务的理念，指定可远程调用的函数及其参数和返回类型。服务器实现此接口并运行 gRPC 服务器来处理客户端调用。在客户端，客户端有一个存根，提供与服务器相同的方法。

协议缓冲区 [10] 是一种灵活、高效、自动化的结构化数据序列化机制。协议缓冲区消息类型在 .proto 文件中定义，其中指定了用于通信的序列化结构。每条协议缓冲区消息都是一条小型的逻辑信息记录，包含一系列名称-值对。

## 3.3 Containerization using Docker on NVIDIA Xavier edge devices
Docker 允许程序员使用名为 Dockerfile 的配置文件来指定构建容器所需的软件和依赖项。Dockerfile 的示例如 [11] 中的 Dockerfile.server 文件所示。Docker 中一个有趣的概念是，应用程序镜像可以构建在其他基础镜像之上。第 1 行告诉 Docker 使用 grpc/cxx:1.12.0 作为基础镜像，其中包含一个标记为 1.12.0 版本的 gRPC C++ 安装。COPY 允许将软件程序和其他文件从本地计算机复制到容器文件系统。WORKDIR 设置容器将在其中运行的工作目录。RUN 允许使用当前镜像（在本例中为基础镜像 grpc/cxx:1.12.0）中指定的工具作为新层执行命令。EXPOSE 用于公开 Docker 容器的运行时端口（在本例中为服务于 gRPC 端口的端口）。所有这些都在创建镜像时内置到镜像中。容器的运行实例会查看指定的命令以及 CMD（在本例中，它运行 conv 可执行文件）。Docker 允许使用本地镜像，或者可以将镜像推送到 Docker Hub [12] 中的公共或私有镜像仓库，然后在需要使用镜像的设备上从 Hub 中检索这些镜像。

要在 NVIDIA Jetson Xavier 上运行 docker，请安装 jetpack 4.2.1 及更高版本。Docker 镜像可以使用基础镜像 nvcr.io/nvidia/l4t-base:r32.2.1 构建，其中 L4T 表示与 NVIDIA Tegra 处理器系列相关的 Linux for Tegra。
与 grpc/cxx 镜像不同，l4t-base 不包含 C++ 编译器和其他常用工具，但它允许像在 Linux Ubuntu 机器上一样安装这些工具。[11] 中的 Dockerfile.xavier 文件显示了可在 Xavier 或任何 Jetson 开发板上使用的 Dockerfile。它使用 l4t-base 镜像，并安装使用 g++ 和 protoc 编译器编译代码所需的工具链，以及安装它们所需的依赖项（第 3 行至第 19 行）。

## 3.4 Kubernetes on NVIDIA Xavier edge devices
Kubernetes (k8s) 与 Linux 类似，拥有众多发行版，例如
microk8s、minikube、k3s 等。Minikube 在虚拟机 (VM) 中运行单节点 Kubernetes 集群，方便用户快速上手。然而，它对跨节点运行的支持较差，也就是说，你可以在虚拟机中的一台机器上运行 k8s 集群，而无法将其他机器加入集群，因为它们也必须位于该虚拟机上。不过，k8s 文档主要关注 minikube，因此初学者可以从这个发行版开始。此外，它不支持在边缘设备上运行。Microk8s 是 k8s 的轻量级版本，它剥离了一些功能，并允许以插件的形式启用它们。据称，它对边缘设备和物联网 (IoT) 提供了良好的支持，同时还支持工作站和 GPGPU。对于边缘服务器模型来说，这是一个绝佳的选择，但许多用户已经证实，microk8s 上的端口会扰乱该设备上的 IP 表，并且很难调试服务之间的通信。

K3s 是另一个轻量级的 K8s 版本，其整个二进制文件大小不到 40MB。与 microk8s 类似，它提供了精简的 K8s 功能，可以作为插件随时添加。K3s 由 Rancher 维护，Rancher 致力于为 Kubernetes 和 Kubeflow 提供持续的支持和帮助工具，因此能够频繁更新 K3s。K3s 上的端口配置非常简单，可以使用服务名称通过 gRPC 连接到服务器端口。K3s 提供了一个便捷的安装脚本，该脚本配置为将 K8s 作为服务运行。该服务在其进程崩溃或终止后以及机器重启时都会重启。K3s 基于主从节点概念构建，常规安装可以通过代码清单 3.2 中第 1 行的命令完成。要在工作节点上安装，请确保它们运行 Docker，并在工作节点上运行第 2 行的命令。使用第 4 行的命令，将 mynodetaken 替换为在主节点上检索到的令牌。将 myserver 替换为主节点的 IP 地址。需要注意的一点是，在线文档中尚未提及，不要从工作节点访问集群，而只能从主节点访问。使用第 5 行的命令，验证主节点是否识别出一个工作节点已加入集群。

## 3.5 Distributed AI processing on edge
本节讨论如何在 Kubernetes 的帮助下在边缘设备上实现分布式 AI 处理。k3s 已被证实是此方法的理想选择，并且使用此方法的先决条件是安装 Docker v19 及更高版本，以及 k3s v0.9.1。0.9.1 版本在边缘设备中普遍使用的 ARM 架构上已被证明稳定，因此建议集群中的所有节点都使用该版本。服务器节点需要安装 k3s-server，边缘节点需要安装 k3s-agent。

单体代码需要解耦成尽可能小的软件组件，以便处理数据并与其他组件通信。动机部分解释了使用 gRPC 进行通信的原因，因此所有解耦的组件都需要拥有能够根据需要实际发送和/或接收数据的软件。生成的微服务可以部署在 Kubernetes 集群上，并以 YAML 文件作为部署和服务在所需的边缘节点上运行。

### 3.5.1 Microservice based pipeline architecture
微服务方法与 gRPC 通信相结合，实现了流水线架构，其中，在处理流式应用程序时，流水线中的阶段数与集群中微服务的数量相同，而本文旨在帮助人工智能中的流式应用程序。图 3.2 展示了如何在 Kubernetes 集群中使用容器化微服务实现流水线，该集群与运行容器的节点无关。

![[Pasted image 20250525142016.png]]
Figure 3.2: Pipelining using containerized microservices in Kuberentes cluster

将每个微服务部署为单独的 Pod 还可以为其分配单独的服务名称。

Pod 1、2 和 3 包含图 3.2 中示例的 gRPC 服务器实现。该服务器实现需要在本地主机的特定端口号上获取远程服务。Pod 2、3 和 4 中的客户端实现无需连接到相应服务器的 IP 地址，而是使用集群上相应部署的服务名称进行连接。由于 gRPC 提供的 DNS 功能以及 Docker 和 Kubernetes 的支持，这种管道结构也可以支持分布式系统，客户端只需通过服务名称连接到其服务器即可。


# 4 EXPERIMENTS AND RESULTS
## 4.1 Experimental setup
为了评估和测试我们的系统，我们搭建了一个异构系统，如表 4.1 所示，该系统包含一个基于 x86 的服务器和一个基于 ARM 的边缘设备（NVIDIA Jetson AGX Xavier）。这两个设备都需要安装 Docker v19.03.5 和 k3s Kubernetes v0.9.1 等基本工具。服务器上已安装 Master（k3s 称之为服务器），边缘 Xavier 设备上已安装 Worker（k3s 称之为代理）。CUDA 10.0 及以上版本是在 Docker 中构建镜像的另一个基本依赖项。

目标是将单体应用解耦为微服务，并在服务器和 Xavier 上运行它们。我们选择了一个卷积应用进行功能测试和评估，以证明其在延迟和吞吐量方面的性能。卷积应用的代码可以在 [11] 中的 conv.c 文件中找到。从代码中可以看出，它可以解耦为四个组件：主组件、卷积、reLU 和 maxpool。将它们容器化将获得四个微服务。每个微服务都必须包含一个 gRPC 模块，并遵循连接相邻微服务的每个链路之间的客户端-服务器拓扑结构。使用 gRPC 时，请使用字节数组作为数据类型，因为其他数据类型在通信速度方面效率低下。这些微服务处理流图像，并将处理后的图像发送到流水线中的下一个微服务。

![[Pasted image 20250525142302.png]]

每个微服务都会访问内部的计时器，并存储接收、处理和发送每张图片所需的时间。传输 1000 张图片的总时间将以打印日志的形式发布。处理一张图片的平均时间可以通过将总时间分成 1000 个等份来计算，并以此推断系统的延迟和吞吐量。此外，通信和计算时间也会被记录下来。测试针对各种尺寸的图片进行，包括 128x128、256x256、512x512 和 1024x1024。

![[Pasted image 20250525142502.png]]

上述设置在边缘设备和服务器设备上均有复制。首先，所有微服务都运行在服务器上，如图 4.1 所示，进行实验。接下来，所有微服务都运行在边缘设备上，如图 4.1 所示，并记录延迟和吞吐量结果。此外，为了测试系统在分布式计算下的性能，微服务分别在边缘设备和服务器上运行，使得一个微服务要么在边缘设备上运行，要么在服务器上运行，即如图 4.2 所示的分布式节点设置。此设置展示了管道对分布式计算吞吐量的影响。


## 4.2 Result
### 4.2.1 Results on Server
这些结果是为了在服务器上单独测试微服务的功能和性能而生成的，如图 4.1 所示。图 4.3 反映了单体架构和微服务架构在四种不同大小的图像上完整处理一张图片时端到端延迟的比较。单体架构表示未容器化的卷积应用程序，而微服务表示将卷积解耦为四个组件，这些组件使用 Docker 进行容器化，通过 gRPC 进行通信并在 Kubernetes 集群中运行。如图所示，与单体架构相比，微服务架构加剧了应用程序的端到端延迟。在微服务架构中，计算时间基本保持不变，但通信时间会达到延迟，从而降低性能。从该图中可以得出一个重要的结论：由于 gRPC 中使用了字节数组，因此通信时间的增长速度不如计算时间那样呈指数级增长。字节数组允许通过通道一次性发送最多 4MB 的数据。
![[Pasted image 20250528141129.png]]
图 4.4 展示了微服务方法针对不同大小的输入图像引入的通信计算比 (CCR)。由于计算时间呈指数级增长，而通信时间增量极小，因此 CCR 随着图像大小的增加而呈指数级下降。当图像尺寸为 512x512 时，CCR 降至 1 以下，而当图像尺寸为 1024x1024 时，CCR 则远低于 0.5。

![[Pasted image 20250528141457.png]]
表 4.2 展示了单片和微服务架构下流水线对服务器吞吐量的影响。吞吐量以流水线传输 1000 张 720p（1280x720）大小的图像并执行二维卷积的帧率来表示。微服务架构下，当应用程序以四个微服务运行时，系统吞吐量提高了 76.58%。
图 4.5 展示了不同流水线配置下的吞吐量实验结果。两阶段流水线表明，单片应用程序仍然可以解耦为四个阶段，但需要解耦为两个阶段。同样，三阶段流水线运行三个微服务，四阶段流水线运行四个微服务。
![[Pasted image 20250528141744.png]]


### 4.2.2 Results on Edge - NVIDIA Xavier
这些结果是为了测试边缘微服务的功能和性能而生成的，如图 4.1 所示。图 4.6 反映了单体架构和微服务架构在四种不同大小的图像上完整处理一个图像的端到端延迟比较。单体架构表示未容器化的卷积应用程序，而微服务表示将卷积解耦为四个组件，这些组件使用 Docker 进行容器化，通过 gRPC 进行通信并在 Kubernetes 集群中运行。如图所示，与单体架构相比，微服务架构加剧了应用程序的端到端延迟。在微服务架构中，计算时间基本保持不变，但通信时间会影响延迟，从而降低性能。
![[Pasted image 20250528141941.png]]
图 4.7 展示了微服务方法针对不同大小的输入图像引入的通信计算比 (CCR)。由于计算时间呈指数级增长，且在图像大小达到 512x512 之前通信时间的增量最小，因此 CCR 随着图像大小的增加而呈指数级下降。CCR 的下降幅度不如服务器结果中的那样呈指数级，因为随着图像大小的增加，边缘计算的计算时间并没有像服务器计算时间那样大幅增加。
![[Pasted image 20250528142127.png]]
表 4.3 展示了单片和微服务方法在边缘端对流水线吞吐量的影响。吞吐量以流式传输 1000 张 720p（1280x720）大小的图像并对其进行二维卷积的帧率来表示。当应用程序以四个微服务的形式运行时，微服务方法将系统吞吐量提高了 103.94%。图 4.8 展示了不同流水线配置下的吞吐量实验结果。两阶段流水线表明，单片应用程序仍然可以解耦为四个阶段，但应该解耦为两个阶段。同样，三阶段流水线运行三个微服务，四阶段流水线运行四个微服务。
![[Pasted image 20250528142354.png]]

![[Pasted image 20250528142405.png]]

### 4.2.3 Distributed Computing Results - Edge Server model
通过这些结果研究了服务器和边缘的系统特性后，本节将讨论边缘服务器模型上的分布式计算结果。
在本节中，边缘服务器和服务器都作为微服务 pod 的节点，如图 4.2 所示。

#### Results for different Edge-Server configurations
这些结果是在边缘服务器模型上维持四阶段流水线架构的情况下得出的。以下每个结果都侧重于在 Kubernetes 集群上运行 4 个微服务，处理特定输入图像大小的卷积时的端到端延迟或吞吐量。需要注意的一点是，卷积是在现实世界中由摄像头输入的输入图像流上执行的。因此，对于分布式系统，第一个微服务必须在边缘设备上运行。此外，在服务器上运行第二个微服务，然后在边缘设备上运行第三个微服务是没有意义的，这只会由于边缘和服务器之间通信时间增加而加剧延迟。为了理解以下章节中的结果，我们将遵循以下术语：

- SSSS: All microservices run on Server.
- ESSS: Microservice 1 runs on Server while microservices 2, 3 and 4 run on
Edge.
- EESS: Microservices 1 and 2 run on Server while microservices 3 and 4 run on
Edge.
- EEES: Microservices 1, 2 and 3 run on Server while microservice 4 runs on
Edge.
- EEEE: All microservices run on Edge.

#### 4.2.3.2 Latency results for different Image sizes
图 4.9、4.10、4.11 和 4.12 展示了 4.2.3.1 节中所述的卷积应用的四阶段流水线微服务架构的所有五种配置的端到端延迟比较。这些图中的结果由于微服务处理的图像大小不同，以及因此需要在集群中跨 pod 传输的数据大小不同而有所不同。从分布式系统的角度来看，ESSS、EESS 和 EEES 是重要的配置。
可以看出，边缘上的处理和数据通信远慢于服务器上的处理速度。然而，通过更多实验，我们发现，与两个边缘设备之间的通信速度相比，边缘和服务器之间的通信速度降低非常小。因此，对于所有图像大小，EEES 和 EEEE 配置的延迟大致相同。

#### 4.2.3.3 Throughput results for different Image sizes
图 4.13、4.14、4.15 和 4.16 展示了 4.2.3.1 节中卷积应用的四阶段流水线微服务架构在所有五种配置下的吞吐量延迟比较。如前文 4.2.3.2 节所述，这些图中的结果会有所不同，因为微服务处理的图像大小不同，因此需要在集群中各个 Pod 之间传输的数据大小也不同。随着图像大小的增加，不同配置下的吞吐量变化较小。尽管卷积应用可以解耦为四个微服务，但这并不是理想的解耦方案，因为中间的两个微服务（卷积和 ReLU）是计算密集型的，这被证明是流水线的瓶颈。


# 5 CONCLUSION AND FUTURE WORK
## 5.1 Conclusion
这项研究探讨了如何利用 Kubernetes 进行分布式计算，以辅助边缘服务器系统架构的 AI 处理。它隐式地利用 Docker 将容器构建为微服务，并将其部署为 Kubernetes 集群中的 Pod。
实验结果证明，这种方法有助于提高单节点集群上二维卷积的吞吐量。然而，实验结果无法清晰展现的更宏观层面是，这种方法为分布式系统上的计算机视觉应用提供了便捷性。从 YAML 配置文件中可以看出，它们允许开发人员轻松地关联节点以运行特定的 Pod。正是这种便捷性，让我们得以探索 Kubernetes 在分布式系统中的可扩展性。

## 5.2 Future Work
这项工作已被证明能够解决分布式系统相关的问题，同时由于流水线技术的应用，在吞吐量方面也提供了更佳的性能。然而，本文仅通过一个示例证明了这一点。未来的工作将涉及使用相同架构运行真实应用程序。此外，该架构的构建过于简单，无法支持 Kubernetes 的其他重要功能，例如自动发布更新和将 Pod 智能部署到节点，从而在计算能力最强、可用度最高的节点上运行它们。
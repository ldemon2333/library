@article{liu2024ed,
  title={ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge Devices},
  author={Liu, Xiang and Song, Yijun and Li, Xia and Sun, Yifei and Lan, Huiying and Liu, Zemin and Jiang, Linshan and Li, Jialin},
  journal={arXiv preprint arXiv:2410.11650},
  year={2024}
}

# 摘要
深度学习模型越来越多地应用于资源受限的边缘设备进行实时数据分析。近年来，Vision Transformer 及其变体在各种计算机视觉任务中展现出卓越的性能。然而，其高计算需求和低推理延迟为在资源受限的边缘设备上部署此类模型带来了巨大挑战。为了解决这个问题，我们提出了一个新颖的框架 ED-ViT，旨在高效地拆分和跨多个边缘设备执行复杂的 Vision Transformer。我们的方法包括将 Vision Transformer 模型划分为多个子模型，每个子模型专用于处理特定的数据类子集。为了进一步降低计算开销和推理延迟，我们引入了一种逐类剪枝技术来减小每个子模型的大小。通过在五个数据集上使用三种模型架构进行大量实验，并在边缘设备上进行实际实现，我们证明了我们的方法显著降低了边缘设备上的推理延迟，并将模型大小分别缩减了高达 28.9 倍和 34.1 倍，同时保持了与原始 Vision Transformer 相当的测试精度。此外，我们将 ED-ViT 与两种在边缘设备上部署 CNN 和 SNN 模型的先进方法进行了比较，评估了精度、推理时间和模型整体大小等指标。我们的全面评估强调了所提出的 ED-ViT 框架的有效性。

# intro
近年来，深度学习模型越来越多地部署在资源受限的边缘设备上，以满足工业系统日益增长的实时数据分析需求 [1]–[3]，并在视频图像分析和语音识别等各种应用中展现出卓越的性能。卷积神经网络 (CNN) [4]（例如 VGGNet [5] 和 ResNet [6]）以及脉冲神经网络 (SNN) [7] 在许多边缘计算场景中都取得了令人满意的性能。随着该领域的发展，研究人员正在探索在边缘设备上部署更复杂的结构化模型，以进一步提升性能。Transformer 架构 [8] 彻底改变了自然语言处理 (NLP) 任务，并激发了计算机视觉领域的类似进步。视觉变换器 (ViT) 模型 [9] 及其变体在各种计算机视觉任务中展现出卓越的效果，包括图像分类 [10]、[11]、物体检测 [12]–[14]、语义分割 [15]、[16]、动作识别 [17]、[18] 以及音频频谱图识别 [19]。ViT 的成功激发了人们对将其功能应用于边缘计算应用的兴趣。

然而，鉴于这些模型配置的复杂性，机器学习技术的快速发展也增加了对计算资源和内存的需求。要通过 ViT 实现更高的准确率，需要大量的计算能力和内存，这对在边缘设备上的部署提出了挑战。例如，ViT-Base [9] 包含超过 8670 万个参数，需要大约 330MB 的内存。研究人员现在面临着在资源受限的情况下部署如此复杂模型的困境。

先前旨在降低部署开销的研究主要集中在压缩 Vision Transformer 模型上。
这些方法可以分为三大类：
(1) 架构和层次结构重构 [20], [21]；(2) 编码器模块增强 [22]–[27]；以及 (3) 集成方法 [28], [29]。然而，由于这些方法试图压缩大型模型以适应内存受限的边缘设备，因此它们通常会面临推理精度低或推理延迟高等问题。

为了开发一种解决方案，以减轻准确率下降的影响，并能够在资源受限的边缘设备上高效部署 Vision Transformer，我们旨在利用多个边缘设备的协作，并提出了一个名为“边缘设备 Vision Transformer”的 Vision Transformer 拆分框架，简称 ED-ViT。如图 1 所示，ED-ViT 首先将原始 Vision Transformer 划分为几个较小的子模型，其灵感来自拆分学习 (SL) 的概念。然而，与不考虑边缘设备约束的传统 SL 不同，每个小子模型负责检测类别的特定子集，并部署在资源受限的边缘设备上。然后，ED-ViT 采用模型剪枝技术进一步减轻每个子模型的计算负载和处理需求。为了优化模型分配，我们设计了一种贪婪分配算法，该算法同时考虑了模型计算资源和内存资源。除了集成前面的步骤外，ED-ViT 还使用多层感知器 (MLP) 模型融合所有子模型的结果。
我们在五个数据集上进行实验，以评估 ED-ViT 框架在边缘设备上的有效性，尤其是在低功耗视频分析领域。通过准确率、推理延迟和模型大小三个关键指标的测量结果一致证明了 ED-ViT 的显著优势。此外，我们将 ED-ViT 与拆分 CNN 和 SNN 的方法进行了比较，突显了在资源受限的边缘设备上部署 Vision Transformer 的巨大潜力，可以在保持小模型大小和低推理延迟的同时实现高精度。我们的主要贡献总结如下：

- 这是首项结合剪枝和拆分技术，将 Vision Transformer 部署到边缘设备的研究。我们提出了一个框架，充分利用 Vision Transformer 的功能，允许多个边缘设备协作，在实际应用中实现分布式推理。
- 我们引入 ED-ViT 来解决 Vision Transformer 的拆分问题，通过将复杂的原始模型分解为子模型，并应用剪枝技术来缩减每个子模型的大小。ED-ViT 采用组合贪婪方法进行模型分配，通过减小模型大小、最小化推理延迟并保持高精度，有效地解决了上述问题，并在这三个指标之间实现了权衡。
- 我们基于三个计算机视觉数据集和两个音频识别数据集，在三个 ViT 结构上进行了广泛的实验。此外，我们在 Raspberry Pi 4B 上实现了 ED-ViT，结果表明，我们的框架显著降低了边缘设备的推理延迟，并降低了整体内存使用量，同时在各种应用中的准确率损失几乎可以忽略不计。
![[Pasted image 20250625090847.png]]

## Related works
在资源受限的环境中部署 Vision Transformer 模型面临着巨大的挑战，因为它们需要大量的计算和内存。以下方法通过剪枝解决了 Vision Transformer 的资源限制问题，并涵盖了以下局部和全局策略。

局部剪枝技术专注于移除模型特定层内的冗余组件。例如，PVT [30] 及其后继者 PVTv2 [31] 在 Transformer 主干网络中引入了金字塔层次结构，从而在减少计算量的同时实现了高精度。[32] 在训练过程中应用稀疏性正则化，随后修剪线性投影的维度，以减少不太重要的参数。[33] 修剪多头自注意力 (MHSA) 和前馈网络 (FFN)，这些网络通常是冗余组件。[34]、[35] 提出网络剪枝，通过减少标记来降低复杂性和模型大小。其他值得注意的贡献包括 DToP [36]，它支持语义分割任务的早期标记退出。相反，全局剪枝技术采用综合视角，通过评估和修剪整个网络中神经元或层的整体重要性。 SAViT [37] 通过协同优化实现结构感知的 Vision Transformer 剪枝。例如，CP-ViT [38] 系统地评估头部和注意力层的重要性以进行剪枝；而 Evo-ViT [39] 则识别并保留重要的标记，从而丢弃那些重要性较低的标记。此外，Skip-attention 方法 [40] 有助于省略整个自注意力层，从而体现了一种全局剪枝方法。X-pruner [41] 采用可解释性感知的掩码来指导其剪枝决策，从而推进了一种更明智的全局剪枝策略。此外，UP-ViT [42] 引入了一个统一的剪枝框架，利用 KL 散度来指导剪枝的决策过程，而 LORS [43] 通过在堆叠模块之间共享大部分参数来优化参数使用，从而减少所需唯一参数的数量。

在现有的剪枝方法中，UP-ViT [42] 与我们的方法最为相似。然而，需要注意的是，这些技术不能直接应用于边缘设备：它们通常在剪枝率较高时性能不佳，或在剪枝率较低时计算开销较大，因此不适用于资源受限的边缘环境。相比之下，我们的工作引入了一种基于类的全局结构化剪枝方法，解决了这些局限性。我们的方法与大多数先前的方法正交，并且不涉及可训练参数，从而有助于实现更稳定的性能。

有多种方法专注于部署 Vision Transformer 边缘设备，可分为三大类。

架构和层级重构：HVT [20] 使用层级池化压缩序列分辨率，
从而降低计算成本并增强模型可扩展性。
LeViT [21] 是一个混合模型，融合了 CNN 和 Transformer 的优势。对于图像分类任务，它利用 LeNet [4] 的层级结构来优化准确率和效率之间的平衡，并在特征映射阶段使用平均池化。MobileViTv3 [44] 提出了对融合模块的改进，解决了缩放问题并简化了学习任务。

编码器模块增强：ViL [22] 引入了一个多尺度视觉长格式器 (longformer)，可在编码高分辨率图像时降低计算和内存复杂度。
Poolformer [23] 特意用一个简单的池化层替换了 Transformer 中的注意力模块。LiteViT [24] 引入了一个紧凑的 Transformer 主干网络，其中包含两个新的轻量级自注意力模块（自注意力和递归空洞自注意力），以减轻性能损失。DualViT [26] 降低了特征图分辨率，由两个对偶块和两个合并块阶段组成。MaxViT [25] 将注意力分为局部和全局部分，并通过窗口和网格注意力将其分解为稀疏形式。 SlideTransformer [27] 提出了一个滑动注意力模块来解决计算复杂度随注意力模块呈二次方增长的问题，而 EdgeViT [45] 使得基于注意力的视觉模型在准确率和设备效率之间进行权衡时，能够与最好的轻量级 CNN 相媲美。

集成方法：一些方法将上述两种方法结合起来。CeiT [28] 结合了 Transformer 和 CNN 的优势，弥补了各自的不足，并加入了图像到标记模块、局部增强的前馈层和逐层类标记注意力机制。CoAtNet [29] 结合了深度卷积，并通过相对注意力机制简化了传统的自注意力机制，通过堆叠卷积和注意力层来提高效率。DeViT [46] 也分解了 Vision Transformer 以进行协作推理。然而，即使在拆分 ViT-Small 时，DeViT 也会为每个子模型训练一个 ViT-Large，并使用模型蒸馏来提高准确率，这会带来显著的训练开销。此外，DeViT 提供的最小模型大小也超过 90MB。然而，他们从未考虑将剪枝与特定的类联系起来，这在需要高性能和低内存使用率时限制了他们的方法。

C. 拆分学习
目前，将 Vision Transformer 与拆分学习相结合的研究主要集中在联邦学习上，旨在解决多客户端环境下的数据隐私和高效协作问题 [47]–[49]。在这种环境下，大型模型的内部结构被拆分到多个较小的设备上，然后再进行融合 [50]。
然而，这些方法并不适用于在边缘设备上部署 Vision Transformer。
传统的机器学习模型拆分通常涉及将大型模型划分为多个较小的子模型，这些子模型可以在资源受限的设备上协同执行，这为在边缘设备上部署模型提供了一种颇具前景的技术。Splitnet [51] 将类别聚类成组，将深度神经网络划分为树状结构的子网络。[52] 根据通信信道的状态动态地对模型进行划分。 Nnfacet [1], [2]
将大型 CNN 拆分为轻量级的、特定于类别的子模型，以适应设备内存和能耗限制，这些子模型稍后会进行融合。[3] 采用类似的方法，将深度 SNN 拆分到边缘设备之间。Distredge [53] 使用深度强化学习来计算 CNN 模型的最优划分。

据我们所知，我们的工作首次探索了 Vision Transformer 模型分区在边缘部署中的应用，标志着对该领域的重大贡献。借鉴先前研究 [1]–[3] 的灵感，我们的框架 ED-ViT 引入了一种创新方法，将多类别的 ViT 模型分解为多个特定于类别的子模型，每个子模型执行一个分类子集。
与依赖通道剪枝不同，ED-ViT 采用了专为 Vision Transformer 独特架构设计的高级剪枝技术。


# Problem Formulation
表一列出了三种具有代表性的 Vision Transformer 模型的结构：ViT-Small、ViT-Base 和 ViT-Large。
运算次数通常用于估算硬件级别的计算能耗。在 Vision Transformer 模型中，几乎所有浮点运算 (FLOP) 都是乘法累加 (MAC) 运算。
对于 Patch Embedding、FFN 和 MLP Head，它们的运算次数很容易推断，因为它们遵循全连接 (FC) 结构，其中 MAC 计数为 (2FCin + 1) × FCout，其中 FCin 和 FCout 分别表示输入和输出特征。对于 MHSA，假设块数为 p，每个块的维度为 dp，嵌入维度为 d，注意头数量为 h，则用于生成 Q、K 和 V 矩阵的线性投影的 MAC 为 3 × p × d2/h。QKT 的 MAC 为 p2 × d/h，softmax 运算以及与 V 相乘的 MAC 为 p2 × d/h。对于 h 个注意头，总 MAC 为
h × (3 × p × d2/h + 2 × p2 × d/h) = 3 × p × d2 + 2 × p2 × d。
基于此分析，假设剪枝后的模型遵循相同的结构，则可以估算出能耗与 FLOP 成正比。
因此，我们将问题表述如下。假设我们需要处理总共 L 个推理样本，N 个边缘设备的集合表示为 D。每个边缘设备 Di 的可用内存和能量（边缘设备的 FLOP）[54] 分别表示为 Mi 和 Ei。子模型集合 {Models} 中子模型 Modelj 的每个推理样本的 FLOP（能耗）表示为 ej，该值基于之前的能耗估算计算得出。为了公式化 Vision Transformer 划分和基于边缘设备的部署问题，我们将目标函数定义为 max{Modelj}minDi∈D{Ei−Lej}，旨在最小化最大推理延迟，因为推理延迟与边缘设备的计算能力密切相关。此外，所有 N 个推理样本的融合结果的准确率 afus 必须大于或等于所需的推理准确率 Are；所有子模型的总内存大小不应超过内存预算bu。
优化问题可以正式表述如下，其中xie是一个二元决策变量：1表示部署在边缘设备Di上的子模型负责类别e，否则为0。每个子模型学习C中类别的特定子集。此外，子模型j的内存消耗（表示为size(Modelj)，用mj表示）必须小于已部署边缘设备的可用内存大小：

![[Pasted image 20250625194746.png]]


# METHdology
本节介绍为解决 (1) 中概述的优化问题而提出的 ED-ViT 框架的设计。我们首先解释 ED-ViT 的主要工作流程，然后详细描述其中涉及的四个关键步骤。
A. 设计概述
如图 1 所示，ED-ViT 利用了 Vision Transformer 的独特特性以及多个边缘设备的协作。该框架包含 N 个并发边缘设备，用于分布式推理，并结合轻量级 MLP 聚合来得出最终的分类结果。
最初，原始 Vision Transformer 在整个数据集上进行训练，以在分类任务中实现较高的测试准确率。ED-ViT 框架由四个主要部分组成：模型拆分、剪枝、分配和融合。在模型拆分过程中，Vision Transformer 模型被划分为子模型，每个子模型负责一个类别子集。为了减少计算开销，这些子模型会使用模型剪枝技术进一步剪枝。随后，
子模型会被分配到合适的边缘设备，
并考虑优化问题。最后，
聚合设备会融合边缘设备的输出，
从而生成最终的推理结果。每个组件的具体细节如下。
![[Pasted image 20250625090847.png]]

B. 模型拆分
在原始 Vision Transformer 中，不同的 head 有助于从样本中进行学习和推断。然而，对于某些类别，维护 head 之间的所有连接可能存在冗余。因此，ED-ViT 会修剪这些连接并重建 head，保留更多的 head 可以保留更多的参数和连接。如算法 1 所示，每个 Vision Transformer 子模型都会根据 head 数量阈值及其相关类别进行修剪，并遵循相对公平的工作负载分配。随后，使用贪婪搜索机制来确定最适合部署特定子模型的边缘设备模型分配方案，同时考虑能耗和内存限制。如果总内存大小超出预算或未找到合适的方案，则采用迭代方法调整待修剪内存大小最大的子模型的 head 数量，重复分配过程，直到所有子模型都成功分配给边缘设备。剪枝和
贪婪分配方法分别如算法 2 和算法 3 所示，分别位于第 IV-C 节和 IV-D 节。

C. 模型剪枝
我们相信，降低 Vision Transformer 的计算负担将显著有助于降低分布式边缘设备环境下的推理延迟。我们专注于原始的 Vision Transformer 架构 [9]，该架构因其简单性和明确的设计空间而被选中，并着重于在不同模块之间重新分配维度，以在计算效率和准确率之间实现更均衡的权衡，如图 2 所示。
可剪枝参数分析：
ViT 模块中的主要可剪枝组件，如图 2 所示。
• 残差连接通道（红色，d）：Transformer 模块内快捷连接之间的通道。
• MHSA 中的头部（绿色，h）：查询、键、值投影的维度（dq、dk、dv）。
• 前馈网络 (FFN) 隐藏层维度（蓝色，c）：用于扩展和缩减的隐藏层维度 c。
剪枝过程：
如图 2 所示，剪枝过程分阶段进行，每个阶段重点关注一个可剪枝的组件。我们计算原始模型和剪枝后模型的输出分布之间的 KL 散度，以评估每个组件的重要性，如下所示：
DKL(P ∥ Q) =
�
i
P(i) log P(i)
Q(i)
其中，P(i) 表示原始模型的输出分布，Q(i) 表示剪枝后的分布。
在第一阶段，我们重点剪枝残差连接的通道（如图红色所示）。利用 KL 散度，我们识别并剪枝贡献最小的通道，将维度从 d 降低到 s × d，剪枝因子 s 控制参数的减少程度。我们以第 j 个子模型为例：我们设定 s = (h − hpj)/h，有效控制剪枝和参数缩减的程度。这有助于简化层间信息流，而不会显著影响模型性能。然后，我们不是直接移除 MHSA 模块中的整个 head，而是在多个 head 中修剪查询、键和值投影（dq、dk 和 dv）中最不重要的维度。此过程有效地将 head 的总数减少到 s × h，而无需完全丢弃任何 head，从而在降低注意力机制复杂度的同时保持其平衡的表示。投影的维度会进行相应的缩放，以反映合并和剪枝过程，确保模型保留其捕获 token 交互的能力。最后阶段涉及修剪 FFN 中的隐藏维度 c，如蓝色所示。通过计算 KL 散度，我们识别出最不重要的神经元，并将隐藏维度从 c 降低到 s × c。在每个剪枝阶段之后，都会对模型进行微调，以弥补参数减少可能造成的性能损失。这确保了剪枝后的模型能够达到与原始模型相似的准确率，同时所需的计算资源更少。
![[Pasted image 20250625200510.png]]

Vision Transformer 模块的结构化剪枝。左图：ViT 模块中可剪枝组件的图示。右图：相应的顺序剪枝过程。我们的方法针对三个关键组件：(1) 残差连接中的通道（红色，表示为 d），(2) MHSA 模块中的 head 数量（绿色，表示为 h），以及 (3) FFN 中的隐藏层通道（蓝色，表示为 c）。剪枝过程分为三个阶段：残差连接通道、MHSA head 和 FFN 隐藏层维度。黄色区域表示当前阶段正在剪枝的参数，灰色区域表示之前剪枝过的参数。

D. 模型分配
为了解决 (1) 中表达的优化问题，我们提出了一种贪婪搜索算法，用于将 Vision Transformer 子模型分配给边缘设备。如算法 3 所示，首先根据子模型的能耗对其进行排序。ED-ViT 首先根据模型大小分配计算密集度最高的子模型，该大小与计算开销成正比，如第三节所述。然后，该算法迭代分配剩余的子模型，以最大化系统的可用能耗。最初，选择计算能力最高的设备。如果剩余内存和能耗可以容纳子模型，我们将更新设备的可用内存和能耗。否则，如果子模型超出了设备的内存容量，则将内存耗尽的设备从集合中移除。如果没有剩余设备，则表示当前的修剪结果无法部署所有子模型。在这种情况下，算法终止，并且
ED-ViT 框架会根据新的头部修剪参数重新修剪子模型，如算法 1 中所述。
最后，算法输出模型分配计划 MA，
表示子模型到边缘设备的映射。

如第三部分所述，Vision Transformer 子模型的划分和分配问题可以表述为一个 0-1 背包问题，其中每个边缘设备具有不同的可用内存和能量。每个子模型负责一组特定的类别，并且可以在单个设备上部署多个子模型。我们执行了协同优化，将 Vision Transformer 模型划分为多个子模型（如算法 1 所示），并使用贪婪搜索分配机制将这些子模型部署到边缘设备上（如算法 3 所示）。该方法为上述问题提供了一个相对最优的解决方案。我们大量的实验证明了我们框架设计和算法的有效性。


Vision Transformer 与传统的 CNN 和 SNN 模型相比，取得了更高的准确率。然而，这些模型在边缘设备上的性能此前尚未被直接比较过。Nnfacet [2] 提出了一种将 CNN 拆分到多个边缘设备上的方法，该方法采用了滤波器剪枝技术 [65]，这与我们的方法不同。EC-SNN [3] 利用卷积脉冲神经网络 (CSNN) [7] 将 CNN 转换为 SNN，采用了类似的策略。这两种方法都以 VGGNet [5] 为主干网络，并且都是基于通道的方法。在我们的实验中，这些方法的基准模型是他们论文中的 VGGNet-16，它的内存大小也与 ViT-Base 相似，并且取得了最佳的原始结果以供比较。我们遵循他们论文中的超参数进行实验。

CIFAR-10 数据集上的准确率结果如表三所示。根据结果，我们观察到 CNN 的表现优于 SNN，而我们针对 ViT-Base 的 ED-ViT 方法的准确率优于 CNN 和 SNN 方法。
ViT-Base、CNN 和 SNN 的原始准确率分别为
98.12%、93.64% 和 93.56%。在不同设备型号上，它们的平均准确率损失分别为 11.16%、8.5% 和 10.15%。由于 Vision Transformer 本身就具有较高的准确率，我们承认其准确率下降幅度略高于 CNN 和 SNN。这正是我们选择 Vision Transformer 的原因：充分发挥其卓越的性能。我们的方法在尺寸缩小 28.9 倍的情况下，保持了相当的准确率下降，与表 III 所示的最先进的基于 CNN 和基于 SNN 的方法相比，准确率分别提高了 4.06% 和 5.55%。

除了准确率结果外，我们还比较了边缘设备数量为 10 台时三种方法的推理延迟和总内存大小。结果如图 7 所示。结果显示，我们的 ED-ViT 方法与 SNN 和 CNN 相比，取得了最佳准确率，而其推理延迟远低于 SNN（4.36 倍）和 CNN（2.70 倍）。此外，由于 SNN 以模型规模小而闻名，ED-ViT 的总内存大小明显低于 CNN，与 SNN 相当。本实验表明，将 Vision Transformer 部署到边缘设备上可以满足延迟和内存限制，同时提供卓越的准确率结果。

![[Pasted image 20250625203630.png]]


传统上，脉冲神经网络 (SNN) 中最优神经模型的选择依赖于反复试验的方法，这种方法既耗时，有时也会导致神经模型选择结果不理想。本研究探索了将自动机器学习 (AutoML) 技术集成到 SNN 中，通过自动选择最优神经模型来简化 SNN 模型的设计流程。例如，在传统神经网络中，AutoML 通过自动选择和更新损失函数和超参数值，证明了其能够显著减少识别最佳性能架构所需的时间和精力。为此，我们提出了一个在 SNN 中应用 AutoML 的独立框架，重点介绍了这种方法如何提高最优神经模型选择过程的准确性、效率和可靠性，因为这些神经模型是设计 SNN 架构的核心原则。所提出的方法不仅加速了SNN模型的开发，而且还通过系统地识别可能被手动方法忽略的最佳配置来提升其性能。因此，为了验证我们的方法，我们使用不同的知名基准测试集（包含不同的神经元组，范围从100到3000个神经元）对所提出的架构进行了测试，并在图像分类任务中展示了最先进的结果。

# 1 Introduction
SNN 代表着神经计算领域的一个有希望的发展方向，因为与传统的人工神经网络 (ANN) [1]–[3] 相比，它提供了一种更具生物学现实性的大脑活动建模方法。此外，SNN 的功能与计算生物学中的遗传回路有些相似，其中两种蛋白质只有当超过某个阈值水平时才会产生绿色荧光蛋白 (GFP) [4], [5]。然而，SNN 的成功部署在很大程度上取决于选择合适或最优的神经模型及其相应的超参数值 [6], [7]。这种选择过程传统上采用反复试验的方法，这通常效率低下且耗时，因为 SNN 涵盖了各种各样的神经元模型，每种模型都有其自身的优势，适用于不同的任务或应用。例如，Izhikevich (Izh) 模型就特别适合硬件级实现 [8]。它在计算效率和生物真实性之间取得了平衡，有助于在利用相对较少的神经元数量的情况下产生准确的结果。这使得 Izh 模型成为嵌入式系统和资源受限环境的理想选择，在这些环境中计算资源有限。尽管像 Izh 模型这样的特定模型具有诸多优势，但挑战在于确定哪种神经模型和超参数配置能够为给定任务提供最佳性能 [9]。因此，SNN 的复杂性（包括其模型范围、广泛的超参数及其平衡值）凸显了这一挑战。即使在模型选择或参数调整方面存在微小变化，也可能导致网络性能的显著差异。这凸显了试错方法的局限性，它不仅消耗大量时间和计算资源，还增加了得出次优解的风险 [10]–[12]。

为了克服这些挑战，将 AutoML 技术集成到 SNN 模型选择过程中提供了一种有前景的解决方案。AutoML 可以自动搜索最佳神经模型（Leaky Integrate-and-Fire (LIF)、Non-Leaky Integrate-and-Fire (NLIF)、Izh）和超参数值（例如使用带有提前停止和学习率调整的训练循环），并系统地探索识别最有效模型及其参数配置的可能性。因此，本研究引入了一个独立的框架，将 AutoML 应用于 SNN 中最优神经模型的选择。我们的方法旨在简化神经模型选择过程（如本研究早期研究 [13] 中所述），减少对试错方法的依赖，并提高 SNN 在各种应用中的整体性能。我们通过在多个著名的基准测试集和数据集上测试我们的方法，并在训练中使用了从100到3000组不同数量的神经元，从而验证了我们的方法。结果表明，我们基于AutoML的方法不仅显著减少了模型选择所需的时间，而且在图像分类任务中达到了最佳性能，并且能够更好地理解使用不同隐藏层神经元利用率的神经模型行为。另一方面，该方法旨在探索SNN的生物实用性和计算效率之间的权衡，这有助于理解使用相同网络架构的不同神经元模型在标准任务上的表现。

# 2 Backward
AutoML 是一个新兴领域，旨在通过使用混合模型执行 [14]–[16] 或自动化模型设计过程（包括特征工程、模型选择、超参数优化和流水线构建）来使机器学习 (ML) 更易于访问。AutoML 的目标是减少构建和部署 ML 模型所需的手动工作量和专业知识，并使非专业人士更容易利用 ML 的强大功能。AutoML 系统通常使用搜索策略和优化算法来探索可能的函数选择和超参数调整空间。这些系统可以自动为给定数据集选择最合适的函数和设置，从而提高模型性能和泛化能力。AutoML 的基础工作之一是微软 Azure AutoML，它显著推动了 AutoML 工具的开发。AutoML 与 SNN 的交叉是一个不断发展的研究领域，尽管与传统 ML 模型相比，其探索程度仍然相对较低 [17], [18]。 SNN 带来的独特挑战，例如需要为特定任务找到最优的神经模型，目前尚未触及，因为为不同的神经模型设计相同的架构，使得 AutoML 技术在此方向上的应用尤其具有挑战性。
然而，许多研究人员正在探索 AutoML 在 SNN 领域的应用，他们使用超参数值更新、损失函数更新或硬件级参数选择方法。例如，Liu 等人 [19] 在 AutoML 框架内应用进化算法来优化 SNN 针对特定任务的超参数，证明了其性能优于手动调整的模型。同样，Roy 等人 [20] 的研究探索了强化学习在 SNN 参数调整中的应用，尤其是在神经形态硬件的背景下，其中功耗、处理速度和隐私的优化对于许多工业应用至关重要 [21]–[23]。

如前所述，与传统的ANN相比，SNN工具代表了一种更具生物学合理性的神经网络[24]，[25]。SNN使用离散脉冲或动作电位处理信息，类似于生物神经元的通信方式。前两代神经网络的设计主要受到人脑执行复杂任务（例如物体识别）的能力的启发，而SNN也受到了大脑卓越效率的启发。具体而言，SNN旨在复制人脑执行高度复杂任务的方式，同时仅消耗少量功率（约8-10瓦）[26]。
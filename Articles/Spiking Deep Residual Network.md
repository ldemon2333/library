We propose a shortcut conversion model to appropriately scale continuous-valued activations to match firing rates in SNN, and  a compensation mechanism to reduce the error caused by discretisation.

# 1. Intro
At current stage, how to train a SNN can be categorised into four classes. The first class seeks to make SNN differentiable with some approximations and apply gradient descent. The second class takes inspiration from biological neurons and utilises synaptic plasticity rules. The third class views SNN as a stochastic process and learns with probabilistic inference. However, the three kinds of approaches are not yet well-developed and cannot deal with deep architectures.

è¿™æ®µè¯æŒ‡å‡ºäº†ç›®å‰**è®­ç»ƒè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNN, Spiking Neural Networksï¼‰**çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºè„‰å†²æœºåˆ¶çš„**ä¸è¿ç»­æ€§**ï¼Œå¯¼è‡´æ— æ³•åƒä¼ ç»Ÿç¥ç»ç½‘ç»œé‚£æ ·ç›´æ¥åº”ç”¨åå‘ä¼ æ’­ç®—æ³•ï¼ˆbackpropagationï¼‰ã€‚ä¸‹é¢æˆ‘ä»¬æ¥é€ç±»è§£é‡Šè¿™å››ç§è®­ç»ƒæ–¹æ³•åŠå…¶å„è‡ªçš„ä¼˜ç¼ºç‚¹ï¼š

---

# Related Work
### ğŸ§  1. **å¯å¾®åˆ†è¿‘ä¼¼ + æ¢¯åº¦ä¸‹é™ï¼ˆApproximate gradientsï¼‰**

**æ€è·¯**ï¼š  
ç”¨æŸç§å¹³æ»‘å‡½æ•°æ¥è¿‘ä¼¼è„‰å†²ç¥ç»å…ƒçš„å‘æ”¾å‡½æ•°ï¼ˆå¦‚ spike å‡½æ•°çš„é˜¶è·ƒï¼‰ï¼Œä»è€Œä½¿å¾—ç½‘ç»œåœ¨æ•°å­¦ä¸Šâ€œå¯å¯¼â€ï¼Œä»¥ä¾¿ä½¿ç”¨å¸¸è§„çš„**æ¢¯åº¦ä¸‹é™æ³•ï¼ˆSGDï¼‰**è¿›è¡Œè®­ç»ƒã€‚

**æ–¹æ³•ç¤ºä¾‹**ï¼š

- surrogate gradientï¼ˆä»£ç†æ¢¯åº¦ï¼‰
    
- straight-through estimatorï¼ˆSTEï¼‰
    

**ä¼˜ç‚¹**ï¼š

- å¯ä»¥åœ¨ç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œä¸­åº”ç”¨è®­ç»ƒå·¥å…·ï¼ˆå¦‚ PyTorchã€TensorFlowï¼‰ã€‚
    
- æ¨¡æ‹Ÿç”Ÿç‰©ç‰¹æ€§ä½†ä¿ç•™è®­ç»ƒä¾¿åˆ©ã€‚
    

**ç¼ºç‚¹**ï¼š

- è¿‘ä¼¼å¹¶ä¸å‡†ç¡®ï¼Œè®­ç»ƒè¿‡ç¨‹å¯èƒ½ä¸ç¨³å®šã€‚
    
- éš¾ä»¥æ‰©å±•åˆ°æ·±å±‚ç»“æ„ã€‚
    

---

### ğŸŒ¿ 2. **çªè§¦å¯å¡‘æ€§è§„åˆ™ï¼ˆSynaptic Plasticityï¼‰**

**æ€è·¯**ï¼š  
æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒä¸­çš„å±€éƒ¨å­¦ä¹ æœºåˆ¶ï¼Œæ¯”å¦‚**STDPï¼ˆSpike-Timing Dependent Plasticityï¼‰**ï¼Œå³æƒé‡çš„æ›´æ–°ä¾èµ–äºçªè§¦å‰åçš„å‘æ”¾æ—¶é—´å·®ã€‚

**ä¼˜ç‚¹**ï¼š

- è®­ç»ƒè¿‡ç¨‹ç”Ÿç‰©å­¦åˆç†ï¼Œé€‚åˆæ— ç›‘ç£å­¦ä¹ ã€‚
    
- é€‚åˆåœ¨ç¥ç»å½¢æ€ç¡¬ä»¶ä¸Šå®ç°ï¼ˆèƒ½è€—ä½ï¼‰ã€‚
    

**ç¼ºç‚¹**ï¼š

- å¾ˆéš¾è·å¾—ç²¾ç¡®çš„å…¨å±€æœ€ä¼˜ç»“æœã€‚
    
- ä¸é€‚åˆå¤æ‚ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€‚
    

---

### ğŸ² 3. **æ¦‚ç‡æ¨ç†ï¼ˆProbabilistic Inferenceï¼‰**

**æ€è·¯**ï¼š  
æŠŠ SNN çœ‹ä½œä¸€ä¸ª**éšæœºè¿‡ç¨‹**ï¼ˆå¦‚æ³Šæ¾è¿‡ç¨‹ã€ç»å°”å…¹æ›¼åˆ†å¸ƒï¼‰ï¼Œé‡‡ç”¨**è´å¶æ–¯æ¨ç†**ç­‰æ–¹å¼è®­ç»ƒç½‘ç»œï¼Œæ¯”å¦‚ï¼š

- è´å¶æ–¯ç¥ç»ç½‘ç»œ
    
- é©¬å°”ç§‘å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMCMCï¼‰
    

**ä¼˜ç‚¹**ï¼š

- ç†è®ºåŸºç¡€æ‰å®ï¼Œå¯ä»¥è§£é‡Šç¥ç»å…ƒçš„éšæœºæ€§ã€‚
    
- æœ‰åŠ©äºæ„å»ºä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å‹ã€‚
    

**ç¼ºç‚¹**ï¼š

- æ¨ç†å¼€é”€å¤§ï¼Œæ”¶æ•›æ…¢ã€‚
    
- éš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡æ·±å±‚ç½‘ç»œã€‚
    

---

### ğŸ” 4. **ANN â†’ SNN è½¬æ¢ï¼ˆANN-to-SNN Conversionï¼‰**

**æ€è·¯**ï¼š  
å…ˆç”¨åå‘ä¼ æ’­è®­ç»ƒä¸€ä¸ªä¼ ç»Ÿçš„äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰ï¼Œç„¶åå°†å…¶å‚æ•°ï¼ˆå°¤å…¶æ˜¯æƒé‡ï¼‰**æ˜ å°„åˆ°ç­‰ä»·çš„ SNN**ã€‚å¸¸ç”¨æ–¹æ³•åŒ…æ‹¬ï¼š

- æ—¶é—´ç¼–ç è½¬æ¢
    
- ReLU ä¸å‘æ”¾ç‡ä¹‹é—´çš„è½¬æ¢
    

**ä¼˜ç‚¹**ï¼š

- åˆ©ç”¨æˆç†Ÿçš„ ANN å·¥å…·ï¼Œå¯è®­ç»ƒæ›´æ·±çš„ç½‘ç»œã€‚
    
- å®ç°ç²¾åº¦è¾ƒé«˜çš„åˆ†ç±»æ€§èƒ½ï¼ˆåœ¨æµ…å±‚ç»“æ„ä¸Šï¼‰ã€‚
    

**ç¼ºç‚¹**ï¼š

- æ·±å±‚ç½‘ç»œè½¬æ¢æ—¶ç²¾åº¦**è¿…é€Ÿä¸‹é™**ï¼ˆå¦‚è¶…è¿‡ 30 å±‚ï¼‰ã€‚
    
- ä¸€äº›æ“ä½œï¼ˆå¦‚æ± åŒ–ã€å½’ä¸€åŒ–ï¼‰éš¾ä»¥ç²¾ç¡®è¿˜åŸä¸ºè„‰å†²æœºåˆ¶ã€‚
    

---

### âœ… æ€»ç»“ï¼š

|æ–¹æ³•ç±»åˆ«|ä¼˜ç‚¹|ç¼ºç‚¹|
|---|---|---|
|è¿‘ä¼¼æ¢¯åº¦|å¯ç”¨åå‘ä¼ æ’­ã€æ˜“å®ç°|éš¾è®­ç»ƒæ·±å±‚ç½‘ç»œï¼Œè¿‘ä¼¼ç²¾åº¦å—é™|
|ç”Ÿç‰©å¯å‘å¯å¡‘æ€§è§„åˆ™|ç”Ÿç‰©çœŸå®ã€èƒ½è€—ä½|éš¾ç”¨äºç›‘ç£è®­ç»ƒã€æ€§èƒ½å¼±|
|æ¦‚ç‡æ¨ç†|ç†è®ºæ‰å®ã€æ”¯æŒä¸ç¡®å®šæ€§å»ºæ¨¡|æ¨ç†å¤æ‚ã€æ•ˆç‡ä½|
|ANN â†’ SNN è½¬æ¢|ç²¾åº¦è¾ƒé«˜ã€æ”¯æŒæ·±å±‚ç»“æ„ï¼ˆæœ‰é™ï¼‰|éš¾ç²¾ç¡®è½¬æ¢ï¼Œæ·±å±‚æ€§èƒ½ä¸‹é™ä¸¥é‡|

Previous conversion methods are not applicable to residual network for the structural difference between residual structure and conventional linear structure. 

We design a shortcut conversion model to jointly normalise synaptic weights in spiking residual network.


Conversion methods. 
Perez-Carrasco ç­‰äººæ˜¯æœ€æ—©å°è¯•å°† CNN è½¬ä¸º SNN çš„ç ”ç©¶è€…ä¹‹ä¸€ï¼Œä»–ä»¬çš„æ–¹æ³•å¤§è‡´å¦‚ä¸‹ï¼š

1. ä½¿ç”¨ **LIF æ¨¡å‹**ä½œä¸ºç›®æ ‡ SNN çš„ç¥ç»å…ƒã€‚
    
2. å°† CNN ä¸­çš„æƒé‡ **æ ¹æ® LIF ç¥ç»å…ƒçš„å‚æ•°è¿›è¡Œç¼©æ”¾**ã€‚
    
3. ä½¿å¾— SNN ä¸­ç¥ç»å…ƒçš„ **è„‰å†²å‘æ”¾ç‡ â‰ˆ CNN ä¸­çš„æ¿€æ´»å€¼**ï¼Œè¾¾åˆ°ç­‰ä»·æ˜ å°„ã€‚

One kind of conversion algorithms builds the mapping between ANN and SNN by fiddling the activation function/artificial neuron to approximate the average firing rate of spiking neurons. These methods require training with less common activation functions and are not well compatible with state-of-the-art ANN architectures.

The other kind of conversion algorithms takes the advantage of the non-negativity of ReLU activation to approximate average firing rate.
![[Pasted image 20250426183358.png]]


# Building Spiking ResNet
The conversion of networks comprised of two different types of neurons is based on the postulation that activation in ANN approximates firing rate of spiking neuron.
![[Pasted image 20250422152619.png]]
Sum layer is no longer needed since IF neuron implicitly implements addition during the integration of spikes. Convolution layer is replaced with a layer of synaptic connections that resembles convolutional operation. Weights of convolution layer are mapped to corresponding synapse layer and biases are converted to constant current injected to spiking neurons. 

å·ç§¯å±‚è¢«ä¸€å±‚ç±»ä¼¼å·ç§¯è¿ç®—çš„çªè§¦è¿æ¥å±‚æ‰€å–ä»£ã€‚å·ç§¯å±‚çš„æƒé‡è¢«æ˜ å°„åˆ°ç›¸åº”çš„çªè§¦å±‚ï¼Œåç½®è¢«è½¬æ¢ä¸ºæ³¨å…¥è„‰å†²ç¥ç»å…ƒçš„æ’å®šç”µæµã€‚

For average pooling, synaptic weights are fixed to 1/poolsize^2. For max pooling, we use logical comparision to select spikes only from neuron with highest firing rate and inhibit other incoming spikes by setting synaptic weights to zero. An extra layer of IF neuron is added after pooling layer or fully-connected layer to integrate spikes from these types of synaptic connections.
![[Pasted image 20250422155153.png]]

è¿™æ®µè®²çš„æ˜¯ **å¦‚ä½•å°† ResNet çš„æ¿€æ´»å€¼â€œå‹ç¼©â€åˆ° SNN ä¸­å¯æ¥å—çš„è„‰å†²å‘æ”¾ç‡èŒƒå›´å†…**ï¼Œå¹¶**è§£é‡Šäº†ä¸ºä»€ä¹ˆè¦åšå½’ä¸€åŒ–ã€å½’ä¸€åŒ–æ€ä¹ˆå½±å“æ•´æ¡è·¯å¾„ä¸Šçš„æƒé‡ä¸è¾“å…¥**ã€‚

æˆ‘ä»¬é€å¥åˆ†æï¼š

---

### ğŸ” ä¸ºä»€ä¹ˆè¦åŒ¹é…æ¿€æ´»å€¼å’Œå‘æ”¾ç‡èŒƒå›´ï¼Ÿ

> â€œThen we let activations in ResNet match the range of firing rates in S-ResNet...â€

- åœ¨ SNN ä¸­ï¼Œä¸€ä¸ªç¥ç»å…ƒæ¯ä¸ªæ—¶é—´æ­¥**åªèƒ½å‘ä¸€ä¸ª spike**ï¼ˆä¹Ÿå°±æ˜¯æœ€å¤§å‘æ”¾ç‡ä¸º 1ï¼‰ï¼›
    
- è€Œåœ¨ ANN ä¸­ï¼ŒReLU æ¿€æ´»å€¼å¯èƒ½å¾ˆå¤§ï¼ˆæ¯”å¦‚ 10ï¼Œ100ï¼Œç”šè‡³æ›´å¤šï¼‰ï¼›
    
- å¦‚æœä½ ä¸åšå¤„ç†ï¼Œè¿™äº›é«˜å€¼åœ¨ SNN ä¸­å°±ä¼š**ä¸¢å¤±ä¿¡æ¯**ï¼Œå› ä¸º spike ä¸Šé™ä¸º 1ã€‚
    

âœ… **è§£å†³æ–¹æ³•**ï¼šå°† ANN ä¸­çš„æ¿€æ´»å€¼**æ•´ä½“å½’ä¸€åŒ–**ï¼Œä½¿å…¶èŒƒå›´å¯¹é½ä¸º [0, 1]ã€‚

---

### â— å¦‚æœä¸åšå½’ä¸€åŒ–ä¼šæ€ä¹ˆæ ·ï¼Ÿ

> â€œOtherwise, information carried by high activation values will be lost during the conversion.â€

æ¯”å¦‚ï¼š

- ä¸€ä¸ª ANN ç¥ç»å…ƒçš„æ¿€æ´»å€¼æ˜¯ 7ï¼›
    
- è½¬æ¢æˆ SNN æ—¶ï¼Œå®ƒæœ€å¤šåªèƒ½å‘ 1 ä¸ª spikeï¼›
    
- é‚£ç›¸å½“äºæŠŠ **7 æ˜ å°„ä¸º 1**ï¼Œä½†å¦ä¸€ä¸ªå€¼ä¸º 1 çš„ç¥ç»å…ƒä¹Ÿå‘ä¸€ä¸ª spikeï¼›
    
- é‚£ä¹ˆè¿™ä¸¤è€…çš„â€œä¿¡æ¯å¼ºåº¦â€åœ¨ SNN é‡Œå°±å®Œå…¨ä¸€æ ·äº†ï¼Œ**åŒºåˆ†åº¦ä¸§å¤±**ï¼Œä¿¡æ¯ä¸¢å¤±ã€‚
    

---

### ğŸ§® å¦‚ä½•è¿›è¡Œå½’ä¸€åŒ–ï¼Ÿ

> â€œweights and biases at each layer are normalised by the maximum possible activation that is calculated from the training set.â€

- ä½¿ç”¨è®­ç»ƒé›†ä¸­æ¯ä¸€å±‚æ¿€æ´»å€¼çš„ **æœ€å¤§å€¼ï¼ˆæˆ– 99.9% åˆ†ä½æ•°ï¼‰** æ¥å½’ä¸€åŒ–ï¼š

âœ… è¿™æ ·åšçš„ç›®çš„æ˜¯è®©è¯¥å±‚çš„æœ€å¤§è¾“å‡ºå€¼ â‰ˆ 1ï¼Œå¯¹åº”åˆ° SNN ä¸­å°±æ˜¯â€œæœ€å¤§å‘æ”¾ç‡â€ã€‚

---

### ğŸ” æ³¨æ„ï¼šå½’ä¸€åŒ–å½±å“æ˜¯ä¼ é€’çš„ï¼

> â€œThis leads to new weights, biases in the residual block and new input as well due to the change of weight in previous layersâ€

- å¦‚æœç¬¬ lâˆ’1l-1 å±‚åšäº†å½’ä¸€åŒ–ï¼Œé‚£å®ƒçš„è¾“å‡ºï¼ˆå³ç¬¬ ll å±‚çš„è¾“å…¥ï¼‰ä¹Ÿè¢«ç¼©å°äº†ï¼›
    
- é‚£ä¹ˆç¬¬ ll å±‚çš„æ¿€æ´»å€¼ä¹Ÿä¼šéšä¹‹å˜å°ï¼›
    
- æ‰€ä»¥ä½ **ä¸èƒ½åªå•ç‹¬å½’ä¸€åŒ–æŸä¸€å±‚**ï¼Œéœ€è¦**ä»è¾“å…¥å¼€å§‹å±‚å±‚è€ƒè™‘æ•´ä¸ªé“¾æ¡**ï¼ŒåŒ…æ‹¬ shortcut åˆ†æ”¯ï¼›
    
- å°¤å…¶åœ¨ ResNet ä¸­ï¼Œæ®‹å·®å—çš„ä¸¤ä¸ªè·¯å¾„ï¼ˆä¸»è·¯å¾„ä¸æ·å¾„ï¼‰è¦ **ä¸€èµ·å½’ä¸€åŒ–**ï¼Œç¡®ä¿æ¿€æ´»çš„åŠ æ³•ç»“æœä¸€è‡´ã€‚

![[Pasted image 20250422162024.png]]
![[Pasted image 20250422162436.png]]

## 3.3 Compensation of Propagation Error
The normalised activation of ReLU is approximated by the firing rate of spiking neuron. 
![[Pasted image 20250422212829.png]]![[Pasted image 20250422212835.png]]
As is seen from the difference between 7 and 9, the approximation of ReLU activation incurs an amount of deviation at each neuron by the last term in 9.
![[Pasted image 20250422213332.png]]

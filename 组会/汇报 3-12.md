本周工作：
- 在看 cluster 的代码实现
	- cluster 的整体架构，几个功能模块
	- 调度状态机的具体实现
	- 服务端接口的实现
	- 容器化思路的实现

目的，两个问题：
- 首先看资源是如何定义的，有哪些定义
- 调度机调度的过程中是否根据资源的限制进行调度

## cluster 接口
提供给 Master 的所有接口
![[Pasted image 20250310101831.png]]
![[Pasted image 20250306113835.png]]


这里探讨与资源分配最为密切的 TaskCreate() 实现，对应就是如何将状态初始化到 kCreate 
# TaskCreate() 具体实现
创建接口提供给应用使用，用于创建一个类脑任务，类脑任务是神经元计算机操作系统分配资源及运行的基本单元。这里更多是资源的预分配，即定义逻辑核心到物理核心上映射的分配方法（通过最大空矩形算法实现）。

拟解决以下问题：
- [ ] 为用户 InstallApp() 中创建的应用创建类脑任务运行实例，任务基本分为预处理、编解码、神经元计算子任务链。
- [ ] 为类脑应用创建程序载体，用以加载神经网络信息、配置和运行信息，并分配全局唯一任务 ID。
- [ ] 指定部署 DPK 应用内的某个模型，创建的神经任务与该模型绑定
- [ ] 根据 SNN 进行资源配置，给网络分配真实的类脑芯片资源（NPU）
- [ ] 将 DPK 占用核心的资源格式映射至实际分配的硬件资源上，并能实际运行和计算脉冲 I/O

![[task_create.drawio.png]]

## 资源分配最终落到 ResourceAllocation 类的 AllocateResource() 上
查这个函数具体的文档
![[Pasted image 20250307204126.png]]
![[Pasted image 20250307204220.png]]
## namespace 实现方法
namespace 好实现，因为只要隔离出任务唯一标识 ID 空间就好了，或者简单实现设置一个映射表，全局任务 ID 映射到他用户空间任务 ID ，这样每个用户的任务 ID 都能从 1 开始。

## 资源虚拟化
资源分配的结果有三种：
- 逻辑核心到物理核心的映射（映射主要采用最大空矩形算法）
- 对物理核心的读写访问中继路径信息
- 对物理核心的脉冲访问中继路径信息

首先逻辑核心到物理核心的映射是 1：1映射，如果物理核心位置定了，后面对物理核心的中继路径也会确定。那么这里资源只有一种，就是逻辑核心。

这里怎么对资源做限制？虚拟化，容器化的意义在于提供服务，如果限制不了每个容器的资源，那么只是单纯使用 namespace 机制进行任务 ID 隔离，毫无意义。这里对资源限制的最简单的限制就是逻辑核心的个数限制，那么问题就变为逻辑核心的个数是怎么得出的？

也就是这个数据结构是如何得出的
![[Pasted image 20250310112311.png]]
然后向上查，这个数据是读 dpk 文件结果，这个数据是在 dpk 文件中 node-graph.json 中读取得到的。

综上看，整个资源分配完全是根据 SNN 编译结果产生的，那么能否在编译时候，就去限制它逻辑核心的数量呢？

然后我就要去看 SNN 是如何编译成 DPK 文件的。

编译实现的方法是根据 SNN 网络拓扑结构，把神经元放到逻辑核中，逻辑核中存储轴突，树突映射。目前假设是整个计算机面向单用户的，把单用户的整个模型编译出来，资源就是整台计算机，不考虑资源限制。

###  需求分析：
用户 A 申请一个神经拟态计算机，该计算机配置是 100 个神经拟态核，那么用户 A 所能使用的所有资源就是这 100 个神经拟态核。用户 A 提交一个应用，经过编译成 DPK 文件，虚拟核心个数就是 100。

# 针对第二问题，去看调度机运行时候做了什么
NeuralTask::Run() 这个函数会根据不同 type 选择不同的处理

这里查看 TaskRun 是如何具体实现的
![[Pasted image 20250310130224.png]]
这里具体实现是构建 inSpks 发送包到输入神经元中，之间资源分配时候所有消息传递路径都已经定死，这里就是填充包，并没有根据资源的限制去限制 Task，也没有存在资源的限制。

# 和魏老师讨论
后续整个集群可以用 k8s 那一套来管理

- 面向资源配额的容器化思路，这方面可以借鉴 k8s 管理集群的思路，或者部署在 docker 中，管理宿主机上 linux 资源。

- 面向安全的容器化思路，更像是一个沙盒，业务和他的 SNN 作为一个沙盒通信，这样业务的任务如果是恶意任务，就不会使整个集群崩溃，

- 还有一个点就是如果用户沙盒所在真实硬件出现故障，正常解决方案，要改硬件，改用户业务部分的接口，那么我们就可以中间抽象出一个虚拟层，业务到虚拟层的映射不变，改的只是虚拟层到新的 SNN 沙盒映射，这部分主要就是实现一个转发机制，有 OS 自动完成。

- 还有一个就是容器化思路就是网络这一块，这个集群上本质都是在同一个网络中，是否可以用类似 k8s 中的 kube proxy 抽象出网络虚拟层，用户有一个自己的虚拟网络，不会进入到其他用户网络中。


![[容器虚拟化安全.drawio (1).png]]

# 后续
能把整个项目运行起来，跟踪整个数据流的变化，观察业务逻辑，关注 node 的实现，cluster 的具体实现，dpk 文件如何生成的，

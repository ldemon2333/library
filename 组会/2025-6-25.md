看了几篇 spliting-learning 的文章，确实和之前讨论的微服务架构很像，其实微服务架构就是子模型拆分后以容器镜像的形式进行部署，不管微服务的粒度是多少，本质上研究的就是如何拆和组合。

大部分文章其实并没有解释我们为什么要将一个大模型进行拆分，这类文章通常与边缘设备推理背景结合，这里拆分的原因往往是因为边缘设备的内存，计算能力受限，所以引入拆分学习，这类文章通常关注的就是网络利用率，模型剪枝效果，推理延迟这些指标。

这里介绍具有代表的一篇文章 《Efficient Partitioning Vision Transformer on Edge Devices for Distributed Inference》，实验部分有拿 EC-SNN 进行对比

这里为什么又要引入容器，是为了引入子模型微服务概念，可以利用 k8s 作可靠，可扩缩容的服务，主要是为了服务计算，更好的服务角度出发。

# Adaptive Compression-Aware Split Learning and Inference for Enhanced Network Efficiency
@article{mudvari2024adaptive,
  title={Adaptive compression-aware split learning and inference for enhanced network efficiency},
  author={Mudvari, Akrit and Vainio, Antero and Ofeidis, Iason and Tarkoma, Sasu and Tassiulas, Leandros},
  journal={ACM Transactions on Internet Technology},
  volume={24},
  number={4},
  pages={1--26},
  year={2024},
  publisher={ACM New York, NY}
}

## 摘要
移动设备中 AI 驱动的应用日益增多，催生了将深度学习模型与可用的边缘云资源相结合的解决方案。由于降低设备能耗、改善延迟、提升网络利用率以及提升隐私保护等诸多优势，将深度学习模型从移动设备中分离出来并以分布式方式进行计算的拆分学习已成为一个备受关注的课题。结合压缩感知方法（学习过程会根据传输数据的压缩级别进行调整）使拆分学习更具优势。
这种方法甚至可以为联邦学习等传统方法提供可行的替代方案。
在本研究中，我们开发了一种自适应压缩感知拆分学习方法（“deprune”），用于改进和训练深度学习模型，使其网络效率更高，从而使其成为借助边缘云资源在性能较弱的设备中部署的理想选择。该方法也得到了扩展（“剪枝”），可以通过迁移学习方法快速训练深度学习模型，这种方法牺牲了较低的准确率，换取了更高效的网络推理能力。我们证明，“去剪枝”方法与未使用我们方法的分割学习方法相比，可以在不损失准确率的情况下将网络占用率降低 4 倍，同时与压缩感知分割学习方法相比，准确率最高可提高 4%。最后，我们证明，“剪枝”方法与压缩感知分割学习方法相比，可以在不影响准确率的情况下，将某些模型的训练时间缩短 6 倍。


好的，这是一份关于所提供学术论文《Adaptive Compression-Aware Split Learning and Inference for Enhanced Network Efficiency》的摘要分析。

### **论文摘要**

[cite_start]这篇论文提出了一种创新的自适应压缩感知分离学习 (Adaptive Compression-Aware Split Learning) 方法，旨在解决在行动装置和边缘云端资源上部署深度学习模型时的网路效率问题 [cite: 1, 5][cite_start]。随着人工智慧驱动的应用程式日益增多，如何在网路资源有限的装置上高效运行深度学习模型成为一项挑战 [cite: 1, 26]。

[cite_start]该研究开发了名为「deprune」和「prune」的两种演算法，核心在于一个可自适应调整压缩等级的「压缩-解压缩模组」[cite: 5, 59, 106, 120]。

**核心贡献与方法：**

1.  **自适应压缩-解压缩模组 (Adaptive Compression-Decompression Module):**
* [cite_start]论文设计了一个可以在神经网路分离点动态调整资料压缩等级的模组 [cite: 59, 107][cite_start]。此模组利用一个可训练的滤波器 (filter) `f` 来控制在分离点需要传输的特征图数量，从而适应不同的网路条件和效能需求，避免了每次变更压缩设定就需要从头重新训练模型的低效率问题 [cite: 105, 164, 177]。

2.  **Deprune 演算法 - 提升训练效率:**
* [cite_start]此方法旨在降低分离学习**训练**过程中的网路消耗，同时不牺牲最终的模型准确度 [cite: 5, 187]。 
* [cite_start]其策略是，在训练初期采用高压缩率（传输较少数据）进行多轮训练，然后在训练后期切换到低压缩率或无压缩状态，完成最终的微调 [cite: 66, 188, 312]。 
* [cite_start]**效能结果：** 实验证明，「deprune」方法与不使用压缩的分离学习方法相比，可将**网路使用量减少4倍**，且最终准确度不受影响；而与传统的压缩感知分离学习相比，**准确度最多可提升4%** [cite: 7, 375]。

3.  **Prune 演算法 - 加速模型部署:**
* [cite_start]此方法旨在快速生成一系列具有不同「准确度-网路成本」权衡的模型，以适应不同的**推论 (inference)** 需求 [cite: 6, 113, 208]。 
* [cite_start]它首先训练一个无压缩的「基础模型」，然后利用迁移学习 (transfer learning) 的方式，将已学到的参数转移到不同压缩等级的模型上进行快速再训练，从而大幅缩短生成这些近似模型所需的时间 [cite: 6, 216, 218]。 
* [cite_start]**效能结果：** 实验显示，「prune」方法与从头开始训练每个压缩模型相比，可将特定模型的**训练时间缩短最多达6倍** [cite: 8, 378]。

**实验与验证：**

* [cite_start]研究人员在真实的测试平台（包括Wi-Fi和乙太网路环境）以及模拟环境中对所提出的方法进行了评估 [cite: 71, 263, 289]。
* [cite_start]实验涵盖了多种设定，包括不同的深度学习模型（如 VGG11）、资料集（如 CIFAR10, Imagenet100, STL10）以及不同的网路分离点和压缩等级，验证了该方法的通用性和稳健性 [cite: 259, 308, 337, 360]。

**结论：**

[cite_start]该研究提出的自适应压缩感知分离学习框架，为在物联网 (IoT) 装置、扩增实境 (AR) 等资源受限的环境中部署和训练人工智慧模型提供了一个高效且灵活的解决方案 [cite: 110, 376, 379][cite_start]。它不仅显著降低了网路负担和训练延迟，还为网路管理者提供了根据即时需求动态选择最佳效能权衡的能力 [cite: 369]。





# Efficient Partitioning Vision Transformer on Edge Devices for Distributed Inference
@article{liu2024ed,
  title={ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge Devices},
  author={Liu, Xiang and Song, Yijun and Li, Xia and Sun, Yifei and Lan, Huiying and Liu, Zemin and Jiang, Linshan and Li, Jialin},
  journal={arXiv preprint arXiv:2410.11650},
  year={2024}
}

## 摘要
深度学习模型越来越多地应用于资源受限的边缘设备进行实时数据分析。近年来，Vision Transformer 及其变体在各种计算机视觉任务中展现出卓越的性能。然而，其高计算需求和低推理延迟为在资源受限的边缘设备上部署此类模型带来了巨大挑战。为了解决这个问题，我们提出了一个新颖的框架 ED-ViT，旨在高效地拆分和跨多个边缘设备执行复杂的 Vision Transformer。我们的方法包括将 Vision Transformer 模型划分为多个子模型，每个子模型专用于处理特定的数据类子集。为了进一步降低计算开销和推理延迟，我们引入了一种逐类剪枝技术来减小每个子模型的大小。通过在五个数据集上使用三种模型架构进行大量实验，并在边缘设备上进行实际实现，我们证明了我们的方法显著降低了边缘设备上的推理延迟，并将模型大小分别缩减了高达 28.9 倍和 34.1 倍，同时保持了与原始 Vision Transformer 相当的测试精度。此外，我们将 ED-ViT 与两种在边缘设备上部署 CNN 和 SNN 模型的先进方法进行了比较，评估了精度、推理时间和模型整体大小等指标。我们的全面评估强调了所提出的 ED-ViT 框架的有效性。


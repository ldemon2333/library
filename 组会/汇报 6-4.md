# k8s 中 device plugin 如何管理 GPU 资源
![[Pasted image 20250527215754.png]]

![[Pasted image 20250528115113.png]]
要进行设备管理，device plugin 插件需要实现以下接口：

- `GetDevicePluginOptions`：这个接口用于获取设备插件的信息，可以在其返回的响应中指定一些设备插件的配置选项，可以看做是插件的元数据
- **`ListAndWatch`**：该接口用于列出可用的设备并持续监视这些设备的状态变化。
- `GetPreferredAllocation`：将分配偏好信息提供给 device plugin,以便 device plugin 在分配时可以做出更好的选择
- **`Allocate`**：该接口用于向设备插件请求分配指定数量的设备资源。
- `PreStartContainer`： 该接口在容器启动之前调用，用于配置容器使用的设备资源。

## 整体流程
device plugin 与 kubelet 打交道，最终把当前节点上的 GPU 信息保存到 APIServer 中记录。

首先，对于每一种硬件设备，都需要有它所对应的 Device Plugin 进行管理，这些 Device Plugin，都通过 gRPC 的方式，同 kubelet 连接起来。以 NVIDIA GPU 为例，它对应的插件叫作[`NVIDIA GPU device plugin`](https://github.com/NVIDIA/k8s-device-plugin)。

这个 Device Plugin 会通过一个叫作 ListAndWatch 的 API，定期向 kubelet 汇报该 Node 上 GPU 的列表。比如，在我们的例子里，一共有三个 GPU（GPU0、GPU1 和 GPU2）。这样，kubelet 在拿到这个列表之后，就可以直接在它向 APIServer 发送的心跳里，以 Extended Resource 的方式，加上这些 GPU 的数量，比如`nvidia.com/gpu=3`。所以说，用户在这里是不需要关心 GPU 信息向上的汇报流程的。

需要注意的是，ListAndWatch 向上汇报的信息，只有本机上 GPU 的 ID 列表，而不会有任何关于 GPU 设备本身的信息。而且 kubelet 在向 API Server 汇报的时候，只会汇报该 GPU 对应的 Extended Resource 的数量。当然，kubelet 本身，会将这个 GPU 的 ID 列表保存在自己的内存里，并通过 ListAndWatch API 定时更新。

而当一个 Pod 想要使用一个 GPU 的时候，它只需要像我在本文一开始给出的例子一样，在 Pod 的 limits 字段声明`nvidia.com/gpu: 1`。那么接下来，Kubernetes 的调度器就会从它的缓存里，寻找 GPU 数量满足条件的 Node，然后将缓存里的 GPU 数量减 1，完成 Pod 与 Node 的绑定。

这个调度成功后的 Pod 信息，自然就会被对应的 kubelet 拿来进行容器操作。而当 kubelet 发现这个 Pod 的容器请求一个 GPU 的时候，kubelet 就会从自己持有的 GPU 列表里，为这个容器分配一个 GPU。此时，kubelet 就会向本机的 Device Plugin 发起一个 Allocate() 请求。这个请求携带的参数，正是即将分配给该容器的设备 ID 列表。

对下：
当 Device Plugin 收到 Allocate 请求之后，它就会根据 kubelet 传递过来的设备 ID，从 Device Plugin 里找到这些设备对应的设备路径和驱动目录。当然，这些信息，正是 Device Plugin 周期性的从本机查询到的。比如，在 NVIDIA Device Plugin 的实现里，它会定期访问 nvidia-docker 插件，从而获取到本机的 GPU 信息。

而被分配 GPU 对应的设备路径和驱动目录信息被返回给 kubelet 之后，kubelet 就完成了为一个容器分配 GPU 的操作。接下来，kubelet 会把这些信息追加在创建该容器所对应的 CRI 请求当中。这样，当这个 CRI 请求发给 Docker 之后，Docker 为你创建出来的容器里，就会出现这个 GPU 设备，并把它所需要的驱动目录挂载进去。

Darwin device plguin 对上部分目前在写。
![[Pasted image 20250527221305.png]]


# k8s 管理硬件资源
很粗粒度的，只能针对硬件个数进行管理，更加细粒度的管理比如“我的 Pod 想要运行在计算能力最强的那个 GPU 上”，Device Plugin 就完全不能处理了。

# GPU 是如何分配给 Pod
![[Pasted image 20250528115902.png]]

# ENABLING KUBERNETES FOR DISTRIBUTED AI PROCESSING ON EDGE DEVICES
![[Pasted image 20250528134632.png]]
微服务方法与 gRPC 通信相结合，实现了流水线架构，其中，在处理流水线应用程序时，流水线中的阶段数与集群中微服务的数量相同，而本文旨在帮助人工智能中的流水线应用程序。图 3.2 展示了如何在 Kubernetes 集群中使用容器化微服务实现流水线，该集群与运行容器的节点无关。

![[Pasted image 20250528135528.png]]

这里将业务拆分成多个微服务，将每个微服务部署在 Pod，每个微服务以 Service 方式暴露出 gRpc 接口，Service 之间的 Pod 通信，采用 gRpc，这种方式的好处是每个微服务都可以以 Service 方式进行编排。坏处是微服务之间加入了网络通信的开销。

之前的想法设计中，是将业务放在一个 Pod，整个业务的 Pod 里有多个容器，部署微服务，容器之间通信相当于在本地 Pod 的localhost 上进行网络通信，但每个微服务的容器是无法用 k8s 来进行编排的。好处是没有网络通信的开销，开销相对来说比上种小。

## 论文里的实验设置
![[Pasted image 20250525142302.png]]![[Pasted image 20250525142502.png]]

我们提出了一种新的基于反向传播的离散时间脉冲神经网络 (SNN) 训练算法。受二值化神经网络的最新深度学习算法的启发，使用直通梯度估计器的二进制激活来建模泄漏积分激发脉冲神经元，克服了使用反向传播训练 SNN 的困难。提出了两种 SNN 训练算法：(1) 具有不连续积分的 SNN，适用于速率编码的输入脉冲；(2) 具有连续积分的 SNN，它更通用，可以处理具有时间信息的输入脉冲。采用 28nm CMOS 设计的神经形态硬件利用了脉冲稀疏性，并展示了高分类准确率（MNIST 上 >98%）和低能耗（51.4–773 nJ/图像）。

# 1. 介绍
最近，许多深度学习算法，如多层感知器 (MLP) 和卷积神经网络 (CNN)，在图像和语音分类任务中表现出了人类级别的识别准确率 [1-2]。这些人工神经网络 (ANN) 中的操作数量通常不依赖于输入数据，而深度网络中大量与输入无关的计算可能导致高能耗。另一方面，脉冲神经网络 (SNN) [3] 更接近地模仿生物神经系统中的操作，并探索类似大脑的认知计算的新途径。SNN 可以利用输入相关的稀疏性或冗余来动态扩展计算量，从而实现节能的硬件实现 [4]。

基于反向传播 (BP) 的随机梯度下降被广泛用于训练 ANN，并且在许多基准测试中都表现出很高的准确性。然而，关于 SNN 训练算法的文献仍然没有证明足够高的准确性，特别是对于具有多层的深度网络。现有的 SNN 训练算法分为无监督学习（无标记数据）和监督学习（有标记训练数据）。生物可信的学习规则，如脉冲时间依赖可塑性 (STDP) [5-6]，已被用于无监督学习，但对于深度网络没有表现出有竞争力的准确性。最近，提出了几种用于 SNN 的监督学习算法，其中首先使用 BP 训练 ANN，然后通过将实值输入/激活映射到泊松脉冲的平均发放率转换为 SNN [7-9]。这种方法有两个缺点：（1）它需要很多时间步骤才能达到高精度；（2）它仅适用于速率编码输入脉冲，而不适用于其他脉冲编码格式，例如时间编码。其他监督学习算法已被提出直接用 BP 训练 SNN [10-11]。然而，这些算法表现出具有指数突触后电位或膜衰减的复杂神经元模型，这在计算上是昂贵的。

在本文中，我们提出了一种基于 BP 的训练算法，该算法可以使用简单的神经元模型训练硬件友好的 SNN，用于速率编码和时间编码输入。我们还展示了 28nm CMOS 中节能神经形态硬件设计的实验结果，该设计实现了我们为 MNIST [12] 和 N-MNIST [13] 数据集训练的 SNN 模型。

# 3. Software results on benchmarks
我们使用 Theano 框架训练了具有 SNN-DC 和 SNN-CT 神经元模型的离散时间 SNN。在以下实验中，使用 Adam 优化器 [15] 训练 SNN，批处理大小为 100，神经元阈值 $\theta$= 1。

# 4. Hardware Implementation results
我们在 TSMC 28nm HPC CMOS 中为 MNIST 和 N-MNIST 分类任务设计了离散时间 MLP SNN 的神经形态硬件。两个 SNN 都有两个 256 个神经元的隐藏层。MNIST（N-MNIST）的 SNN-DC（SNN-CT）有 784（1,156）个输入神经元和 10（12）个输出神经元。突触权重存储在片上 SRAM 中，神经元膜电位值存储在寄存器中。图 5 显示了整体硬件架构，其中使用同步时钟和广泛的时钟门控。我们对隐藏/输出层采用并行输出神经元，而神经元的输入尖峰在每个时钟周期内串行处理。

## Spike scheduler for event-driven operation

## Pipeline architecture and handshaking


# 5. 结论
在本文中，我们介绍了离散时间 SNN 的新训练算法和分类硬件设计。通过使用直通估计器的二进制激活，我们可以使用误差反向传播有效地训练 SNN。具有不连续神经元积分的 SNN-DC 适用于速率编码的输入脉冲，而具有连续神经元积分的 SNN-CT 更为通用，适用于具有时间信息的输入脉冲。相应的 MLP SNN 在 28nm CMOS 中实现，表现出高精度和低能耗。SNN-DC 对 MNIST 的准确率为 98.0-98.70%，每次分类 51.4-773 nJ，SNN-CT 对 N-MNIST 的准确率为 96.33%，每次分类 294 nJ。
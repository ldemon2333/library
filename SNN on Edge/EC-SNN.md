# Abstract
深度脉冲神经网络 (SNN) 是一种以多层结构为特征的 SNN 高级形式，最近在各个领域的性能方面取得了重大突破。SNN 的生物学合理性和能量效率自然与边缘计算 (EC) 场景的要求相一致，从而引起了研究人员对将这些深度 SNN 模型迁移到传感器和智能手机等边缘设备的兴趣。然而，由于模型参数大幅增加的影响和实际应用中苛刻的计算要求，迁移工作的进展尤其具有挑战性。在这项工作中，我们提出了一个名为 EC-SNN 的深度 SNN 拆分框架，用于在边缘设备上运行复杂的 SNN 模型。我们首先将完整的 SNN 模型划分为较小的子模型，以将其模型参数分配到多个边缘设备上。然后，我们提供了一种通道修剪方法来减小每个子模型的大小，从而进一步减少计算负荷。我们对六个数据集（即四个非神经形态数据集和两个神经形态数据集）设计了大量实验，以证明我们的方法可以显著减少边缘设备上的推理执行延迟，并降低每个部署设备的整体能耗，平均分别减少 60.7% 和 27.7%，同时保持准确性的有效性。

# 1. 介绍
机器学习技术的快速发展导致对计算资源的需求不断增加。传统的计算架构以处理和存储功能之间的二分法为特征，通常使得许多组织无法在财务上实施大规模人工智能系统。在这种环境下，Loihi 和 TrueNorth 等神经形态芯片的发展预示着第三代神经网络（即脉冲神经网络）有可能逐渐取代传统的人工神经网络 (ANN)。这些 SNN 模型以其能源效率和生物保真度而著称，使其成为解决计算和能源限制的有希望的途径。

最近，更大、更深的 SNN 在不同领域的性能改进方面取得了突破 [Li et al., 2023; Yao et al., 2024; Su et al., 2023]，从而有可能实现更广泛的功能。 然而，这些更复杂的 SNN 相关模型导致其在资源受限的边缘设备（如智能手机和传感器）中广泛应用的优势丧失。这些模型的部署仍然受到边缘设备有限的计算能力和存储能力的限制。为了应对这一挑战，我们提出了一个名为 EC-SNN 的新框架，它可以利用多个边缘设备的协作，为在边缘设备上部署复杂的 SNN 模型提供一种节能策略。

在 EC-SNN 中，我们主要采用并整合了两种方法来解决这个问题。第一种方法是将原始的大型深度 SNN 模型划分为几个较小的子模型，也称为拆分学习 (SL) [Thapa 等，2022]，将它们部署在多个边缘设备上，并协调它们的合作以集体执行相同的任务。此方法涉及将完整的机器学习 (ML) 模型划分为几个较小的网络段，这些网络段进一步在云服务器或分布式客户端上进行单独训练并利用本地化数据集。SL 的主要重点是减轻计算负担和处理要求。然而，他们都没有考虑过在边缘设备上拆分 SNN。在 EC-SNN 中，我们建议在边缘设备上分配拆分的 SNN 子模型以降低能耗。

另一种方法涉及简化初始 SNN 模型中的计算机制。先前的 SNN 修剪方法 [Kim et al., 2022; Chen et al.,2022a] 主要涉及直接应用 ANN 中一些成熟的修剪技术 [Frankle and Carbin, 2018; Hoefer et al., 2021]。这些转移方法已通过 SNN 修剪领域的大量实验证明了其有效性。在本研究中，我们采用通道修剪方法 [Chen et al., 2023] 进行初始修剪阶段，大大减少了子模型的执行时间，而不会对性能产生重大影响。通过整合这两种方法 ECSNN 是一种新颖的拆分框架，可最大限度地减少执行延迟并最大限度地提高所有可用边缘设备的利用率，同时遵守每个设备的单独能量约束。

我们的主要贡献总结如下。
• 这是第一项针对深度 SNN 分裂问题的研究。该问题的解决为通过并行操作在多个边缘设备上部署深度 SNN 提供了可行的解决方案。它不仅有效地利用了深度 SNN 的功能，而且保持了模型能耗低的优势。此外，解决这个问题可以在实际实施中充分利用多个边缘设备。
• 为了解决深度 SNN 的分裂问题，我们提出了一个名为 EC-SNN 的新框架，用于在边缘设备上运行复杂的 SNN 模型。EC-SNN 首先根据其特点将深度 SNN 模型分解为较小的子模型。然后，我们利用通道修剪方法来减小每个子模型的大小。通过在每个边缘设备上权衡模型大小和准确性，我们有效地降低了深度 SNN 模型的能耗、计算负载和推理延迟。
• 我们对两种传感模式中的四个非神经形态数据集和两个神经形态数据集进行了广泛的实验，以证明我们的框架显着降低了边缘设备上的推理执行延迟，并降低了总体能耗，而各种 SNN 应用中模型精度的损失可以忽略不计。

本文的其余部分安排如下。第 2 节回顾了相关工作。第 3 节描述了 EC-SNN 的设计。第 4 节介绍了评估结果。第 5 节总结了本文。

# 2. Related work
边缘计算中的脉冲神经网络。大多数现有的关于 SNN 的研究主要集中在其显著的能源效率上，这一特性使它们特别适合边缘计算 (EC) 上的应用。EC 是一种通过靠近数据源的节点促进快速数据流传输的范式，可实现实时、低延迟的数据处理。与此同时，越来越多的 SNN 在广泛的边缘计算应用中表现出卓越的计算性能，包括面部识别 [Barchid et al., 2023]、机器人技术 [Jiang et al., 2023] 和物体检测 [Su et al., 2023]。然而，资源受限的边缘设备的限制对利用 SNN 的功能提出了重大挑战，尤其是在 SNN 的训练和推理阶段。

有几种方法可以解决 SNN 资源限制的问题。解决这一问题的关键部分是设计一个计算效率高的框架，旨在减少不必要的计算并优化 SNN 中的权重参数。[Liu 等，2024] 利用 SNN 的稀疏性，通过彩票假设 (LTH) 执行权重参数修剪，以发现保持相似性能的非常稀疏的子网。[Nguyen 等，2021] 引入了一种创新的连接减少方法，在基于片上 STDP 的学习过程中使用 TTFS 编码修剪 SNN。[Chowdhury 等，2021] 将基于 PCA 的空间修剪方法与时间修剪结合起来，以减少总时间步骤并提高推理效率。不同的是，我们采用了针对深度 SNN 量身定制的逐个过滤器的修剪技术，旨在调整模型大小并降低能耗，以增强与各种边缘设备的不同资源约束限制的兼容性。

分割学习。我们提出了另一个潜在方向来解决与 SNN 结合的资源受限边缘设备的局限性。解决边缘设备上深度 CNN 模型部署问题的一个有前途的方向涉及分解模型并以分布式方式将其部署到多个边缘设备上，这也称为分割学习 (SL) [Gupta and Raskar, 2018]。在 SL 中，它在服务器上执行模型的部分训练，以解决边缘客户端之间的全负荷问题，同时最大限度地减少资源受限网络中的延迟并防止敏感信息通过中间信息通信泄露。根据分割庞大的自上而下网络结构的方式，SL 可分为两类，即水平 SL 和垂直 SL。垂直 SL 缩小了完整模型解决的问题的范围，将特定的本地数据扩散到子模型，然后在云服务器上进行训练。 [Kim et al., 2017] 提出了一种解决方案模型，通过将类别聚类为组，将深度网络拆分为子网络树。[Chen et al., 2023] 将大型 DNN 拆分为多个轻量级的类特定模型，以满足传感器的内存和能量限制。[Hou et al., 2022] 采用深度强化学习方法确定 CNN 的最佳分区决策，旨在跨异构边缘设备部署以进行协作推理。相比之下，水平 SL 旨在找到最佳点来拆分整个模型，并在云服务器上训练每个部分。[Bakhtiarnia et al., 2023] 引入了一种基于通信信道状态的动态拆分计算方法。[Kim et al., 2020] 将单个深度学习架构分为通用提取器、云模型和本地分类器，用于分布式学习。 [Chen et al., 2021a] 提出了一种基于损失的异步训练和基于搜索的量化方法，以获得指数位和偏差的最佳组合。

然而，他们的工作仅考虑在边缘设备上分割 CNN。据我们所知，我们是将 SL 应用于 SNN 的先驱贡献者。此外，与之前的工作不同，在本文中，我们在深度 SNN 上采用了混合 SL 策略，其中完整的大型模型被划分为多个负责特定类任务的特征提取器和一个集成最终推理的融合模块。

# 3. Methodology
## 3.1 Design Overview
为了解决具有多个类别的分类任务，大型​​ DNN 模型，尤其是 CNN 模型 [Sengupta et al., 2019] 在应用中表现良好。类似地，深度 SNN 模型依赖于复杂的网络结构和广泛的参数训练来增强其功效。然而，这种原始 SNN 模型很难部署，尤其是在资源受限的边缘设备上。为了解决这个问题，我们建议利用 SNN 的特性和多个边缘设备的协作。具体来说，我们方法的关键思想是将原始 SNN 模型拆分为几个满足资源受限限制并充分利用多个可用设备的类特定子模型。

我们考虑一个分类系统，它涉及 N 个用于感知的边缘设备和一个用于分类结果的聚合服务器。图 1 概述了我们提出的方法的工作流程：边缘协作与脉冲神经网络 (EC-SNN)，它主要包含四个组件：脉冲模型训练、模型分裂、模型修剪和融合推理。在脉冲模型训练中，我们训练并从传统的 CNN 模型转换深度 SNN。在模型分裂中，我们将深度 SNN 模型拆分为包含类子集的子模型。为了减少计算开销，我们在模型修剪中修剪子模型。最后，我们将子模型分配到相应的边缘设备上，而聚合服务器融合提取的特征以获得最终的推理结果。每个组件的详细信息如下。
![[Pasted image 20241224222747.png]]


## 3.2 Spike-wise Model training
与传统的 CNN 模型不同，卷积脉冲神经网络 (CSNN) 使用脉冲神经元代替 ReLU 激活层，同时以相同强度进行再训练 [Deng and Gu, 2020]。在 CSNN 中，所有偏差成分都被消除，并且强烈建议使用平均策略进行池化操作，以模仿神经细胞的行为，该行为整合了感知到的所有信息以释放脉冲并增强生物保真度。

在本研究中，我们使用泄漏积分和激发 (LIF) 模型 [Gerstner and Kistler, 2002]，这是最有前途的脉冲神经元模型之一，由于其计算成本低且具有生物合理性，因此可以构建 CSNN。LIF 神经元不是像人工神经元那样直接将加权输入的总和传递给激活函数，而是将输入随时间进行积分，并产生泄漏。当积分超过阈值时，神经元会发出一个脉冲 [Eshraghian et al., 2023]，即离散事件。这些神经元抽象了层间脉冲的分布，因此传输的信息存储在脉冲频率中，而不是脉冲本身中，也称为时间信息集中 [Kim et al., 2023]。与传统的人工神经元相比，这种独特的信息传输导致了 CSNN 的节能优势。

此外，SNN 的整体有效性取决于时间步长 t，如公式 (1) 所示。与循环网络类似，较大的时间步长通常对应于反向传播过程中的梯度消失或爆炸现象等挑战。随着 t 的增加，它还会导致更多的计算操作和更多的能源消耗。因此，选择合适的 t 对于我们即将进行的比较不同指标的实验至关重要。

## 3.3 Model Splitting
在传统的 CNN 中，每个过滤器在学习和推断某些类别时都有其独特的影响 [Wang et al., 2020]，这提供了将原始 CSNN 模型拆分为几个特定于类的 CSNN 子模型的见解。每个子模型都经过精心策划，仅包含与其负责的类别相关的基本过滤器。如算法 1 所示，每个 CSNN 子模型都根据阈值 ζ 及其相应的类别进行修剪，遵循相对公平的工作负载分配。随后，启动贪婪搜索机制，以考虑能源和内存限制，确定最适合部署特定子模型的边缘设备。如果找不到合适的设备，则采用迭代方法来微调阈值并重复分配过程，直到所有子模型都成功分配到边缘设备上。

![[Pasted image 20241223144945.png]]

## 3.4 Model Pruning
传统的深度 CNN 模型通常包含四个部分，即卷积、激活、池化和全连接 (FC) 层，其中卷积是计算最密集的。修剪滤波器有助于加快推理执行速度并减少在传统 CNN 中存储参数的开销。考虑到 CNN 和 CSNN 之间的相似性，我们认为修剪滤波器以构建 CSNN 子模型也可以显着减少计算开销。例如，图 2 中举例说明了具有 6 个相同卷积滤波器的 CifarNet [Chen et al., 2021b] 的典型结果。因此，我们可以在测试准确率可接受的范围内降低能耗并缩小子模型。
![[Pasted image 20241223145306.png]]
具体来说，受 [Chen et al., 2023] 的启发，我们使用一种称为平均零百分比 (APOZ) [Hu et al., 2016] 的排名指标来评估每个过滤器对学习和推理的重要性。具体来说，每个卷积层都需要一个 LIF 层来为每个内部过滤器生成激活图。激活图可能包含一些零尖峰，这些尖峰传达了不太有用的特征信息。因此，我们应用训练后的 CSNN 模型来推断由特定类 C 束中的 N 个样本组成的传感数据。然后，卷积层 l 中过滤器 f 的 APOZ表示为：

# 3.5 Fused Inference
在结果融合阶段，边缘设备上的每个子模型都会感知输入并提取相应的特征。服务器通过连接聚合生成的特征，并将其馈送到 MLP 以获得最终的预测结果。请注意，一旦提供了所有经过训练的子模型，结果融合的 MLP 就需要进行训练。考虑到从子模型中得出的预训练权重，MLP 的大量训练周期被认为是不必要的。

# 4. 实验
## 4.1 Experimental Settings
考虑到该框架的广泛适用性，我们选择了 4 个非神经形态数据集（CIFAR、Caltech、GTZAN 和 UrbanSound）和 2 个神经形态数据集（CIFARDVS、NCaltech）来构建实验中的分类任务。此外，我们使用具有不同深度（分别为 5、9 和 16）的 VGG 结构架构 [Simonyan and Zisserman，2014] 对这些公共数据集进行分类。为简单起见，我们以 EC-SNN-VGG9 为例，表示我们提出的具有 VGG9 结构的 EC-SNN 方法。

所有模型均基于 Pytorch 和 SpikingJelly 实现。我们将所有网络的时间步长设置为 5，训练周期总数设置为 70。在训练过程中，我们使用 Adam 优化器，将余弦衰减学习率设置为 1e-4，并将批处理大小设置为 16。对于 LIF 神经元，时间常数 τ 设置为 1.33。每次试验都在一个 NVIDIA GeForce RTX 4090 GPU 和 9 个 Raspberry Pi-4B 上进行，作为边缘设备，用于评估特定子模型处理一个样本的执行时间。

## 4.2 Performance Evaluation
测试准确度。在这项工作中，最重要的关注点并不是主要集中在准确性上。对于该指标，我们的目标是保持准确性的有效性。与一个边缘设备上各种场景的 ANN 结构和 SNN 结构基线相比，EC-SNN 的准确度偏差在 1% 到 3% 之间，可以认为是可以接受的。根据结果，很明显，随着网络深度的增加，ANN 结构模型通常通过大量计算操作实现比 SNN 结构模型略高的准确度性能。尽管如此，令人印象深刻的是，在特定情况下，SNN 模型表现出令人惊讶的能力，略微超过 ANN 模型的准确性。这些结果证实了从 ANN 转换为 SNN 的可行性，而不必担心准确性的过度妥协。

当考虑 EC 场景时，表 1 展示了 EC-SNN-VGG9 的准确率随着边缘设备数量的增加而保持一致。在大多数情况下，最终融合预测的准确率波动保持在小于一个百分点的方差内。更多子模型的参与说明了部署更大规模模型的可行性，而不必担心高精度损失。

# 5.结论
本研究专门设计了一种名为 ECSNN 的新型模型分割框架来解决深度 SNN 的分割问题。该问题的解决为在多个边缘设备上部署深度 SNN 提供了可行的解决方案。EC-SNN 将深度 SNN 模型分解为较小的子模型，并利用通道修剪方法来简化复杂的网络架构，并通过显着降低能耗和延迟来实现更高效的推理。基于两种传感模式、三种网络架构和六个数据集，构建了大量实验来评估所提出的框架。结果表明，EC-SNN 可以显着减少边缘设备上的推理执行延迟并降低总体能耗，同时保持推理准确性

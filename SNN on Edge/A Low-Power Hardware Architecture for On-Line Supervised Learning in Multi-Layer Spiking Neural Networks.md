本文提出了一种事件触发的脉冲神经网络硬件架构，该架构具有权重相关的脉冲时序相关可塑性 (STDP) 学习算法。对原始学习算法进行了多项算法调整，以降低硬件复杂度并提高硬件的能效。此外，还采用了算法-硬件协同设计方法来提高性能。通过利用网络中脉冲序列和本地存储单元的稀疏性，算法的内存需求和每次学习迭代所需的时钟周期都显著减少。所提出的硬件架构采用 65 纳米技术实现。演示了一个配置为 256-50-10 的三层神经网络。设计的芯片可以对下采样的 MNIST 数据集进行推理，能耗为 1.12 μJ/推理，同时实现 90% 以上的识别率。

# 1. 介绍
脉冲神经网络 (SNN) 近来引起了电路界研究人员的广泛关注。与机器学习界广泛使用的传统人工神经网络 (ANN) 相比，SNN 基于事件的特性可以实现更节能的计算 [1],[2]。此外，地址事件表示 (AER) 可以方便地在大型网络中路由一位信息，从而实现更好的可扩展性 [2]–[4]。例如，4096 个核心使用 AER 互连，形成一个由一百万个脉冲神经元组成的大型网络 [2]。此外，一些 SNN 具有以渐进精度进行推断的功能 [5]，这有助于提高吞吐量并在准确性和能耗之间提供灵活的权衡。

由于上述优点，近年来已展示了许多硬件 SNN [1]–[3]、[6]–[10]。尽管在推理阶段表现出极佳的能效，但大多数 SNN 要么基于经过良好训练的 ANN 的离线转换 [1]、[7]、[8]、[11]，要么基于无监督学习 [6]、[9]、[10]。这些学习模型极大地限制了 SNN 芯片的适用范围。为了解决硬件 SNN 缺乏有效的片上监督学习算法的问题，在 [5] 中提出了一种硬件友好的学习规则，作为一种节能的多层 SNN 在线（片上）监督学习算法。该学习算法利用神经网络中的脉冲时序 (ST) 来估计梯度并进行随机梯度下降 (SGD) 学习。

==在本文中，我们为 [5] 中概述的学习算法提出了一种高效的硬件架构。所提出的事件触发架构提供了良好的可扩展性和低功耗。==提出了对原始算法的几种简化和调整，以提高性能并降低系统功耗。利用缓存结构来利用神经网络中现有的稀疏性，以减少内存需求。此外，还提出了一种后台 ST 更新技术来提高硬件 SNN 的吞吐量和能效。==所提出的硬件架构采用 65 纳米 CMOS 技术实现，并报告了各种性能指标。==

# 3. CMOS Implementation Results
所提出的硬件架构采用 65 纳米技术以及本文提出的所有技术实现。表 I 分别显示了学习模式和推理模式的估计功耗细目。表中的数字是使用 Cadence Innovus 估计的，包括从门级模拟中获得的布局后寄生和电路切换活动。观察到大部分功率消耗在第二层，因为它涉及大多数突触操作。此外，推理模式比学习模式消耗的功率更少，因为在推理模式下不会更新 ST 信息和权重。

SNN 的一个吸引人的特征是能够以渐进的精度进行推理，如 [5] 所示。它为设计人员提供了一个额外的旋钮来动态优化系统性能。图 7 绘制了当采用第一个脉冲 k 个脉冲编码方案时达到一定推理精度所需的时间和能量 [5]。首先发出 k 个脉冲的输出神经元被选为获胜神经元，并相应地读出相应的数字。整个训练集用于训练神经网络。如图所示，通过在推理中投入更多时间和精力可以实现不断提高的识别精度，从而提供可在运行时利用的额外自由。

表 II 总结了 CMOS 实现的性能。该性能还与最先进的 SNN 硬件实现进行了比较。与文献中的其他 SNN 芯片相比，本文设计的一个显着特点是它是一个多层神经网络，能够进行片上监督学习。

# 4. 结论
在本文中，==我们提出了一种用于多层 SNN 中的监督学习的硬件架构。学习是在芯片上使用我们提出的在线学习算法进行的。对原始学习算法进行了一些调整，以降低电路的复杂性，同时不降低性能。提出了一种事件触发架构来降低功耗。==此外，设计中利用了脉冲序列的稀疏性来减少内存需求。还利用了后台 ST 更新来加速算法的推理和学习过程。该设计采用 65 纳米技术实现，并展示了如何以渐进精度进行推理以节省能源。
# Abstact
数字模拟混合神经网络利用高效的模拟计算和数字网络内通信进行特征提取和分类。==利用局部竞争算法 (LCA) 固有的低 SNR 要求，内部模拟神经元比等效数字设计小 3 倍，能效高 7.5 倍。==这项工作展示了使用传统可扩展数字工作流程大规模集成 512 个模拟神经元，以实现 3.43TOPS/W 的最佳功率效率，用于对象分类。在 48.9pJ/像素和 50.1nJ/分类下，原型 512 神经元 IC 的效率是数字设计的 2 倍，同时保持了 PVT 上可靠的分类结果。

# 1. 介绍
当前机器学习的革命是基于由许多层组成的数字前馈 CNN。众所周知，这些 CNN 非常耗电。相比之下，局部竞争算法可以用更少的层完成类似的任务 [1]。LCA 除了在脉冲神经网络中使用前馈连接外，还使用横向抑制连接，如图 1 所示。LCA 依赖于受生物启发的模拟漏积分和激发 (LIF) 神经元模型，这表明这是一种高效的模拟实现。

==我们引入了一种模拟神经元，它比同类数字神经元小 3 倍，功耗低 7.5 倍，利用了 LCA 的稀疏尖峰行为和低 SNR 要求。我们的内部模拟神经元提供全数字 I/O，以通过地址事件 (AE) 实现高效的全数字神经元间通信，并实现与数字 CAD 流程的兼容性。原型 512 神经元 IC 实现了 48.9pJ/像素和 50.1nJ/分类，比最先进的数字实现 [2] 效率高 2 倍，同时在宽温度（0C 至 100C）和 0.8 至 0.95V 电源范围内保持可靠的分类，适用于 20 个测量设备。==

## 模拟/数字神经元比较
在 LCA 所要求的中等精度下，对每个时钟周期的能量进行基本分析清楚地表明，模拟神经元的能源效率可能提高 1,000 倍。图 2 比较了 LIF 神经元的等效数字和模拟实现的能量与比特分辨率（假设保守稀疏度为每 10 个周期触发 1 次）。在此比较中，数字神经元的功耗是针对在 40nm CMOS 中合成的具有各种位宽的神经元。模拟 LIF 神经元的精度由积分电容器的 kT/C 噪声和比较器噪声决定。模拟 LIF 神经元的能量消耗主要由更高分辨率下的 CV2 能量损失决定。比较器噪声是负载电容的函数，它设置比较器能量。由于积分电容器的能量仅在 LIF 神经元激发时才会耗散，因此模拟神经元充分利用了 LCA 的稀疏激发行为。

## Analog Neuron
紧凑型模拟 LIF 神经元（图 3）分为两个阶段：激发和抑制。在激发期间，神经元会累积来自输入像素的加权刺激，并将总和存储为激发项。在抑制期间，神经元会根据 LCA 的动态进行演化，累积激发项，同时减去泄漏项以及由来自其他神经元的传入尖峰触发的任何加权抑制项。当神经元电位达到阈值时，它会输出尖峰以抑制其他神经元。
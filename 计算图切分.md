几个问题：
- 对现在 ANN2SNN 这种方法对 SNN 进行研究，保持怀疑态度，ANN2SNN 这种方法本质上模型结果的好坏完全取决于  ANN 的设计，而且这种方法有点像量化，但又不是完全彻底的量化，导致这种 ANN2SNN 的方法研究出的 SNN 有点四不像，性能不如原本 ANN，量化的压缩又不如完全的量化模型，甚至引入时间步，导致模型推理时间严重增加。虽然这种方法研究出 SNN 结果是很比 STDP 这些更加符合生物合理性的 SNN 效果更好。
- 计算图切分，算子拆分，找到公用模块，conv+BN，Pool 层，这部分做的类似与算子融合吗（要怎么融合？）

两种主流方式， ANN2SNN，直接训练 SNN，模型结构和训练方法不一样

ANN2SNN ，在训练好的 ANN 上进行模型参数类似于量化操作，映射到发放率区间内$[0,r_{max}]$ 
内，并将 ReLU 激活函数换成 IF 神经元：
- 可以对每一层的参数 W 和 b 进行压缩
- 直接对激活前的输入进行压缩

围绕拆分后的算子进行容器化设计，不同算子一定要进行合并，那么合并的准则没有一个很好的出发点（参数量、功耗）。比如放在 Darwin 上的核心是根据其内部的硬件条件（内存大小）进行选择哪些层的放置，而且这一点是必须要做的（根据底下的硬件配置）。

一个合并思路就是 Conv2d 与 BN2d 进行合并，这是因为数学上是等价的，（w1 和 w2 的相乘）两个层的合并。

# ANN2SNN，VGG 模块转换
卷积层，就是将脉冲输入$[T, B, ...]$ 看作 $[T*B, ...]$，然后当成普通数据进行处理。处理完后，在还原回来

参考 Spikejelly 的转换方法。
![[计算图.drawio (1).png]]

## Pool 池的转换
maxPool 无法直接哪来用，不能直接用到 SNN 中，需要进行适当的准换。

因为是直接在训练好的 ANN 上转换，转换时可以自动将 conv 层与 BN 层进行融合

## Softmax 的转换
SNN 中没有 softmax，改成 IF 神经元并简单统计 Spike Count 的总数



# ANN2SNN，ResNet
与 VGG 类似，主要是残差怎么处理的问题
![[Pasted image 20250422152619.png]]
# VGG-16
![[Pasted image 20250422140400.png]]

![[Pasted image 20250422140519.png]]


# 直接训练 SNN （Transformer）


![[Pasted image 20250428230737.png]]
引自 Spike-driven Transformer

实现了脉冲输入版本的 self-attention 机制。

## ANN2SNN transformer
Towards High-performance Spiking Transformers from ANN to
SNN Conversion，针对量化设计补偿模块

针对 softmax，gelu，layernorm 这些非线性层设计补充模块
![[Pasted image 20250506153334.png]]


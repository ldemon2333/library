四个类脑计算主要部分的现存方法：算法，硬件平台，软件工具，基准数据。

# 摘要
类脑计算，跨学科，
- What are the key issues to enhance the development of BIC? 
- What **roles** do the current mainstream technologies play in the general framework of BIC?
- Which techniques are truly useful in real-world applications?
首先阐述类脑计算的最大挑战：
- 人工智能模型如何从计算神经科学的最新进展中受益？
BIC结构的四部分
- 模型/算法
- 硬件平台
- 软件工具
- 基准数据

# 1.介绍，动机和概述
ANNs不太可能重构大脑，因为它们只利用了单个神经元的算术运算概念，而没有考虑神经网络的动态性，也就是说，它们忽略了大脑在单个神经元、突触、神经回路、网络和系统层面的内部结构/功能，一个根本的问题是如何利用神经科学的先进研究成果来增强人工智能模型。

BIC定义到现在也不够明确。

TrueNorth chip，一种非冯·诺依曼架构的神经形态芯片，具有可编程脉冲神经元和可配置突触，神经形态计算的另一种术语，描述模仿生物神经系统某些功能的设备和系统。2014年建成

两个概念，“neuromorphic computing”和“BIC”是两个不同的概念。neuromorphic computing说明神经元是如何组织，交流和学习在硬件层面上，同时也是实现BIC的关键。

BIC 是一个新兴的研究领域，旨在通过学习信息处理机制以及生物神经系统的结构和功能，构建面向更通用的人工智能的基础理论、模型、硬件架构和应用系统，即通过SNNs去产生更多模型理论，训练方法，硬件平台和应用系统以突破现在AI的瓶颈。

SNNs，它们可以自然地表示神经系统的多尺度动态特性，并包含来自计算神经科学领域的单元，包括树突、突触、胞体和轴突。

从这个意义上讲，BIC 可以很容易地与当前基于 ANN 的深度学习技术区分开来，在 ANN 中，无论网络有多深、多宽、多复杂，每个神经元都是一个乘法和累积 (MAC) 单元，后面跟着一个非线性函数。

BIC，节约能源，有望突破冯诺依曼架构的瓶颈。

深度学习成功归因于系统学习理论，比如随机梯度下降方法，各种基准任务和数据集，友好的编程工具和高效处理平台GPUs，高并行和内存带宽。
![[Pasted image 20241014105410.png]]![[Pasted image 20241014105555.png]]
在modeling/algorithm侧，现有的方案涵盖了细胞层面的单个神经元和网络层面的SNN模型及其训练机制。SNN=ANN+neuronal dynamics。ANN和SNN可以共享相同的网络拓扑，区别是SNN中的神经元根据neuronal dynamics被描述为不同的equations，其中spike取决于相应的dynamics。神经元动力学有很多种：甚至动力学模型可以存在于胞体soma或树突dendrites中
- 简化为一阶微分方程（the leaky IF model）
- 一组微分方程（the Hodgkin-Huxley（H-H）模型）
从学习算法，目前方案可以分为三类：
- unsupervised learning
- ANN-to-SNN conversion
- direct training algorithms
我们主要关注SNN领域的算法。除了SNN，还有其他受大脑启发的算法：
- liquid state machine
- echo state networks
- continuous attractor neural networks(CANNs)
从更一般的层面上看，借鉴大脑的学习规则和组织结构而开发的模型或算法都属于BIC领域的范畴。
我们认为，从更具生物合理性的神经元（如multicompartment neuron models or refined neuron models）中学习具有巨大的潜力，同时必须考虑以下四个方面：bio-plausibility, effectiveness, efficiency, and trainability.

在硬件平台这一侧，BIC chips 也叫 neuromorphic chips。不同于深度学习加速器，BIC 芯片目标从一开始模仿类脑SNNs。在大脑皮层中，计算和记忆是集成在一起的，而不是像冯·诺依曼架构中那样明确分开。受此启发BIC 芯片采用多核分散式架构，其中每个核心都有紧密耦合的本地计算和内存资源。BIC 芯片具有
大规模计算并行性和高内存局部性，无需访问片外内存。主流BIC 芯片和平台可以被分位三个独立部分：
- functionality（BIC 芯片可以被分位三类）
	- 支持SNNs
	- 支持SNNs和ANNs
	- 支持learning rules
- computing architectures
	- near-memory-computing 架构
	- in-memory-computing架构
	- ANN accelerator variants
- implementation techniques，，the trade-off变得更加复杂，因为需要综合考虑许多因素，包括应用场景、性能、功耗和面积 (PPA) 以及可编程性

在软件侧，目前类脑软件可以划分为三类，根据它们使用和基础结构
- 神经形态工具链，通常是为特定芯片同时设计的，旨在促进高级模型设计并将程序编译为由芯片支持的计算原语描述的低级可执行代码。
- 算法编程平台，算法编程平台中的软件希望促进 SNN 的实现并利用计算机科学的进步。
- 脑网络模拟器，旨在通过支持多种神经活动和突触模型来模拟生物神经网络，或在缺乏广泛部署的硬件的情况下，作为验证硬件性能、测试潜在硬件修改和开发脑启发算法的重要工具。
软件工具制造以提升神经形态硬件的效率，类似Pytorch和TensorFlow框架以促进算法和应用的发展。

在基准数据集侧，现有BIC 数据集可以被分为两类：
- 模拟数据集
- 真实世界数据集
从数据集生成的角度，可以被分为
- 单模态数据集
- 多模态数据集
from the modality perspective，模拟BIC数据集通常基于event-based simulators 产生的，或者event 相机在LCD监视器上记录popular frame-based datasets。现实世界数据集通过使用神经形态传感器直接记录各种现实世界物体来包含事件数据。关于数据集模态角度，它关心异步时空事件是否由单个神经形态
传感器或多个不同的传感器生成。随着各种应用中的数据集越来越多，一个基本问题是

随着各种应用中的数据集越来越多，一个基本问题是，BIC 数据集受生物神经系统启发的关键特性是什么。


然而，该领域工作繁多、方法各异，非常需要自上而下地指导研究人员，尤其是初学者选择方法。迫切需要对 BIC 研究进行全面调查，以帮助正确识别和分析这些令人困惑的方法。促进 BIC 发展的关键问题是什么？当前主流技术在 BIC 的总体框架中扮演什么角色？哪些技术在实际应用中真正有用？这些问题非常有趣，但在很大程度上仍未解决。此外，整个社区也期待着有见地的机遇和挑战。在这篇调查中，我们将系统地总结四个组成部分的大多数现有方法，并描述它们的主要挑战和未来趋势。基于学习算法、硬件芯片、软件工具和基准数据集的共同设计，我们将提出一个 BIC 系统的总体框架，这有望造福人工智能和脑科学。

最后，本文的结构总结如下。第二部分简要介绍了一些初步知识。第三至第六部分将依次总结 BIC 模型和算法、硬件平台、软件工具和基准数据的最新进展。第七部分将提出 BIC 系统的框架。第八部分总结了本文，澄清了一些普遍的误解，并提出了许多可能的机遇和挑战。

# 2. 简单的预备知识，概念和主要挑战
DNN中的卷积操作是受到感受野(receptive field)启发的，感受野是神经科学的一个基本概念。因此，随着 DNN 的蓬勃发展，“BIC”概念与其对应概念“深度学习”之间的界限变得模糊。
![[Pasted image 20241014142226.png]]
首先简短的介绍ANN和SNN的概念各自主要起源于深度学习和计算神经科学。SNN与ANN的不同是SNN利用了大脑中的神经动力学。我们还将区分用于支持 ANN 的 DNN 加速器和用于支持 SNN 的神经形态芯片。稍后，我们将介绍 BIC 的两个定义：经典/广义 BIC。同时，我们将 BIC 的范围与两个新兴研究领域进行比较，即人工智能大脑和大脑人工智能。经典 BIC 是人工智能大脑的一个子集，它专注于 SNN 模型和神经形态芯片；其相关应用也受到计算神经科学的启发，而广义 BIC 则同时考虑人工智能大脑和大脑人工智能领域。

## A. 人工神经网络
一个神经元是神经网络的基本单元，它接受与它连接的前神经元的信号，并执行一个非线性变换，然后产生一个输出信号，向后多播到其他神经元。连接可以称为突触，连接的功效视为weight（$W$）,神经元信号视为激活（$x$），非线性变换视为激活函数（$\varphi(·)$）.神经元$i$的输出用$y_i$表示：
$$
y_i  = \varphi(\sum_jW_{ij}\, x_j+b_i) \tag{1}
$$
$b_i$是偏置。

## B.Spiking Neural Networks
SNN可以被视为ANN，用神经动力学代替每个神经元或突触权重。
$$
\rm{SNN=ANN +\rm{Neuronal} \,\rm{Dynamics}}\tag{2}
$$
神经动力学有很多种。
![[Pasted image 20241014143930.png]]
b）胞体动力学 ：胞体动力学可以有膜电位反应，用$u(t)$表示，当膜电位超过阈值$v$，胞体就会发放一个spike
c）树突动力学：树突几何形状可分为多个隔间，每个隔间可视为包含电容和电阻的等效电路（电缆理论模型），接收来自不同来源的信号。子图显示了树突动力学的一个隔间。

SNN 中的尖峰依赖于可以轻松引入 BIC 的神经回路概念，如今 BIC 正成为传统神经计算的一种有前途的节能替代方案。

## C. ANN加速器 vs 神经芯片
ANN 加速器的架构通常采用冯·诺依曼架构的变体，而神经形态芯片从一开始就以模拟大脑启发的 SNN 为目标，在设计芯片架构时会考虑生物神经元的每个部分（树突、胞体、突触和轴突）

## D. 神经形态计算
神经形态计算也称为神经形态工程，它基于使用超大规模集成电路 (VLSI) 系统来模拟神经系统的生物功能。本调查主要指使用硬件神经元进行计算的任何设备。最近，“神经形态”一词已用于描述模拟、数字、模拟-数字-混合 VLSI 和实现神经系统模型的软件系统。神经形态算法通常指 SNN，神经形态计算通常被认为是在神经形态芯片上运行 SNN。在本次调查中，“神经形态”是经典 BIC 的一个子集，因为它与硬件更相关，而硬件依赖硬件神经元进行计算。

## E.经典BIC
经典 BIC 旨在通过学习生物神经系统的机制、结构和功能来构建理论、模型、架构和硬件系统。从建模角度来看，经典 BIC 主要指受计算神经科学启发的 SNN 模型、神经形态芯片及其相关应用。从计算架构角度来看，经典 BIC 通常利用near-memory-computing 架构和in-memory-computing 架构。在本综述中，经典 BIC 是神经形态计算的超集，因为经典 BIC 不局限于硬件，其理论、模型、架构和硬件系统都可以在行为和物理上受到生物神经系统的启发。因此，神经形态芯片也可以称为 BIC 芯片。

## F. Brain for AI
Brain for AI 目的是为了提高AI技术，受到以下启发：
- 神经系统种的信号传输机制和学习法则
- 大脑的结构和功能
- 人类的心理或认知处理
达到以下目标：
- 减少AI对资源和能源的消耗
- 在对大脑来说是初级的但是对传统AI来说困难任务取得可比的性能
- 为下一代人工智能建立通用原则和框架
- 解决深度学习领域现有的问题
经典BIC是brain for AI的一个子集，前者侧重于系统的计算/学习能力，后者还可以指人工智能一般性的领域。

## AI for Brain
AI for Brain通过AI技术推导脑科学的发展
- 使用AI框架来解释大脑复杂的现象
- 对大脑结构和功能有一个更好的理解（此外，AI可以加强大脑成像技术并促进大脑结构和功能方面的研究）
- 预测认知，发展和心理健康
- 控制行为和心理过程

## H. 广义BIC
广义 BIC 的范围考虑了brain for AI 和AI for brain两个领域。考虑到这一点，大多数新兴技术，如经典 BIC、脑机接口、脑图谱、神经图像，以及与人工智能和大脑结合相关的各种应用 ，都属于广义 BIC，其概念必须与科学和工程的发展保持同步并不断变化。

## I. BIC vs 深度学习
目前对BIC与深度学习的区别还缺乏系统的讨论，我们需要明确区分“BIC”和“深度学习”两个概念，后者在各个学科中都蓬勃发展。注意，深度学习是一个更广泛的概念，使用的典型网络是ANN（CNN，RNN，Transformers等），其中每个神经元都是一个MAC单元，后面跟着一个非线性函数，无论网络有多深，多宽，多复杂。由于ANN是深度学习中最常用的模型，我们在这里将BIC与深度学习的区别简化为BIC与ANN的区别。因此，在这篇综述中，深度学习主要被认为是建立在ANN之上的，我们在这里将BIC与深度学习的区别简化为BIC与ANN在三点上的区别。
- 不同的开发动机：尽管早期的 BIC 系统和 ANN 都受到大脑神经元和电路的启发，但这两个领域逐渐走向了不同的方向。现代 ANN 是由实际任务驱动的，例如高精度处理图像、语音信号和文本，无论模型是否具有生物学合理性。相比之下，BIC 的发展总是更多地受到神经科学的驱动，包括大脑中的生物神经元、电路和区域。
- 不同的建模特征：ANN 更注重应用任务的端到端准确性，而忽略了内部过程。BIC 社区广泛使用的模型是 SNN，它是生物神经元和电路的抽象。因此，在 SNN 模型中，有两个特征与 ANN 根本不同：时间动态和脉冲活动。膜电位的时间动态可以描述内部状态演化过程，这将影响网络动态，如attractors、working memory 和 robustness。例如，先前的研究发现 SNN 在抵抗噪声和对抗攻击方面比 ANN 更强大。神经元的脉冲活动表明 SNN 模型是一个可激活的系统。具体而言，脉冲神经元只有在发射脉冲时才会刺激突触后神经元，这与 ANN 根本不同。
- 不同的计算行为：ANN 模型采用同步和密集的 MAC 操作，因此需要强大的硬件（即 GPU 和 AI 芯片）才能实际执行。相比之下，神经形态/BIC 芯片采用非冯·诺依曼架构，其结构和功能受到大脑的启发。一些独特的基本操作性质，包括高并行性、共置计算和内存、固有可扩展性和尖峰/事件驱动计算，源于它们选择将生物可信的神经元和突触作为主要计算单元。因此，SNN 算法的重要性与支持 SNN 的神经形态芯片的独特计算特性相结合。因此，BIC 模型通常采用异步和稀疏累积操作，因此在神经形态硬件上非常节能。这就是为什么 AI 芯片更多地优化高计算能力，而神经形态芯片更注重低能耗的原因

## J. BIC的主要挑战
主要的挑战在于BIC系统如何利用计算神经科学的先进成果来弥合人工智能与神经科学之间的鸿沟。具体来说，挑战在于如何通过学习生物神经系统的机制、结构/功能，更好地对BIC的四个组成部分（即模型/算法、硬件芯片、软件工具和基准数据集）进行协同设计，并构建研究生态以不断促进BIC的繁荣。这四个组成部分是BIC系统的基础设施，将在以下章节中回顾。


# 3.模型和学习算法
## A. 从单个神经元到SNN
1）生物神经元建模：作为神经系统的基本单元，生物神经元的主要功能是将大量突触输入转换为有意义的输出动作电位流。典型的神经元结构包括四个部分：树突，突触，胞体和轴突。树突收集从突触那其他神经元传入的输入信号，并将输入信号传入胞体，当输入信号更新神经元膜电位并使他超过一个阈值，胞体产生动作电位，脉冲沿着轴突无衰减地传播，并通过轴突末端的突触将信号传输到下游神经元。
![[Pasted image 20241014152248.png]]
在建模单个神经元时，生物合理性和计算复杂度是我们主要关注的点。

Herz 等根据特定目标将单神经元建模抽象为五个层次。前三个层次（1-3 级）关注精细结构和生理细节，主要用于计算神经科学领域。后两个层次（4 和 5 级）关注抽象结构和计算效率，常用于人工智能领域。目前的情况是，人工智能与计算神经科学之间存在差距，即 4 和 5 级的人工智能领域的模型几乎不可能利用 1-3 级神经元模型的发现。确定与人工智能相关的神经元动力学、电生理学、神经化学和调控机制的哪些细节是重要的，哪些可以忽略，对于 BIC 的建模至关重要。

在本综述中，我们从时间和空间的角度介绍了神经元模型，如图 5 所示。在空间复杂性方面，单室模型仅考虑了胞体的建模，完全忽略了树突和轴突的存在。其中，最著名的是 LIF 模型、H-H 模型和 Izhikevich 模型。相比之下，多室模型考虑了细胞的形态，因此它不再是点神经元。神经元的树突结构被简化为一组稀疏连接的隔室，而轴突由于其对输出的影响微不足道而经常被忽略。输入流被注入到树突区室的固定子集中，这些区室与胞体的距离可能彼此不同。信号以非线性方式在相邻的树突区室之间单向或双向传播。胞体从相邻的区室获取输入，像在点神经元模型中一样处理信息，并产生输出信号。多区室模型比单区室模型更符合生物学原理。此外，由于这种模型中只有少量的区室，因此可以在人工智能框架中使用该模型。最后，具有最高空间复杂度的详细神经元模型试图使用数百（甚至数千）个区室精细地再现真实神经元的形态，以尽可能接近地模拟生物神经元内的信号传输过程。这种昂贵的模型在人工智能范围内几乎没有意义。就时间特性而言，神经元模型可根据其是否表现出时间动态特征分为两类。例如，ANN 神经元产生的输出没有时间维度。另一方面，脉冲模型模拟了细胞体（和树突区室）中膜电位（可能还有其他状态变量）的动态，其输出是一段时间内的脉冲序列。
![[Pasted image 20241014152930.png]]
最受欢迎的spiking 神经元模型是LIF模型，它结合了膜电位整合、泄漏项和基于阈值的触发，同时忽略了特定的离子电流和形态细节：
![[Pasted image 20241014155210.png]]
$u$代表膜电位，$\tau_m$代表膜电位时间常数，$u_{rest}$是初始电位，$I$作为输入电流，$R_m$ 是优点leak 电阻，$t_f$ 是spiking time。LIF模型可以达到更低能源消耗通过编码信号用二进制事件。

另一种广泛使用的单室模型，即 ANN 点神经元，试图以放弃时间动态为代价来降低计算复杂度（见 (1) 和图 5）。信号以连续值编码，ANN 仅在空间域中传播信息，尤其是在前馈网络中。作为一种特殊情况，二进制 ANN 神经元通过用二值化函数替换连续激活函数来在彼此之间传输 0 或 1 值。

图 6 比较了神经元模型的生物学特征及其能量成本。通常，具有更高生物合理性的模型将消耗更多能量，因此代表这些模型的点分布在图的对角线上。一方面，LIF 神经元模型不仅比典型的 ANN 神经元包含更多的生物学特征，而且由于用高效的 ac（累积）操作取代了昂贵的 MAC 操作，因此消耗的能量更低，尽管必须承认，当前的冯·诺依曼计算机可以更好地执行 AI 任务中 ANN 的密集矩阵向量乘法。然而，二元 ANN 神经元可以被认为是最简单的没有时间动态的脉冲神经元，并且它消耗的能量成本最低。另一方面，更复杂的神经元，包括精细神经元模型，包含更多的生物合理性特征，但需要更高的计算复杂度和能量成本。未来的研究可以集中在图6左下角的虚线圆圈上，以提高计算效率并降低功耗，或者关注右上角的圆圈，以设计具有更多生物特征的模型，以完成传统AI难以完成的任务。

2）将神经元模型嵌入到SNN中：为了进一步实现BIC的目标，需要将上述神经元模型嵌入到神经网络中。图7显示了三种常用的神经网络架构，包括MLP、RNN和CNN，其中相应的基本层称为全连接（FC）层、循环层和卷积（Conv）层。也可以使用具有其他拓扑结构的网络。虽然这些网络通常使用ANN神经元作为其基本单元，但我们可以通过添加时间动态简单地用脉冲模型替换神经元。然后，我们可以构建相应的SNN。显然，选择神经网络中使用的神经元模型的挑战在于在生物合理性和计算复杂性之间进行权衡。例如，形态逼真的模型（如图5（a）所示）可以很好地近似真实神经元的动态，但它们的高维数将导致巨大的计算负担。因此，精细神经元模型不适合大规模网络。相反，简化的隔室模型可以大大缓解高计算成本的问题，同时仍然模拟树突和细胞体之间的真实相互作用。它们可以在对人类大脑来说很自然但对人工智能来说很难的任务中展示它们的优势。最终，由于其简化的形态和基于事件的性质，LIF 模型具有极低的能耗，因此在大多数当前的 SNN 中都使用。总之，网络中神经元模型的选择可能取决于任务设置和研究人员的兴趣领域。
![[Pasted image 20241014160133.png]]

## B. 学习算法
梯度下降算法与误差BP相结合是当前ANN优化理论的核心，其一系列变体从vanilla SGD 发展到ADAM和AMSGrad。此外，规范化方法和分布式训练的引入使得大规模高性能ANN的实现成为可能，广泛应用于实际的AI场景中。相比之下，SNN领域没有公认的核心学习算法或技术。由于对生物合理性和任务性能的重视程度不同，以及网络中采用的神经元模型和编码方案不同，学习算法表现出明显的多样性。

目前的SNN算法可分为两类（图8）：源于生物突触可塑性的无监督学习和结合深度学习方法的监督学习算法。后者又可分为ANN到SNN的转换和直接训练算法。一般来说，无监督学习算法注重生物合理性，ANN到SNN的转换算法主要考虑有效性并且直接训练更注重效率和可训练性。下面我们将对每一种方法进行介绍，它们的比较研究总结在表1和表2中。
![[Pasted image 20241014174200.png]]
1）无监督学习：突触可塑性被认为是神经记忆和学习的生物学基础。SNN 允许一种依赖于直接连接的神经元对之间脉冲相对时间的生物启发式学习，例如Hebbian学习和脉冲时间依赖性可塑性 (STDP)。与逐层 BP 不同，权重修改所需的信息是本地可用的，这可以在分布式计算和在线学习中轻松实现。与 [226] 中的观察结果非常吻合的常见描述可以表述为
![[Pasted image 20241014174551.png]]
$\Delta w_{ij}$ 表示权重改变，$t_i$ 和 $t_j$ 代表突触前和突触后的发放时间，相应的$\tau^+$ 和$\tau^-$ 代表常数影响time window的尺度；$a^+$ 和 $a^-$对应长时辰增强（LTP）和长时抑制（LTD），突触前和突触后神经元的相关脉冲可导致突触增强或减弱，具体取决于脉冲的时间顺序[见图 9(a)]。
![[Pasted image 20241014175011.png]]
SNN 算法的一个目标是验证突触可塑性规则在智能系统构建中的潜在作用，并且已经证明网络级学习可以是时间相关的突触动力学的结果。在单个神经元层面，Guyonneau 等人发现，在 STDP 规则下的突触模拟中，传入神经元群发出的靶向脉冲序列可以在单个突触后神经元中引发快速识别和选择性响应。Diehl 和 Cook 展示了一个两层 SNN，仅由单层兴奋神经元及其一一对应的抑制神经元层组成，其中来自输入的兴奋连接由 STDP 训练，随后的抑制层确保神经元之间的侧抑制和竞争。完成无监督训练后，兴奋神经元能够选择性地对输入特征做出反应，在 MNIST 上获得 95% 的准确率。Masquelier 和 Thorpe 设计了一个基于 STDP 的前馈 SNN，模仿大脑中的腹侧视觉通路。网络逐渐对共同特征产生选择性，缩短激发神经元所需的延迟，最终快速发出包含图像重要特征信息的脉冲，并可进一步用于分类任务。此外，已报道了将 STDP 规则应用于脉冲 CNN 的研究，其中包含多个卷积层用于特征提取，而以前的模型大多只包含一层 STDP。最后一个神经元的膜电位被插入支持向量机 (SVM) 作为分类器监督训练的输入，在 MNIST 上实现了 98.4% 的准确率。Illing 等人表明，自监督和局部可塑性规则可以学习深度分层表示。

Bienenstock–Cooper–Munro (BCM) 规则是 1982 年基于视觉皮层实验提出的另一条突触可塑性规则。原始的赫布学习规则没有包括突触连接的衰减或增强机制，导致构建的模型不稳定。因此，BCM 规则假设神经元具有动态适应历史权重变化的阈值，并利用该阈值来确定突触变化的趋势，使连接最终达到稳定状态。后续的突触权重关联训练 (SWAT) 将可变阈值纳入 BCM 规则中，对 STDP 窗口的形状形成负反馈调节，增强训练期间的稳定性。

2）ANN-to-SNN 转换，基本思想是使用ReLU的激活函数得到的激活值可以近似用SNN中的平均发放率来替代，对离线训练好的ANN模型进行适当的权重调整。本质上，训练依赖于ANN的BP算法。各种转换trick 使得时间步的需要数大幅下降。

总体而言，ANN 到 SNN 的转换允许将 ANN 的突破快速转化为 SNN 领域，并作为利用神经形态平台的有效方法，但它也有其固有的局限性。除了对原始 ANN 施加的限制导致的性能下降之外，低延迟转换的 SNN 是一个持续的研究挑战，因为与直接训练 SNN 相比，它仍然需要很长的模拟长度才能完成推理，这可能会导致额外的延迟和能量消耗。大多数转换后的 SNN 仅适用于静态图像，不适用于神经形态流数据。此外，转换方法更注重缩小 ANN 和 SNN 之间的性能差距，而不是探索内在的动态或 SNN 的独特性，因此它们在推动大脑启发智能方面的作用相对有限。

3）直接训练：端到端的SNN训练使用BP算法，训练方法主要分为两类：rate coding 和 temporal coding。在主流机器学习框架中，通常使用（3）的显式迭代版本来实现它，并将阈值门控非线性表示为
![[Pasted image 20241015101936.png]]
这里，$o_i[t]$和$u_i[t]$表示输出spike 和神经元$i$ 在 $t$ 时刻的膜电位，$\Theta$ 代表Heaviside step 函数受阈值$u_{th}$ 控制并表示尖峰输出二进制形式不可微状态。基于rate编码的方法解决了触发函数与替代导数的不可微性问题，并计算相对于脉冲激活的梯度，而基于时间编码的方法则关注现有脉冲的时间，并计算相对于脉冲时间的梯度。

对于前者，原始的学习方法主要基于 Windrow-Hoff 规则，但只能训练层数较少的 SNN。用代理梯度来使得BP through time 训练SNN。

在 Lee 等人的研究中，脉冲输入通过低通滤波器对膜电位产生持续影响，而膜电位的突然改变则被归类为噪声并被忽略。Jin 等人引入了 HM2-BP 算法，这是一种混合方法，将 SNN 中的误差 BP 分解为两个部分：由突触输入引起的突触后电位的微观变化和由rate编码定义的损失函数的宏观 BP。Wu 等人提出了时空 BP (STBP)，该算法基于迭代 LIF 模型和脉冲活动的几个近似导数，同时考虑了roll-out SNN 中的空间和时间权重分配。最近的一些研究提出了可学习替代梯度的概念，这是一种能够在训练过程中动态调节函数形状和平滑度的机制，旨在减轻梯度估计可能产生的差异。

在基于时间编码的方法中，通常使用脉冲响应核来描述一个神经元的脉冲事件如何影响另一个神经元，这使我们能够在没有显式积分的情况下模拟SNN，并且将单个放电时间（而不是0/1放电模式）视为神经元的状态变量，例如
![[Pasted image 20241015104553.png]]
这意味着梯度仅在脉冲发生时传播，而在基于rate编码的方法中，它会在整个时间窗口中发生。SpikeProp 是这一类别的先驱，其中隐藏单元的脉冲时间表达式是线性化的，允许对近似隐藏层梯度进行分析计算。 EventProp 是第一个为连续时间 SNN 提出的，用于反向传播脉冲时间的误差并以基于事件、时间和空间稀疏的方式计算梯度。

虽然基于rate编码的方法确实产生了明确定义的梯度，但它们可能受到某些限制 。由于无法通过学习脉冲时间来描述脉冲的产生和消除，因此网络可能相对脆弱，需要良好的初始化状态。每次试验只允许一个脉冲是这种方法中常见的神经元约束。此外，需要复杂的排序算法来配置清晰的梯度 BP 逻辑链，这阻碍了这种方法在深度网络中的适用性。

直接训练的网络擅长高效编码信息，与转换网络相比，所需的时间步长明显减少，这对于在节能的神经形态硬件上实现尤其有吸引力。此外，它们本质上更适合处理来自新兴地址事件表示 (AER) 传感器的时空数据，这项任务不太适用于 ANN 和转换的 SNN。不幸的是，直接训练的 SNN 的一个突出问题在于模型规模有限。神经网络的容量对于它们的成功无疑是至关重要的，但早期直接训练的 SNN 通常会经历严重的准确度下降，并且仅限于浅层结构和简单任务。受深度 ANN 的表示能力的启发，人们更加重视面向 SNN 的网络结构和训练的设计。新兴的研究，例如规范化技术、脉冲残差学习、和基于注意的 SNN，正在逐渐缩小与 ANN 相比的性能差距。诸如 SpikeGPT 和 SpikingBert 之类的模型符合当前从大型语言模型中寻求智能的趋势，并展示了大规模 SNN 在更复杂任务中的巨大潜力。它们提供与非脉冲模型相媲美的性能，同时降低了计算复杂度和能耗。


## C. 关键考虑点
需要注意的是，神经科学长期以来一直是人工智能进步的重要驱动力；因此，开发真正的BIC模型/算法尤为关键，而如何构建更具生物学合理性的方法来定义或执行一些当前人工智能模型无法完成或做得不好的任务将成为亟待解决的主要任务。我们认为，利用受单个神经元或神经系统的结构/功能启发的神经动力学具有巨大的潜力，例如多室神经元模型甚至精炼神经元模型。为此，必须考虑四个关键方面，如图10所示，包括基于生物合理性的巨大发展潜力、面向实际应用场景性能需求的发展目标、稀疏脉冲活动带来的固有低能耗发展优势，以及大规模SNN的可训练性造成的发展瓶颈。

![[Pasted image 20241015105131.png]]

1）生物合理性：如前所述，BIC 和传统 AI 都从神经科学中汲取了灵感，前者由于其脉冲神经动力学而更具生物学可行性。传统 AI 近年来取得了令人难以置信的进步，拥有多种实际应用场景。然而，这些壮举掩盖了一些严重的缺陷，例如巨大的能耗和较差的鲁棒性，导致传统 AI 研究发展陷入难以逾越的瓶颈期。幸运的是，这些限制很容易被人脑克服，这为机器智能的发展指明了一个有希望的方向。由于现有的 BIC 算法只能模拟生物神经元的体细胞动力学，因此，如果将更多的神经科学机制有机地融入 BIC 算法中以增强其生物可行性，那么传统 AI 中的这些瓶颈有望得到克服。例如，对脉冲神经元进行建模可以从生物神经元的动态中受益匪浅。BIC 系统可能更像人类大脑，可以更精确地建模细粒度神经元结构（如突触、树突和轴突）的动态。注意力过程的使用是另一个这样的例子，它帮助人类专注于关键信息。通过在 BIC 算法中实现注意力过程，任务性能得到显著提高，能量成本显著降低。这是因为注意力抑制了 SNN 中噪声神经元的脉冲发射，这与神经科学中的观察结果一致。此外，长短期记忆机制对于时间序列应用至关重要。缓慢的超极化后脉冲神经元可以轻松执行此功能，而传统 AI 则很难做到这一点。简而言之，生物合理性为通过BIC实现通用AI提供了无限的可能性。

除了克服传统人工智能的瓶颈之外，生物可信学习算法还可以加深我们对大脑的理解。反向传播是一种被广泛认为是传统人工智能基石的学习算法，但它不太可能成为人类大脑的学习方式，因为它对前向和后向路径中的权重对称性有严格的限制，并且需要全局教学信号和精确的梯度计算。为了解决权重对称问题，提出了反馈对齐，将反馈通路中的权重设置为随机矩阵。反馈对齐在浅层网络和小数据集上的成功表明，BP 的权重对称性要求可以放宽，生物大脑有可能以类似 BP 的方式分配责任，但约束较少。为了解决基于全局梯度的信号问题，Lillicrap 等人在一篇综述中提出了通过活动差异 (NGRAD) 进行神经梯度表示的方法。在这个框架中，权重更新由局部产生的神经活动之间的差异驱动。例如，差异目标传播使用神经活动和目标活动之间的差距以分层方式优化突触权重。目标活动是使用后续层的目标生成的，确保局部性，同时不引入精确的梯度计算。另一项工作构建了具有减少的隔室神经元的微电路，并通过顶端隔室将误差信号传回 [53]。根据不同隔室电位之间的预测误差更新突触权重 [51]。此外，第二条研究路线转向对比预测编码以寻求帮助 [281]，旨在以局部自监督的方式学习稳健的表示。贪婪 InfoMax (GIM) 通过使用模块本地 InfoNCE 损失优化每个编码器来贪婪地最大化其连续输出之间的互信息 [282]。 CLAPP 进一步将这一想法与突触可塑性相结合，使用预测树突输入和广播调制因子将对比预测编码带入生物环境 [230]。这些工作提供了关于大脑如何解决信用分配问题而无需计算梯度并将其沿整个网络向后传输的新观点。值得注意的是，Hinton 最近提出了前向-前向学习算法，旨在彻底改变神经网络的学习 [283]。前向-前向算法是一种贪婪的多层学习框架，灵感来自玻尔兹曼机和噪声对比估计。通过分层训练网络以最大化正数据的“优度”并最小化负数据的“优度”，即使在不知道前向传递的精确细节时也可以使用该方法。该算法具有局部性和灵活性的优势，是一种很有前途的皮层学习模型。我们相信，BIC 系统可以被视为大脑的精简模型，而生物合理的学习规则有可能暗示大脑如何学习和工作。

2）有效性：对BIC的期望是有效、高效地实现机器智能。人们最关心的是BIC或AI在各种现实应用中的有效性。传统的AI系统，如Deepmind的AlphaGo [271]和AlphaZero [284]，在复杂的策略游戏中取得了惊人的胜利，击败了最优秀的人类玩家，成为传奇事件。从那时起，AI领域受到了前所未有的关注，其丰富的应用场景和卓越的任务性能不断更新。相比之下，BIC由于缺乏出色的任务性能和令人信服的应用场景，受到的关注要少得多。有很多工作专注于如何使BIC算法更有效[16]。借鉴神经科学的研究是主线之一，例如对脉冲神经元的建模[70]，[285]和受突触可塑性启发的学习[286]，[287]。另一条重要路径是从传统人工智能的发展中汲取养分，包括设计网络规模不断扩大的SNN[63]，[64]，[65]，以及基于成熟BP算法进行更有效的训练[76]，[246]。这些领域的发展推动了BIC的显著进步。一些最新的结果表明，BIC在一些时间分类任务上的表现能够超越传统人工智能[288]。

3）能源功耗：能源效率是 BIC 系统的突出优势，这源于 SNN 的事件驱动计算和稀疏脉冲活动。使用基于脉冲的编码，突触 ac 操作比传统 AI 中固有的 MAC 操作便宜约五倍 [289]。另一方面，产生的脉冲越少，执行 ac 操作的次数就越少。因此，稀疏触发机制是实现能源优势的关键 [277]。用更少的脉冲实现性能提升很有价值且具有挑战性，但脉冲活动与性能之间的关系很复杂，尚未得到充分探索。一些已知的影响因素包括脉冲神经元的建模、网络结构和规模、数据集大小、训练技术、惩罚函数等。例如，结合可学习的膜时间常数 [70]、融合长短期记忆单元 [277] 或在神经元水平上纠正膜电位分布 [290] 可以在调节脉冲活动和任务准确性方面发挥作用。参数正则化，例如活动正则化或网络压缩 [247]、[291]、[292]，也是一种常用的方法，它以任务性能为代价来降低脉冲活动。另一种值得注意的方法是数据依赖处理，它根据输入调整脉冲响应，例如直接屏蔽时间维度上的不重要信息 [69]，以减少脉冲同时保持任务性能。目前的实践表明，BIC 在并发处理的有效性和效率方面具有巨大潜力，但这需要在理论和算法层面不断努力。

4）可训练性：可训练性是 BIC 发展的主要瓶颈之一。由于脉冲神经元的复杂时间动态和脉冲活动的不可微性，SNN 的训练具有挑战性 [183]​​、[246]。因此，SNN 模型的性能和规模在很长一段时间内受到限制。然而，网络规模已被证明是提高深度学习模型性能和可扩展性的关键因素。如上所述，目前 SNN 的三种主要训练方法是：基于转换的学习，首先训练 ANN，然后将其转换为相应的 SNN 对应物 [239]、[293]；基于错误驱动的脉冲直接训练，使用替代梯度进行错误 BP [76]、[246]；以及面向神经科学的局部学习规则，当触发异步脉冲时，这些规则会更新突触前和突触后神经元之间的权重 [54], [229]。

近年来，许多先进的学习算法应运而生，推动了 BIC 的快速发展。例如，最先进的基于转换的方法可以在实现最佳性能的同时大大减少模拟步骤 [293]，直接训练方法将 SNN 规模扩展到 100 层以上 [64], [65]，局部和全局学习的混合优势在小样本学习和持续学习等各种任务中非常突出 [286]。

另一方面，在开发更具生理学合理性的细粒度脉冲神经元模型时，可训练性是需要克服的重大挑战之一。最著名的 LIF 神经元已经是生物神经元复杂动力学与简化数学形式之间的折衷，如图5 和 6 所示。然而，最近有报道称，基于 LIF 的 SNN 的训练算法正在逐渐成熟和稳定。仍然不可能有效地训练具有更复杂的神经元模型（例如 HH 神经元模型或多室模型）的网络。当使用具有更丰富的神经元动态的网络来解决当前 AI 模型无法完成或做得不好的任务时，可训练性成为一个需要解决的主要问题。简而言之，迫切需要解决硬件成本、时间投入和训练复杂性问题，这些问题仍然明显高于传统 AI[66]，[112]。


# 4. 硬件平台
如图 3 和第二节所述，在硬件层面，BIC 可以被视为神经形态计算。为了更好地与文献保持一致，在本节中，我们不区分“类脑”和“神经形态”。我们知道硬件平台为类脑系统提供计算能力，而类脑系统在 BIC 生态中起着至关重要的作用。高效的硬件平台可以显著加速 BIC 模型的评估，从而促进新模型及其在现实中的应用的探索。

请注意，BIC 硬件平台包含神经形态传感器和神经形态芯片。在本节中，我们首先讨论典型的神经形态传感器，然后概述并对神经形态芯片进行比较。通常，神经形态传感器用于实现涉及视网膜、耳蜗和皮肤的生物感觉器官的部分感知功能（见图 11）。由于神经形态传感器的开发主要集中在视野领域，因此我们主要关注以下两种典型的神经形态相机的讨论。
![[Pasted image 20241015110655.png]]

## A. 神经形态相机
具有脉冲输出的神经形态相机使用时空 1 位点来编码光强度。一般来说，这些仿生相机可以分为两类，包括动态视觉传感器 (DVS) 和基于时间的图像传感器。如图 12 所示，我们展示了这两种类型的神经形态相机（例如 DVS 和 Vidar）的视觉采样机制。我们还通过将异步视觉流映射到事件图像和脉冲图像中来展示代表性示例。
![[Pasted image 20241015110944.png]]

1）动态视觉传感器：DVS，即事件相机，对场景动态敏感，以微秒时间分辨率直接响应光变化。实际上，动态感知原理可以看作是光强度的差分采样。因此，这些硅视网膜的工作方式与传统相机完全不同，DVS获取的是异步事件流，而不是提供结构化帧序列。一些具有代表性的事件相机包括 DVS128 [160]、DAVIS346 [149]、Prophesee Gen4 [295]、OV60B10 [296]、Tianmouc [297]、ATIS [298] 等。由于具有高时间分辨率（us）、高动态范围（HDR）、冗余度小、延迟低、功耗低等优点，事件相机在计算机科学和神经科学界越来越受到关注。目前，事件相机是神经形态相机在研究和工业应用中的主流，因为数据集、代码和软件工具的开源创造了良好的生态环境。

2）基于时间的图像传感器：后一种类型的基于时间的图像传感器 [301] 采用积分采样模型，当像素的光子积累达到预定阈值时，会产生一个尖峰。这些仿生相机通常将光强度编码为每个像素的瞬时频率或尖峰间隔。这种无帧成像范式带来了使用尖峰频率或尖峰间隔重建精细图像的能力。这些典型的视觉传感器包括 ATIS [298]、章鱼视网膜 [299]、Vidar Gen1 [294] 等。例如，Vidar Gen1 具有 40 000 Hz 的高时间采样频率，适合处理高速视觉任务。

总体而言，自 2008 年 iniVation 公司开发出第一台商用事件相机（即 DVS128 [160]）以来，一系列神经形态相机已与更先进的 CMOS 技术融合。如表 3 所示，我们将一些具有代表性的神经形态相机与特定的性能参数进行了比较。从表 3 可以看出，在减小芯片尺寸和像素尺寸的同时，空间分辨率、传输带宽和填充因子的趋势很明显。为了实现时空事件的异步读出，大多数神经形态相机的传输协议都采用了 AER [302]。为了获取静态信息，一些事件相机（例如 DAVIS346、ATIS 和 OV60B10）采用了灰度读出。基于时间的图像传感器（例如章鱼视网膜和 Vidar Gen1）可以通过尖峰频率或尖峰间隔重建精细纹理 [303]，但它们无法直接从读出电路获取动态信息。

神经形态相机可以作为很好的例子，这表明 BIC 系统为计算目的而进行的简化可能会给它们带来优势，并使它们能够超越生物系统。每个像素内的电路都基于 CMOS 技术，与传统相机中的电路没有实质性区别。虽然这些电路可以部分模拟视网膜中的信号处理，但它们无法复制生物神经元复杂的非线性动力学。此外，视网膜包含以复杂方式互连的多种类型的神经元 [304]，但大多数现有的神经形态相机为了更简单的实现而忽略了这种连接特性。尽管如此，这些简化被证明是有益的。由于光强度采样和神经元间布线的轻量级实现，电子设备在处理速度方面相对于生物系统的优势可以得到充分利用。现代集成电路可以处理比生物神经元脉冲间隔小六个数量级的通信周期，因此有可能超越生物视觉的速度限制。以 Vidar 为例 [294]；它的光电转换过程仅需 10 纳秒，而人类视网膜的光电转换过程则发生在微秒或毫秒的尺度上 [305]。这表明，BIC 系统的简化有时可以使它们的表现优于生物系统。

## B. 神经芯片
1）与深度学习芯片的区别：深度学习芯片与传统的通用处理器相比，专注于对ANN的执行优化，以获得更好的PPA。深度学习芯片的研究人员通常来自计算机体系结构社区，该社区受到CPU / GPU处理器设计的严重影响。因此，深度学习芯片的架构通常是冯·诺依曼架构的变体，具有精细的处理单元和相应的内存层次结构[306]。受益于ANN中卷积运算的丰富数据重用模式，在处理单元中探索了处理元素（PE）之间具有高数据重用率的各种数据流架构。内存层次结构与CPU / GPU的内存层次结构类似，包括不同级别的片外DRAM和片上缓冲器。深度学习芯片的早期设计主要考虑单芯片，很少考虑芯片间链接。由于近年来对高性能服务器和数据中心的需求不断增加，这种情况逐渐改变。

![[Pasted image 20241015113341.png]]

与深度学习芯片不同的是，如图 13 所示，BIC 芯片从一开始就以模拟受大脑启发的 SNN 为目标。在大脑皮层中，计算和内存是集成在一起的，而不是像冯·诺依曼架构那样明确分开。受此​​启发，大多数 BIC 芯片采用非冯·诺依曼分散式多核架构，其中每个核心都有紧密耦合的本地计算和内存资源。BIC 芯片无需访问片外内存即可提供大规模计算并行性和高内存局部性。此外，由于不可能将人脑的所有神经元和突触都放在一个芯片上，因此 BIC 芯片的设计在芯片间通信方面考虑了很多，以追求强大的可扩展性：将一个核心扩展到一个芯片、一个电路板、一个服务器，最后扩展到一个大脑模拟器[307]。深度学习加速器的设计更多地考虑提高性能，而 BIC 芯片更注重效率。下面，我们从功能、架构和实现角度分别回顾现有的 BIC 硬件。本文中的 BIC 硬件分类概述如图 14 所示，典型 BIC 硬件的更详细特征如图 4 所示。

2）从功能角度：芯片的功能是由它的指令集和他所支持的模型。早期BIC硬件支持的主要模型是SNN，然而一些现代的BIC 芯片扩展到ANN模型。此外，学习能力也是一个重要的功能。
	a）支持SNN：BIC 芯片，也被称为神经形态芯片。早期神经硬件停留在小尺度电路水平，知道经典的功能性芯片出来，包括斯坦福大学的Neurogrid [95]、海德堡大学的 BrainScaleS [89]、曼彻斯特大学的SpiNNaker[82]、IBM 的 TrueNorth [14]、苏黎世联邦理工学院的 ROLLS [90] 和 DYNAPs [96]、浙江大学的 Darwin [85]、英特尔的 Loihi [42]、ODIN [91] 和鲁汶天主教大学的 MorphIC [92]。这些硬件平台的主要目标模型都是类脑SNN。
	由于事件驱动计算的性质和 SNN 的稀疏活动，神经形态芯片的计算电路通常表现出较低的切换速率和较小的内存访问带宽，因此比传统处理器消耗的功率低得多。这种优势可以通过时钟门控等技术进一步增强。如果芯片仅通过跳过零来处理经过验证的尖峰事件，也可以改善延迟 [102]。
	b）支持spiking和ANN：支持 SNN 实现强大而高效的大脑智能的最终目标很酷，这也是 BIC 社区的最初动机。不幸的是，如今人们对大脑的了解仍然非常有限，即使研究人员在不同领域取得了许多突破。将更多的大脑特征纳入 SNN 的建模中，使其更加强大，然后加强其与现实世界任务的联系，对于 BIC 来说当然非常重要。然而，这并不能掩盖目前的困境，即与 ANN 相比，SNN 尚未在执行效率之外表现出更优异的结果。虽然目前在面向深度学习的基准上对 SNN 和 ANN 的比较有点不公平 [254]，但从应用角度对 SNN 的批评确实在持续 [42]。为此，一些研究小组提出开发结合 SNN 和 ANN 的跨范式平台。清华大学的天机芯片 [87]、[88] 是第一款发明混合模型和架构的 BIC 芯片，可以支持 SNN、ANN 和混合神经网络 (HNN) [309]。这为当前阶段通过融合 SNN 和 ANN 的优势将 BIC 系统推向更高的智能水平铺平了道路。例如，HNN 能够从 ANN 获得高精度，并从 SNN 获得丰富的动态、高效率和高鲁棒性 [72]、[254]、[310]。天机团队在跨范式比较 [72]、[254]、神经建模 [309]、学习算法 [286]、硬件平台 [87]、[88] 和应用程序 [311]、[312] 等方面进行了大量的研究，为这种混合 BIC 路线构建了生态。最近，混合思想被最新一代 BIC 芯片广泛借鉴，如 BrainScale 2 [84]、SpiNNaker 2 [97]、Loihi 2 [108] 以及 [308] 和 [313] 中的芯片。
	c）支持学习规则：学习能力是大脑的重要功能。大多数 BIC 芯片仅支持神经网络的推理阶段，而学习阶段必须提前在 GPU 上完成。然而，GPU 架构是为通用或面向 ANN 的工作负载量身定制的，而不是 SNN 主导的神经形态工作负载。在 GPU 上学习 SNN 效率低下且难以优化 [93]。为了解决这一挑战，一些工作设计了可以支持学习规则的 BIC 芯片。例如，ROLLS、ODIN 和 MorphIC 支持脉冲驱动的突触可塑性 (SDSP) 规则[90]、[91]、[92]，Loihi 为 STDP 规则添加了学习模块[42]，FlexLearn 进一步扩展到更广泛的支持突触可塑性规则范围[101]。在 SpiNNaker 和 BrainScaleS 中，STDP 学习已经通过时间戳记录和学习电路得到证明[89]、[94]。此外，在新一代芯片中，借助嵌入式可编程单元 [84]、[97]，可以实现更灵活的学习规则。最近，BPTT 学习已应用于 SNN，并且与生物可信的突触可塑性规则相比，其准确性要高得多 [68]、[76]。H2Learn [103] 和 SATA [104] 等几项工作为 SNN 的 BPTT 学习设计了特定的架构。未来，学习规则的结合对于 BIC 芯片探索大型复杂的神经形态模型将变得越来越重要。

3）从架构角度：有了目标功能，设计一个计算架构来实现该功能是下一个重要步骤。架构的抽象可以在图 15 中找到。在 BIC 芯片系列中，典型的设计采用分散式多核架构，每个核心都有近内存计算或内存计算。最近，ANN 加速器的设计也已适应执行 SNN 工作负载。
![[Pasted image 20241015185947.png]]
	a）近内存计算：在粗粒度上，BIC芯片经典的去中心化多核架构（无片外内存）将计算和内存很好地融合在一起，可以看作是一种非冯·诺依曼架构；而在细粒度上，每个核内计算和内存资源的组织方式可以有所不同，如果PE和片上内存在物理上是分离的，这种架构可以称为近内存计算架构，这里的“近”是指一个核内内存和计算虽然是分离的，但距离很近。
	早期的神经形态芯片大多可以归类为近内存计算架构。例如，TrueNorth、Tianjic、Darwin、Loihi、ODIN 和 MorphIC 都在每个核心中采用独立但紧密的内存和计算。对于 SpiNNaker，片外内存紧密地封装在每个计算芯片上，与常见的冯·诺依曼架构相比，这也可以视为近内存计算。最近，研究人员借用了 BIC 领域的类似架构来设计 ANN 加速器。例如，Graphcore 的新兴 IPU [314]、Habana 的 Goya 和 Gaudi[315]、阿里巴巴的 Hanguang [316]、NVIDIA 的 Simba [317] 和 IBM 的 NorthPole [318] 都将分散的多核架构应用于其 ANN 加速器以实现高吞吐量。这些芯片中各个核心的组织也属于近内存计算架构家族。这意味着不同领域的研究人员在现代智能芯片的设计中正在互相学习。
	b）内存内计算架构：除了近内存计算架构之外，每个核心中的 PE 和片上内存可以物理融合在一起，这可以称为内存计算架构。在这种架构中，突触整合的矩阵运算是在突触内存本身中执行的。在实践中，通常有两类用于计算的内存：基于传统内存或基于新兴内存。请注意，虽然我们在本调查中更多地强调执行 SNN 工作负载，但以下技术也适用于 ANN 工作负载，因为内存阵列上的矩阵运算对于各种神经网络模型本质上是通用的。
	传统存储器（如 SRAM、DRAM 和 Flash）可以重新设计以支持某些逻辑运算。例如，Liu 等人 [319] 设计了一个 6T-SRAM 单元，该单元具有由两个附加晶体管组成的传输门，用于二进制权重向量矩阵乘法 (VMM)，然后通过构建突触阵列和神经元块来实现 SNN。在 [320] 中，通过 STDP 算法的软硬件协同设计，提出了一种支持片上无监督学习的基于 SRAM 的新型 SNN 芯片。Guo 等人 [321] 使用在亚阈值域工作的嵌入式非易失性浮栅单元阵列进行模拟 VMM，该阵列由 NOR 闪存修改而来。Wu 等人[322] 进一步引入了泊松神经元，利用基于 1T-1M 阵列的 STTMRAM 中的后跳振荡进行二进制权重 VMM，从而实现 SNN 的运行。使用传统存储器的优点之一是使模拟和制造更容易，因为传统存储器的生态已经成熟。这里的新兴存储器主要指基于忆阻器的存储器设备 [323]，[324]，[325]。忆阻器的交叉开关是一个天然的 VMM 引擎 [326]，[327]，[328]，它只是神经网络中的主要工作负载。具体来说，突触权重存储在交叉开关上，与突触前输入的乘法也在同一交叉开关上计算，以这种方式融合计算和记忆。例如，Zhang 等人。 [329] 设计基于忆阻器的突触和神经元，并构建能够使用 STDP 规则进行学习的全忆阻器 SNN。受制造工艺的限制，目前基于忆阻器的神经形态芯片规模仍然较小。基于忆阻器的 BIC 硬件是一个很大的领域，具有大量的材料层次和架构设计，可以在一些已发表的调查中找到，但这不是本文的重点。
	c）ANN 加速器变体：与借鉴神经形态芯片设计 ANN 加速器（例如前面提到的 IPU、Habana 和 Hanguang）的理念类似，神经形态芯片的设计也可以借鉴 ANN 加速器的架构理念。具体而言，PE 之间具有高数据重用性的数据流架构主要用于 ANN 中的高精度矩阵乘法。修改 SNN 的 PE 阵列的一种简单方法是用选择器替换乘法器，如果输入脉冲作为选择信号有效，则选择器传递突触权重进行累积，否则传递零权重 [330]。
	基于上述与 Eyeriss 脉冲版本类似的设计，SpinalFlow [102] 进一步重组了每层的输入脉冲，并通过仅将非零输入发送到 PE 阵列来跳过无效脉冲（即零输入）。由于 SNN 的脉冲活动非常稀疏，这种架构可以实现显着的加速。SATA [104] 将乘法器和选择器组合在 PE 阵列中，以允许处理高精度矩阵运算和脉冲矩阵运算，从而支持前向和后向传递中的矩阵运算。通过为矩阵运算以外的其他运算量身定制的矢量单元，SATA 最终能够为 SNN 执行 BPTT 学习。与 SATA 不同的是，H2Learn [103] 设计了一个基于查找表 (LUT) 的引擎，用于前向传递中的脉冲矩阵运算，并设计了一个 PE 阵列，用于后向传递中的高精度稀疏矩阵运算，从而也为 SNN 实现了 BPTT 学习。这些用于 SNN 的 ANN 加速器变体再次反映了深度学习和神经形态计算之间的相互学习趋势。

4）从实现的角度：实现架构设计并最终得到成品芯片是实现实际应用的关键步骤。从实现角度来看，权衡变得更加复杂，因为需要综合考虑许多因素，包括应用场景、PPA 和可编程性。在本节中，我们粗略地讨论两类：小规模芯片和大规模芯片。为简单起见，我们排除了没有流片的设计。
	a）小规模芯片：对于边缘应用，低功耗和实时响应通常是BIC芯片的两大刚性需求，相比冗余的大规模设计，小规模设计更受青睐。总体而言，小规模BIC芯片的制造成本可以较低，无需考虑规模化和完整的生态，从而更容易探索新兴技术。
	例如，ROLLS [90] 利用晶体管的亚阈值模拟信号自然地模拟脉冲神经元的动态，这实际上是神经形态工程 [307] 的根源。该芯片只有 256 个神经元、256×256 个短期突触和 256×256 个长期突触。尽管模拟电路具有响应速度快、功耗低的特点，但它们不够稳定，无法长距离传输信号。因此，模拟神经元通常与数字路由结构配合使用，以实现神经元之间的通信，形成模拟-数字-混合芯片。由于模拟电路难以编程且对工艺、电压和温度 (PVT) 变化敏感，因此它们并不是现代大规模 BIC 芯片的主流，尽管在一些早期设计中很受欢迎。
	DYNAPs [96] 遵循模拟-数字-混合解决方案，同时进一步探索具有三个级别的分层路由网络：L0（内核内）、L1（内核间）和 L2（芯片间）。芯片内通信采用多播技术，内核间通信采用树形拓扑结构，芯片间通信采用网格拓扑结构。这样，树形通信的低延迟和网格通信的高带宽结合在一起。DYNAPs 中有四个内核，每个内核有 256 个神经元。ODIN [91] 是 ROLLS 的全数字实现，具有相同数量的神经元。MorphIC [92] 是 ODIN 的增强版，有四个内核，每个内核有 512 个神经元。 MorphIC 的路由网络与 DYNAP 的路由网络几乎相同，但它还展示了可通过随机 SDSP 学习的 1 位突触权重。此外，DYNAP 已被数字化、放大，然后由 SynSense [45] 将其产品化为 DYNAP-CNN 和 DYNAP-SE，用于具有快速响应（几毫秒）和超低功耗（几毫瓦）的 AlwaysON 设备。此外，神经形态芯片可以与事件相机集成，以构建传感-记忆-计算集成解决方案，例如同样由 SynSense 开发的 Speck [331]。
	与常用的具有全局时钟的同步数字电路相比，异步数字电路被认为更接近大脑的运行模式。因此，近年来，许多用于边缘计算的异步SNN芯片应运而生。例如，[332]、[333]和[334]为各种边缘场景设计了支持片上学习的异步SNN芯片，都表现出很高的能效。张等人[335]介绍了一种异步神经形态硬件架构搜索(ANAS)方法以及一个可配置的异步神经形态硬件模拟器，以优化异步神经形态芯片的数值和非数值设计空间。尽管能效较高，但缺乏成熟的设计方法和EDA工具阻碍了异步数字电路的广泛应用，这可能是未来需要解决的关键问题。
	b）大规模芯片：制造大规模 BIC 芯片的主要动机是构建一个大脑模拟器，用于探索大脑等通用人工智能。因此，大规模 BIC 芯片的设计原则与小规模 BIC 芯片有很大不同。对于大规模 BIC 芯片，必须考虑几个原则 [307]。
		1）可扩展性：芯片可以扩展到主板、服务器，最后扩展到大脑模拟器，这需要灵活、高速的路由基础设施。
		2）可编程性：用户无需太多硬件知识即可轻松对芯片进行编程，这需要易于使用的编程框架和高效的编译器。
		3）可靠性：芯片必须能够可靠地生成计算结果，这需要稳定的电路和信号。
		4）兼容性：芯片应与主流通用处理器兼容，以构建实用的智能系统，这需要标准的通信接口。
	Neurogrid [95] 和 BrainScaleS [89] 是两个早期的 BIC 平台，它们使用模拟数字混合电路。Neurogrid 采用上述晶体管的亚阈值信号来模拟神经动力学，而 BrainScaleS 采用超阈值信号，通过更大的电流实现显著加速。新一代 BrainScaleS，即 BrainScaleS 2 [84]，进一步整合了两个可塑性处理单元 (PPU)，以允许更多的神经模型和学习规则。
	如上所述，模拟电路难以编程且对 PVT 变化敏感，最近的 BIC 芯片大多采用全数字电路实现。SpiNNaker [82] 是一个数字神经形态平台，是欧洲 BIC 的主要平台之一（另一个是 BrainScaleS）。每个 SpiNNaker 板有 48 个芯片多处理器 (CMP)，每个 CMP 有 18 个 ARM 内核和一个片外 SDRAM。新一代 SpiNNaker，即 SpiNNaker 2 [97]，进一步整合了用于指数、对数或随机函数等常见函数的数值加速器，以及用于矩阵运算的乘法累加数组。通过混合 ARM 内核和加速器，SpiNNaker 2 可以同时支持 SNN 和 ANN。与 SpiNNaker 使用通用处理器不同，TrueNorth [14] 是第一款用于大规模 SNN 的全数字 ASIC 芯片。通过利用 SNN 的事件驱动处理和稀疏活动，TrueNorth 采用异步电路设计来实现低功耗（每芯片数十毫瓦）。遵循 TrueNorth 的数字 ASIC 解决方案，Darwin [85] 缩小了每个芯片中的脉冲神经元数量，直到其用于大规模网络的第二代问世。通过类似的芯片级架构，Loihi [42] 进一步添加了特定电路来实现上述几种突触可塑性规则，例如 STDP，而 Tianjic [87], [88] 通过最大限度地重用资源，为 ANN 添加了一条额外的数据路径，同时将面积成本降至最低。在 Tianjic 的指导下，Kuang 等人[308]在 LIF 神经元框架内统一 ANN 和 SNN 范式，并展示混合神经形态芯片。
	近年来，越来越明显的是，神经形态芯片，甚至包括机器学习芯片在工业化过程中遇到的困难不仅仅是学术论文所强调的性能结果，还在于全栈可用性。为了提高可编程性，许多团队开发了软件工具链来简化高级模型设计和硬件上的低级部署，例如 BrainScaleS 的 BrainScaleS OS [336]、[337]，SpiNNaker 的 SpiNNTools [338]，TrueNorth 的 Corelet [339] 和 Compass [340]，Darwin 的 Darwin-S [341]，以及 Loihi 的 Lava [108]。

5）挑战和趋势：尽管BIC硬件平台众多，但如前所述，它们在实践中确实面临着一些挑战，并因此呈现出一些有趣的趋势。首先，为了打破单一领域内的内在限制，ANN加速器和BIC芯片的设计开始相互借鉴功能和架构理念。例如，一些ANN加速器利用了BIC社区的分散式多核架构，而一些BIC芯片则反向采用ANN加速器等架构。其次，一些新兴的BIC芯片开始集成传感器，形成将传感、内存和计算紧密结合在一起的解决方案。第三，除了高生物合理性和低功耗优势外，BIC芯片尚未证明其在解决智能任务方面的有效性，其性能优于GPU和ANN加速器。这一挑战也是整个BIC社区面临的共同问题。最后，与早期专注于芯片设计本身的BIC芯片不同，最近的BIC芯片更加注重BIC系统、软件工具链和应用程序池。这反映了BIC社区已经开始关心用户体验，并试图使系统更易于构建、编程和部署，这一点在全球学术界和工业界的几个大团队的新芯片中得到了特别的证明。

# 5. 软件工具
当前的脑启发软件可根据其用途和基础设施分为三类：神经形态工具链、算法编程平台和脑网络模拟器。接下来，我们将分别回顾每个类别中现有的软件工具，并总结出挑战和趋势。表5中显示了典型软件工具的概述。
![[Pasted image 20241016155050.png]]

## A. 神经形态芯片工具链
近年来，神经形态芯片的兴起得益于 BIC 的效率，例如 TrueNorth [41] 和 Loihi [42]。神经形态工具链旨在促进高级模型设计，并将程序编译为由硬件支持的计算原语描述的低级可执行代码。如图 16 所示，神经形态芯片的编译通常包括四个步骤 [312]。
- 首先，将神经网络模型转换成计算图。
- 其次，将计算图转换为原始图。
- 然后，映射器将原始图转换为映射图。
- 最后，代码生成器生成要在芯片上执行的低级代码。
![[Pasted image 20241016155319.png]]
CPU 和 GPU 的工具链已经发展了几十年，已经相当成熟。然而，神经形态芯片的设计工具链面临着额外的挑战。神经形态芯片相对新兴，具有去中心化和 in-memory 计算的特点。因此，它们需要分布式存储和核心/芯片间数据流，使编程过程复杂化。我们回顾了一些典型的神经形态芯片的工具链如下。

HICANN [89] 使用 PyNN [123] 作为 Python 接口来指定模型架构、输出评估和在硬件上部署的实验形式。由于生物网络具有基于图形的表示，因此配置数据的计算类似于将使用 HDL 编写的设计部署到现场可编程门阵列 (FPGA) 上的过程。BrainScaleS [89] 软件试图通过 PyNN [123] 提供统一的实验电子规范，并将生物设计转换为硬件原语。BrainScaleS2 [84] 使用经过修改的 GCC 编译器作为工具链来支持矢量指令。基于此编译器，硬件与 C++ 标准库兼容。因此，硬件抽象库 (HAL) 可在芯片和主机系统上运行。Neurogrid [95] 的软件有一个用于交互式可视化的 GUI 界面和一个用于将模拟网络转换为硬件空间的 HAL。

Darwin [85] 提供了一个分层、模块化的操作系统 Darwin-S，具有自己的模型定义语言。应用程序 IDE 包括模型开发工具包和调试工具，方便用户使用。TrueNorth [41] 团队开发了一种用于模型定义的本机 Corelet 语言以及编程环境和用于模拟芯片架构的 Compass 软件。它还支持用于开发应用程序的各种可组合算法。Loihi [42] 的工具链由一个 Python API 组成，用于设计 SNN 拓扑和自定义学习规则。编译器和运行时库用于将模型转换为在芯片上执行的原语。Loihi 2 [108] 是 Loihi 的第二代。它提供了一个与硬件无关的开源软件 Lava。Lava 不针对特定芯片，据称可以在传统和神经形态处理器上移植。

Braindrop [106] 提供了一种自动合成软件，用于将抽象的计算转换为芯片上的可执行代码。该软件包括与 Nengo [342] 软件的前端交互和与驱动程序软件的后端接口。SpiNNaker [107] 提供了一种基于 PyNN 描述或其他格式的配置软件 SpiNNTools。它采用神经电路的定义并生成路由信息。神经网络和节点拓扑被视为一个图，可以通过软件配置，然后上传到节点网格。Tianjic [87], [88] 是第一个尝试将 ANN 和 SNN 集成到具有多核架构的混合神经形态芯片中的公司。他们开发了一个软件框架，用于统一描述和转换各种神经网络。

最近的神经形态芯片也提供了它们的工具链。为 NeuRRAM 芯片 [343] 开发的软件工具链可以快速实现各种机器学习应用程序。它提供了不同级别的基于 Python 的 API：用于每个芯片模块基本操作的低级 API、用于神经网络层所需基本操作的中级 API 以及用于完整实现神经网络层的高级 API。这种设计允许软件开发人员（即使是那些不熟悉 NeuRRAM 芯片的开发人员）在其上部署他们的机器学习模型，从而促进一系列机器学习应用程序。NorthPole [318] 的工具链自动在专用硬件阵列上协调计算、内存和通信任务，以确保最佳利用率并防止资源冲突。它以空间和时间效率战略性地管理神经网络层处理，使现有应用程序能够无缝过渡到这种先进的架构。

所述软件自动协调专用硬件阵列上的计算、内存和通信任务，以确保最佳利用率并防止资源冲突。它以空间和时间效率战略性地管理神经网络层处理，使现有应用程序能够无缝过渡到这种先进的架构。

总而言之，当前的神经形态工具链通常是同时设计的，通常包含大量代码。它们通常与硬件配置高度耦合，因此很难快速迁移到其他芯片，阻碍了在不同硬件平台上快速评估算法。通用工具链的开发对于加速神经形态系统的发展至关重要。Loihi 2 的 Lava 软件代表了朝这个方向迈出的积极一步，但该领域预计会进一步成熟和繁荣。

## B. 算法编程平台
算法编程平台旨在简化 SNN 的实施。这些平台通常建立在传统的通用计算硬件之上，经过量身定制，可与流行的深度学习框架兼容，充分利用计算机科学的大量进步。

目前，流行的深度学习框架（如 PyTorch 和 TensorFlow）主要面向 ANN，缺乏针对 SNN 独特属性的优化。在开发 BIC 算法编程平台时，必须考虑几个关键因素。首先，它们应该支持各种神经元模型和神经形态数据集。其次，它们应该充分利用 SNN 的特性来加速在 GPU 上执行。最后，需要有在神经形态芯片上部署的接口。

到目前为止，已经出现了许多以 SNN 为重点的算法编程平台。SpykeTorch [109] 和 SINABS 瞄准卷积 SNN。SpykeTorch [109] 旨在模拟每个神经元最多一个脉冲和排序编码方案的卷积 SNN。它可以轻松实现学习规则，并且具有通用性，能够重现各种研究的结果。同样，SINABS 实现了几个脉冲卷积层，并提供了与传统 CNN 进行比较的 API。BindsNet [110]、PySNN、 SpyTorch、 SpikingJelly、 Norse [111] 和 SNNtorch [112] 进一步扩展了对一般 SNN 的支持。BindsNet [110] 特别适合机器学习和强化学习，并具有与 OpenAI gym [113] 的接口。 PySNN 面向基于相关性的学习方法。SpyTorch 提出了一种名为 SuperSpike 的新替代梯度方法来平滑脉冲信号。SpikingJelly 整合了许多神经形态数据集，例如 CIFAR10-DVS [114]、ASL-DVS [115] 和 DVS Gesture [116]，用户可以轻松加载这些数据集。Norse [111] 尝试引入 SNN 的稀疏和事件驱动特性，并支持许多典型的神经元模型。SNNtorch [112] 建立了一些更具生物学合理性的 BP 在线变体。

这些平台继承了 PyTorch 的用户友好性、模块化和动态特性，因此支持 CPU 或 GPU 部署。因此，它们能够实现高效的 SNN 模拟，并具有显著的并行性。

对于与神经形态芯片的接口，SpikingJelly 支持在名为 Lynxi HP300. 的神经形态芯片上部署。结合 CTXCTL 软件，SINABS 还可以将模型移植到DynapCNN 芯片上 [117]。PyNCS [118] 不针对特定硬件。它通过提供前端来指定网络架构和一组用于与硬件接口的 Python API，具有模块化、可移植性和可扩展性，从而将高层设计和底层描述解耦。

算法编程平台对于神经形态算法开发的重要性已得到广泛认可。理想情况下，此类软件应该对初学者来说足够直观，但对于构​​建新模型来说又足够灵活。用户能够进行高级编程，而无需深入研究硬件级的复杂性。尽管如此，核心功能必须用低级语言实现，并且必须进行精细调整以利用特定的硬件基础设施。

综上所述，虽然现有算法编程平台的举措奠定了良好的基础，但仍有很大的进步空间。未来的努力可能会集中在拓宽深度学习框架上，以进一步增强功能并优化性能。

## C. 脑网络模拟器
脑网络模拟器旨在模拟生物神经网络，支持多种神经活动和突触模型。它们主要基于 SNN，SNN 被认为比 ANN 更符合生物原理且更节能。模拟可以在从单个分子和隔室模型到整个大脑系统的各种规模上进行。在专用硬件尚未普及的情况下，软件模拟器对于验证硬件性能、测试修改和开发 BIC 算法至关重要。此外，它们还可用于脑部疾病分析。理想情况下，脑网络模拟器应具有以下特点。首先，它们应该支持广泛的神经元模型。其次，它们应该能够进行模块化开发并保证高效执行。第三，它们应该具有高并行性，并且应该与分布式处理器或集群兼容以支持大规模模拟。

先驱性的大脑模拟器，如通用神经模拟系统 (GENESIS) [119]、神经元 [120]、NEST [121] 和 Brian [122]，已经建立了基础框架。GENESIS [119] 是一个通用的模拟器，用于模拟真实的神经系统，例如亚细胞成分和单个神经元的复杂模型。NEURON [120] 使用部分来模拟单个神经元。传统上，用户应该手动创建隔间，而在 NEURON 中，这些部分会自动分成单独的隔间。提供了主要的脚本语言以及 Python 接口。基于 MPI 协议支持并行化，从而可以在多核处理器上部署。NEST [121] 具有针对大型网络的优化功能。它表示连续时间中的峰值，并支持网络中不同类型的神经元模型的组合。它还可以利用多处理器计算机和集群。实现基于 C++，带有名为 PyNEST 的 Python 接口。它还支持开发名为 SLI 的内置脚本语言。Brian [122] 直观高效地设计新模型，尤其是那些具有单室神经元的模型。用户可以通过编写数学方程任意定制神经元模型。因此，它特别适合用于教学。

PyNN [123] 充当各种脑网络模拟器的统一接口，通过代码重用提高效率。Brian2 [125] 使用数学方程来描述神经模型的各个方面，扩展了 Brian 的范围，并使其能够独立于计算设备。然而，它仍然因其陡峭的学习曲线而受到批评。CARLsim 6 [344] 是一个用户友好且功能强大的 GPU 优化库，旨在模拟具有复杂生物保真度的广泛 SNN 模型。它能够在传统的 x86 CPU 以及流行的 GPU 上部署具有逼真突触行为的 Izhikevich 神经元模型。CARLsim 6 提供了一个让人联想到 C/C++ 中的 PyNN 的编程接口，有助于对突触、神经元和网络层的配置和参数进行精细控制。

Auryn [126]、ANNarchy [127]、GeNN [128]、Brian2GeNN [129] 和 BSIM [130] 专注于提高模拟速度。Auryn [126] 是一个具有突触可塑性的循环 SNN 模拟器，它增加了并行性以减少延迟。ANNarchy [127] 用面向方程的数学描述定义神经元和突触模型。然后利用该定义生成 C++ 代码，以便在并行硬件上高效执行。GeNN [128] 是一个代码生成框架，旨在促进图形加速器在大型神经元网络计算模型中的使用。BSIM [130] 利用跨种群并行性充分利用 GPU 资源，利用稀疏感知负载平衡来处理活动稀疏性，并利用专门的优化来支持多个 GPU。 DeepDendrite 框架 [345] 采用了树突状分层调度 (DHS) 方法，推动了神经元模拟的发展，有效地缩小了神经元模拟与人工智能之间的差距。这种创新方法显著提高了模拟速度，从而能够广泛探索计算大脑原理，促进人工智能算法的发展。该框架具有潜在的应用前景，有望推动神经科学和人工智能领域的发展，特别是在空间模式分析和图像分类任务领域。

近年来还有一些值得一提的大型大脑模拟器。2020 年发布的 MONET 模拟器 [131] 旨在使用超级计算机进行人类规模的大脑模拟。作者提出了一种瓦片分割方法，大大降低了网络通信成本，从而保证了 MONET 的可扩展性和效率。2023 年发布了一个名为 BrainCog [346] 的大型大脑模拟器，旨在实现大脑启发智能。它支持多种尺度的各种 BIC 和大脑模拟模型以及感知、决策和推理等多种类型的认知功能。2022 年发布的 BrainPy [132] 专注于大脑动力学建模，这对于挖掘大脑功能下的神经机制是必不可少的。它提供了一个通用的编程框架，以简化和促进基于即时 (JIT) 编译的用户开发。 [134] 中的工作利用通用深度学习框架 TensorFlow 在单个 GPU 上模拟区域 V1 的生物学详细大规模模型，称为“V1 模拟器”。DeepDendrite 框架 [345] 利用 DHS 方法加速神经元模拟，弥合其与 AI 之间的差距。该方法显著提高了模拟速度，促进了大脑计算原理的探索和 AI 算法的开发。该框架在神经科学和 AI 中展示了有希望的应用，例如空间模式分析和图像分类任务。

关于脑网络模拟器在脑疾病分析中的应用，虚拟大脑（TVB）[124]是一个用于全脑模拟的多尺度神经信息学平台，它借鉴了统计物理学的方法来降低微观层面的复杂性并完成宏观层面的组织。Stefanovski等人[347]挖掘了使用TVB了解阿尔茨海默病机制的潜力。Meier等人[348]还表明，TVB有潜力提高深部脑刺激治疗阿尔茨海默病的有效性。TVB还在EBRAINS中提供云服务[349]。

一般来说，目前大多数模拟器都专注于提供易于使用的工具包，用于构建具有多功能的 BIC 模型并加速计算。使用这些软件工具有几个功能优势。首先，一些模拟器 [121]、[131] 专注于大规模大脑模拟，最终目标是实现人类水平的模拟，支持大规模网络构建、分裂和多集群超级计算机上的模拟。人类小脑（680 亿个神经元）[131]、猫小脑（10 亿个神经元）[357] 和猕猴皮层（400 万个神经元）[353] 都已取得这一成就。在相反的方向上，其中一些专注于神经元或子神经元结构的详细模拟，例如多隔室模型、受体甚至蛋白质结构，从而提供更细粒度的模拟。另一方面，一些软件工具专注于加快模拟速度，尤其是使用 GPU 加速，例如 NeuroGPU [359] 和 GeNN [128]。实时模拟（模拟可以以 0.1 或 1 毫秒的 time_step7 比生物时间执行得更快）是主要追求。目前，1 平方毫米猕猴视觉皮层内的实时模拟是可以实现的 [352]；然而，实时大规模模拟仍然具有挑战性。如表 6 所示，MONET 比人类小脑 [131] 慢 578 倍，NEST 比 1B 神经元平衡神经网络 [356] 慢 600 倍。很难实现较高的模拟速度、较大的规模和较细的模拟粒度，目前的情况如表6所示。此外，一些模拟器，如NEST8和NEURON9，提供了一个包含丰富模型的库，包括神经元模型、突触模型和网络模型以及相关的计算神经科学出版物[360]，这有助于神经科学界。与大规模人脑模拟相关，数字孪生大脑（DTB）的概念最近被引入，旨在通过模拟人脑的结构和功能来弥合生物和人工智能之间的差距[361]。它通过多尺度大脑建模帮助个性化医疗干预，例如在神经康复和神经外科手术中[362]。DTB计算平台还可以模拟个性化的生物大脑结构以及[363]中提出的整个人脑尺度。

除了上述成就，脑网络模拟器的发展仍面临诸多挑战。首先，用户需要具备一定的计算神经科学基础，这对于没有相关背景的用户来说可能是一个艰巨的任务。其次，许多模拟器的通用性不够，导致跨平台兼容性和优化不足。第三，目前很难在脑模拟和机器学习之间建立任务级联系。最后，它们被设计为在冯·诺依曼架构上执行，无法利用神经形态硬件的高效率。

## D. 挑战和趋势
BIC 是即将到来的 AI 进步浪潮的变革典范，它站在最前沿，超越了摩尔定律的传统限制。研究界见证了面向 BIC 的软件开发的显著上升，证明了其蓬勃发展的潜力。然而，仍然存在一些艰巨的挑战，表明未来的关键趋势即将出现。

首先，迫切需要创建通用软件工具。这需要将软件与硬件分离，这取决于与神经形态系统的兼容性。传统的计算系统以图灵完备性和冯·诺依曼架构为基础，拥有完善的层次结构。Zhang 等人 [364] 提出了一个名为“神经形态完备性”的概念，它减轻了对硬件完备性的必要性，并制定了一个新的系统层次结构。这包括图灵完备的软件抽象层和灵活的神经形态架构。他们的关键工作为通用软件工具的设计奠定了理论基础，促进了该领域的进一步发展。

其次，当代软件工具应进行优化，以与脑启发模型和算法的特征产生共鸣，例如稀疏脉冲和事件驱动过程。这些独特的属性给软件设计带来了额外的复杂性，包括不规则的内存访问模式和数值精度问题——这些主题需要深入调查和实验。

第三，现有的软件工具，特别是算法编程平台和脑网络模拟器，主要部署在CPU和GPU等标准硬件上。预计未来的研究将使这些工具与神经形态硬件的先天优势协同起来。深度学习的蓬勃发展很大程度上归功于GPU等高性能计算平台的出现。包括PyTorch和TensorFlow在内的深度学习编程框架已经实现了硬件的无缝集成。类似水平的复杂软件工具开发在BIC社区中备受追捧，以加速算法和应用程序的进步。开发用户友好的软件工具链对于向公众推广BIC杀手级应用起着至关重要的作用，这对于建立BIC应用生态系统也至关重要。
![[Pasted image 20241016170151.png]]


# 6. 基准数据集
在本节中，我们首先回顾神经形态数据集的现状。然后，我们从两个角度对现有的神经形态数据集进行广泛分类，然后全面概述神经形态数据集的属性。最后，我们讨论神经形态数据集的挑战和未来趋势。

## A. 目前状态
大规模数据集在深度学习时代起着主导作用 [365]，它允许使用定量基准监控进度，并为学习算法提供数据驱动的可能性。当前的神经形态数据集在任务多样性、数据大小和场景复杂性方面显然无法与计算机视觉中传统的基于帧的数据集（例如 ImageNet [366] 和 KITTI [367]）相提并论。可能有两个原因：1）神经形态传感器（例如 DVS [160]、DAS [161] 和触觉传感器 [162]）比标准传感器（例如传统相机）贵得多；2）异步时空事件 [368] 在三维中呈现稀疏点，因此，大规模手工注释的标签不像传统帧那样容易获得。然而，BIC 技术 [16]、[272] 正在蓬勃发展，以极大的能量处理信息，导致神经形态数据集的数量急剧增加。

## B.分类
1）模拟数据集与真实世界数据集：从数据集生成角度，现有的神经形态数据集大致可分为两类（即模拟数据集和真实世界数据集）。第一类是将基于帧的数据集转换为神经形态域的模拟数据集 [145]。其中的第一部分 [135]、[136]、[137]、[138]、[369] 采用基于事件的模拟器，例如 ESIM [139] 和 V2E[140]，将大规模视频数据集转换为异步时空事件。例如，Rebecq 等人 [135] 使用 ESIM [139] 模拟由 MS-COCO [141] 图像中的随机相机运动触发的动态事件。
Gehrig 等人[136] 在 CARLA [370] 中实现了一个基于事件的采样模块，该模块渲染高帧率图像并使用 ESIM [139] 将其转换为动态事件。Li 等人 [137] 利用 V2E [140] 将视频转换为动态事件以进行对象检测，并且他们直接使用 KITTI [367] 数据集中现有的大规模注释标签。Lin 等人 [138] 提出了一种全向离散梯度算法将帧转换为事件流。其中的第二部分（例如 N-MNIST [142]、N-Caltech101 [142]、N-UCF50 [143]、CIFAR10-DVS [114] 和 N-ImageNet [144]）使用事件相机在 LCD 显示器上记录来自流行的基于帧的数据集的图像。例如，CIFAR10-DVS [114] 数据集的转换是通过重复的闭环平滑移动图像来实现的。N-ImageNet [144] 数据集由 LCD 显示器前移动的事件摄像机使用可编程硬件记录。这些模拟数据集可以降低成本并推进基于事件的算法的发展。然而，这些转换策略无法捕捉高速或低光真实场景中的动态变化，而这正是事件相机所擅长的。

第二类是指通过直接记录各种现实世界对象而包含事件数据的真实世界数据集。通常，它们可以分为分类任务和回归任务[145]。第一组主要包括基于事件的对象识别数据集（例如，DVS-Gesture [116]，N-CARS [146]和ALSDVS [115]）和动作识别数据集（例如，DVS-PAF [147]和DHP19 [148]）。在第二组中，用于回归任务的神经形态数据集包括图像重建（例如，CED [149] 和 BS-ERGB [150]）、物体检测（例如，PKU-DDD17-CAR [151]、1Mpx 汽车检测 [152] 和 PKU-DAVIS-SOD [371]）、物体跟踪（例如，FED240hz [153] 和 VisEvent [154]）、深度估计（例如，MVSEC [155] 和 DESC [156]）和 SLAM（例如，UZH-FPV [158] 和 VECtor [159]）等。如表 7 所示，现有的大多数现实世界数据集用于对象识别任务，很少有用于复杂回归任务的数据集，尤其是具有像素级注释的场景分割数据集。

2）单模态和多模态：从数据集模态角度来看，神经形态数据集也可以分为单模态和多模态。第一类中的几个数据集仅包含来自单个神经形态传感器（例如，DVS [160]、DAS [161] 或触觉传感器 [162]）的异步时空事件。由于事件相机的广泛关注和快速应用，神经形态社区中的大多数数据集 [145] 都是基于事件的视觉数据集。最近还发布了几个用于语音和触觉的神经形态数据集。例如，N-TIDIGIS18 [163] 数据集是通过将 TIDIGITS 数据集中的音频文件播放到动态音频传感器（DAS，即 CochleaAMS1b）来录制的。 ST-MNIST [164] 数据集包括通过在神经形态触觉传感器 (NTS) 阵列上手写获得的大规模手写数字。第二类中的其他数据集提供来自多个神经形态传感器的混合异构传感流。例如，GRID [166] 视听唇读数据集是使用两个仿生硅多模态传感器（即 DVS [160] 和 DAS [161]）记录的。使用神经形态指尖触觉传感器和事件相机为智能节能机器人系统构建了一个基于视觉触觉事件的数据集 [165]。此外，一些研究 [151]、[153]、[154]、[155]、[156]、[158]、[159]、[167]、[372] 尝试将神经形态传感器与其他传感模式（例如，光检测和测距 (LiDAR)、RGB-D 相机、红外相机、IMU 和 GPS）集成在一起，用于具有挑战性的场景中的智能机器人。这些涉及神经形态传感器的新兴多模态数据集将刺激对稳健感知的研究（见表 8）。为了更好地可视化，我们在图 17 中从模态角度展示了三个代表性数据集。
![[Pasted image 20241016170518.png]]


## C. 数据集属性
1）稀疏事件：受生物神经系统的启发，神经形态硬件系统使用事件驱动通信（例如 AER [302]）实现神经元和突触计算操作。因此，神经形态传感器（例如 DVS [160]、DAS [161] 和触觉传感器 [162]）可以实时感知具有异步时空事件的动态变化。以 DVS 为例，事件可以描述为一个元组 $⟨x, y, t, p⟩$，包括四个组成部分：空间坐标、时间戳 $t$ 和极性 $p$。直观地讲，所有像素都会在三维空间中生成异步离散稀疏点 [368]。

2）时空特征（混合表示）：神经形态传感器在传感信息获取方面呈现出一种新的范式转变 [145]。因此，大多数现有的为帧（图像或视频）设计的深度学习技术不能直接应用于异步时空事件。这提出了一个关键问题：充分利用事件流中的时空特征以最大化给定任务的性能的最佳方法是什么？通常，需要使用核函数 [373] 将离散点转换为连续测量值，该核函数可以采用手工制作的函数或神经网络架构。根据文献，这些事件表示策略可以分为类似图像的表示、手工制作的描述符、DNN 和 SNN。早期的尝试直接将异步事件映射到类似图像的表示（例如，事件图像 [374] 和体素网格 [375]）。此外，一些有效的时空描述符（例如时间表面 [376]）是从异步事件中提取的，但它们非常耗时，并且与移动对象的类型密切相关。一些端到端学习表示（例如 EST [373] 和 Matrix-LSTM [377]）由深度学习模型生成。生物学上可解释的 SNN [246]、[378]、[379]、[380] 被用于充分利用基于时空事件的信息，并且计算功率极低。然而，在多个异构事件流的混合表示方面，很少有人进行探索。

## D. 挑战和趋势
BIC 是深度学习时代的一项新兴技术。事实上，直接比较 BIC 和深度学习两类数据集的任务多样性、数据大小和场景复杂性是不公平的。然而，一个重要的问题是研究 BIC 数据集从生物神经系统中获得的关键特性是什么。关于实际应用，BIC/神经形态数据集的一个主要趋势是构建更具挑战性的场景（例如高速或低光），以突出神经形态传感器相对于传统传感器的优势。也有人建议提供更多不同的开源神经形态数据集将为 BIC 社区开辟机会。

# 7. BIC系统框架
图 18 展示了 BIC 硬件和软件的几个里程碑。早期的研究主要集中在 BIC 算法或 BIC 芯片的研究上，而现代的研究则更加注重全栈解决方案，以增强实践中的适用性。BIC 系统的设计必须考虑一个完整的框架，包括算法、软件、硬件和潜在应用程序，如图 19 所示。BIC 硬件和软件通常充当高级算法/应用程序和低级系统之间的中间环节。BIC 芯片的设计目标是以最佳方式实现可以支持所涉及算法的指令集。这样，软件就可以专注于编程神经模型，并将模型映射到具有硬件限制的芯片指令集上，而不再需要许多硬件细节。软件和硬件的解耦在 BIC 系统框架中非常重要，因为它可以实现软件工具和硬件平台的独立和迭代开发[364]。对于大规模系统，硬件平台旨在通过快速的通信层次结构和多个芯片甚至服务器之间的高效工作负载调度策略来最大限度地发挥芯片的计算能力。调度策略可以作为操作系统的一部分集成到软件工具中。
![[Pasted image 20241016171528.png]]
![[Pasted image 20241016171643.png]]
近年来BIC系统的构建呈现出大系统和小系统两个方向，需求也有所不同（见图20）。大BIC系统，比如云端的BIC服务器，更注重高可扩展性和高吞吐量，这对于增加系统规模以部署多个大模型以及提高这些大模型的运行速度至关重要。大系统的芯片设计往往需要更强的可编程性和可靠性，软件设计需要更好的通用性。相比之下，小型BIC系统更注重高效率，比如低功耗、低延迟、体积小，通常应用于实时、低功耗的移动设备和边缘机器人。由于小型BIC系统不太关心系统扩展的复杂性以及对高可编程性和通用性的需求，我们可以在小型BIC系统的设计中看到更多关于新型学习规则、电路设计和制造工艺的探索。

上述从单一算法或芯片设计到全栈解决方案的转变将 BIC 系统的设计工作推向了复杂的项目。BIC 系统的现代设计需要不同学科之间的全面合作，包括神经科学、人工智能、计算机架构、集成电路和软件工程。这种情况增加了学术团队开发实用 BIC 系统的难度。对于大型 BIC 系统，由于扩展复杂性高和对实用性的考虑更多，挑战变得更加严峻。

大型BIC 系统的建造需要以下考虑：
1）完整的指令集，可以很大程度上避免不支持的运算符。当用户遇到系统无法执行的运算符时，他们可能会放弃产品，即使这些运算符只是少数。
2）异构 SoC 集成，可以包括核心设计之外的额外单元，例如 CPU 内核、DDR 接口、图像或视频编码器/解码器以及传感器接口，以提高实际任务中的可编程性和可用性。
3）高速通信接口，如SerDes、PCIE接口，可在芯片和板卡之间提供足够的带宽，以降低系统级延迟。
4）先进的制造工艺，可带来较高的芯片性能和良率。
5）高可靠性，这对于减少系统崩溃非常重要。
6）用户友好的软件工具，应足够通用，以便轻松部署大规模神经模型。
7）可持续的应用生态，体现了使用系统开发应用的动机，最终代表了系统的生命力。然而，解决这些挑战的成本非常高，需要大量的人力，这对于学术界来说可能是难以承受的。

未来可行的路线可能是由学术团队以创新技术开发原型系统，而产业团队进一步制作相应的产品。

# 8. 讨论和结论
本文从 BIC 基础设施的四个组成部分（模型/算法、硬件平台、软件工具和基准数据集）对 BIC 进行了系统概述，并在此基础上提出了该研究领域的一些基本相关概念。对于每个组成部分，我们总结了最近的进展、关键考虑因素和主流技术以及未来趋势。最后，介绍了 BIC 系统的框架，并指出这四个组件的协同设计是 BIC 未来发展的普遍趋势。要真正发挥 BIC 的潜力，整个领域都需要做出系统的努力。BIC 研究还有很长的路要走，我们在此讨论 BIC 的当前和长期愿景。

## A.目前阶段的未来方向
在现阶段，我们提供了几种可能需要 BIC 特性的潜在应用和场景：
1）轻量级智能：SNN 具有事件驱动计算的特点，可以避免高精度乘法运算，并受益于稀疏尖峰活动。这种轻量级计算在 BIC 系统上运行时可降低资源开销和功耗。这对于资源和电源有限的边缘智能是有益的。特别是，事件驱动的 SNN 算法与异步神经形态芯片设计相结合，可以最大限度地发挥神经形态计算中低功耗计算的优势 。
2）多任务代理：代理（例如机器人或无人机）通常执行多项任务，以处理不同的传感器信号，然后做出可靠的决策。BIC 芯片的多核分散式架构由于核心之间的计算、内存和控制分离，自然适合运行多个智能模型。这减轻了在冯·诺依曼架构上运行多个线程时的内存冲突，从而提高了多任务工作负载的执行性能。
3）复杂环境混合感知：传统摄像机采集的帧图像具有丰富的纹理信息，而 DVS 摄像机采集的尖峰事件具有 HDR 和低延迟。集成混合感知技术可以结合互补优势，在运动模糊、光线不足、恶劣天气和突发异常等复杂环境中实现高质量成像和综合决策。
4）混合计算实现灵活配置：ANN 具有高精度神经编码和密集计算的特点，而 SNN 具有二进制神经编码和稀疏计算的特点。对于不同的应用数据集，ANN 和 SNN 通常呈现出不同的精度结果和计算成本。通过调整 ANN 和 SNN 之间的资源比例，集成混合计算范式可以结合互补优势，灵活配置应用精度和计算成本。
5）仿生大型语言模型：大型语言模型 (LLM) 的出现预示着 AI 新时代的开始。最大的担忧之一是当前的 LLM 消耗大量能源。人类大脑执行的任务比 LLM 复杂得多，并且包含更大的网络（拥有 860 亿个神经元和数万亿个突触），但仅消耗 20 W 的功率。因此，BIC 系统是解决能源问题的有前途的方法。借鉴计算神经科学和计算机科学是成功的关键。神经科学长期以来一直是 AI 进步的重要驱动力，但当前的 AI 与人脑的工作方式截然不同。一个例子是当前的人工神经元过于简单。生物神经元具有丰富的树突非线性计算单元和体细胞动力学。这些复杂的动力学使单个生物神经元的表达能力远高于人工神经元。当前的 ANN 使用低复杂度人工神经元和超大网络来实现机器智能。另一种可能的生物启发方法是将高复杂度脉冲神经元和一定规模的网络结合起来。因此，通过广泛借鉴生物神经元中树突和胞体的动力学，希望能够构建一座从计算神经科学到大规模模型的桥梁。
6）算法-软件-硬件协同设计：构建强大的BIC生态系统并阐明BIC的应用优势需要整个领域的共同努力。以基于脉冲的神经形态计算为例，视觉和听觉仿生感知的神经形态范式表现出高灵敏度和低能耗的优势。得益于大脑启发的事件驱动过程和时空动力学，神经形态算法特别适合处理神经形态数据。应用-算法协同设计是解锁神经形态计算杀手级应用的基础。复杂的时空动力学使得神经形态算法的训练比传统AI更具挑战性。全面的软件工具链可以帮助研究人员降低与算法设计相关的成本。神经形态算法优势的有效实现取决于强大的神经形态硬件的支持，而完整的软件工具链可以降低算法部署的成本。因此，BIC系统的发展和完善离不开算法、软件和硬件的协同设计。

## B.探索BIC的本质
众所周知，建模和算法是 BIC 的驱动力，它倾向于通过学习生物神经系统的机制、结构和功能来构建新的模型和算法。在神经科学和物理学领域，在表征不同尺度上与信息处理相关的神经动力学方面已经取得了实质性进展。一般来说，学习有三个层次，从微观尺度（例如单神经元模型 [215]、[381]）、中观尺度（例如神经群体和回路的随机网络模型 [382]、[383]），最终到宏观尺度（例如大脑区域的平均场神经质量模型 [384]、[385] 和整个大脑网络模型 [124]、[386]、[387]），在神经动力学的跨尺度传输过程中，新的属性不断出现，而以前的属性可能消失，这可以通过应用重正化群在粗粒度流中发现 [388]、[389]、[390]。

虽然现有的 BIC 框架源于对大脑的学习，似乎与神经科学有相似的术语，但学习本身仍然局限于具有计算简化的现象学模拟。从这个意义上说，BIC 的研究仍处于对单个神经元进行建模并通过单个神经元之间的简单连接构建集群架构的早期阶段，也就是说，现有的工作主要利用单个神经模型，很少从神经回路或更高层次的大脑结构和功能中学习。来自神经连接的复杂拓扑结构和特殊动态现象可能是大脑协调身体无数功能和行为并实现人类智能的关键点。然而，BIC 研究人员仍然缺乏将它们应用于实际建模的理解。类似地，对于基于软件或 BIC 芯片的大脑模拟，现有的方案都基于还原论的信念，试图通过简单地增加建模神经元的数量或使神经元之间的迭代规则复杂化来重现整个大脑的功能[17]，[391]。虽然这种想法在工程上是实用的，但它与大脑功能不是单个神经元动态的简单总和这一事实相冲突，无论建模的单个神经元有多复杂[381]，[383]。

当然，BIC 设计没有必要精确地模拟大脑复杂的物理性质。然而，通过投资神经计算的基础研究来加速当前人工智能的进步将具有巨大的潜力。越来越多的科学家相信，这方面的研究可能会揭示智能的基本成分，并催化人工智能的下一场革命 [265]。我们认为，BIC 可以从神经科学中还原论的失效中吸取教训，开发真正的多尺度架构，以实现跨尺度神经动力学。总之，BIC 如何同时在微观、中观和宏观尺度上从大脑中学习，BIC 如何利用跨尺度神经动力学，并开发相应的理论、模型、架构和硬件系统来处理现实世界的应用，对于未来的长期研究具有巨大的潜力。
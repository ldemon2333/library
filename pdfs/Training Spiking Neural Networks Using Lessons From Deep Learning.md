# 摘要
大脑是一个理想的地方去探索更高效的神经网络。我们突触和神经元工作的原理给未来的深度学习带来了展望。我们也探索了脉冲编码和学习过程的复杂交互联系。

# 1.介绍
大脑能量效率远远高于神经网络，新一代受大脑启发的脉冲神经网络有望弥补这一差距。

## A.神经形态计算：A Quick Snapshot
类脑工程致力于模仿大脑计算原则来降低人工智能系统的功耗，为了复制生物系统，我们建立了三个部分。

1）神经形态传感器受生物传感器启发，比如视网膜或耳蜗，通常记录信号的变化，而不是定期采用。仅当发生变化，才产生信号，信号通常称为“尖峰”。Static suppression

2）学习理解尖峰意义的神经形态算法被称为SNN，SNN不使用浮点值，而是使用单位二进制激活(尖峰)，这些激活随时间而不是强度编码信息。因此，SNN有着低精度参数和高时空稀疏性优点。
Spikes and Sparsity

3）这些模型在设计时考虑到了在专用神经形态硬件上的高能效执行。稀疏激活减少了芯片内外的数据移动，以加速神经形态工作负载，与传统硬件上的相同任务相比，这可以带来巨大的功耗和延迟增益。

## B.野外神经系统
总体目标是将已经在广泛领域证明其价值的人工神经网络(ANN)与SNN的潜在效率相结合。

![[Pasted image 20241009105021.png]]

尖峰算法已用于在医疗、机器人和混合现实领域以及许多其他领域实现低功耗人工智能算法。鉴于他们的能源效率，最初的商业产品通常目标是边缘计算应用。

在生物信号监测中，脑机或生物信号接口的神经植入物必须以最小功率在本地预处理信息，并且缺乏传输云计算数据的带宽。使用SNN完成的工作包括spike sorting,生物信号异常检测和脑机接口。除了生物医学干预之外，SNN 模型还用于机器人技术，旨在使其更像人类，并降低运营成本。无人机还必须在低功耗环境中运行，才能从轻型电池中获取尽可能多的价值，并且已从使用神经形态处理器中受益。由于 SNN 为时间信号处理提供了高效的计算机制，因此音频信号可以在神经形态硬件上以亚毫瓦 (mW) 功耗和低延迟进行处理。

[49] 中回顾了大量使用 SNN 的高效计算机视觉应用。SNN 同样适用于跟踪天空中的卫星等物体，以实现空间态势感知，并且已经进行了研究以促进人工智能的可持续利用，例如监测智能建筑中的材料应变和面临电力输送挑战的偏远地区的风力预测。在 2018-19 年特柳赖德神经形态和认知研讨会上，甚至建造了一个神经形态机器人来玩桌上足球！

除了神经形态应用之外，SNN 还用于测试关于自然智能如何产生的理论，从大脑的高级学习规则和记忆的形成方式到神经元和突触层的低级动态。


# 2. 从人工神经网络到脉冲神经网络
关于神经元是如何编码的，有个3s主题，脉冲神经网络的三个优点

1）Spikes：在大部分神经元中，动作电位的出现远比其出现微小变化重要。现在技术模型将神经元发放简化为离散1bit变化。在传统神经网络芯片中，各层激活传播所存储的高精度激活值使得其代价按昂贵。高精度乘法会带来进位延迟。相反，基于脉冲方法只需要权重去乘以脉冲“1”。这样就可以用简单的内存读出花费代价。
尽管激活受限于单个比特，脉冲网络不同于二分神经网络。重要的是脉冲时间，时间不是二进制量，并且可以使用数电的时钟信号。

2）Sparsity：只需存储发放值出现时间位置，因为有大量0的存在

3）Static suppression（即事件驱动处理）感觉外围具有多种机制，当受到刺激时促进神经元的兴奋性动态的、变化的刺激，同时抑制其对静态、不变的信息的响应。这也意味着每个像素独立激活所有其他像素，而不是等待全局快门产生静止帧，这将会带来许多能量节省，这低功耗和异步像素的混合允许时钟速度快，提供商用 DVS相机的时间分辨率为微秒。

## A.脉冲神经元
SNN的神经元单元是脉冲神经单元，就像ANN一样，脉冲神经单元也是将输入加权求和。但不是将这结果传入激活函数，这个结果代表这个神经元的膜电压。当膜电压达到一个阈值时，神经元就会给后续连接的神经元发放一个尖峰，然而，大多数神经元输入是脉冲，通常是一段很小时间的电活动。所有输入脉冲不会同时到达神经元体。这意味着存在随着时间推移维持膜电压的时间动力学。

![[Pasted image 20241010101247.png]]
这些动态早在1907年就被量化了。Lapicque 使用一个组装的电流源刺激了青蛙腿的神经纤维，并根据驱动电流 $I_{in}$ 的幅度和持续时间观察了青蛙腿抽搐所需的时间。他得出的结论是，
脉冲神经元大致类似于由电阻器 R 和电容器 C 组成的低通滤波器电路，后来被称为
the leaky integrate-and-fire (IF) 神经元。一个世纪后，这一理论依然成立：从生理学上讲，电容来自构成神经元膜的绝缘脂质双层。阻力来自门控离子通道的打开和关闭，调节跨膜的电荷载体扩散。使用 RC 电路建模的无源膜的动态可以表示为
![[Pasted image 20241010102209.png]]
$\tau=RC$是这个电路的时间常数，典型值为1-100ms，如果恒流输入，解是：
![[Pasted image 20241010102401.png]]
这表明，在电流注入后，$U(t)$ 呈指数松弛至稳态值，其中 $U_0$ 是 $t = 0$ 时的初始膜电位。为了使这个时变解与基于序列的神经网络兼容，在最简单的情况下使用前向欧拉方法来找到近似解
![[Pasted image 20241010102557.png]]
其中时间被明确离散化，$\beta=e^{-\frac{1}{\tau}}$是$U(t)$的衰减率（或者逆时间常数），完整推导看附录A1。

在深度学习中，输入的权重因子通常是一个可学习的参数。放宽迄今为止所做的物理上可行的假设，将 (3) 中的输入电流系数$(1 − β)$ 纳入可学习的权重 $W$，并简化 $(1-\beta)I_{in}[t] = W X[t]$ 以分离 $β$ 对输入 $X[t]$ 的影响。这里，$X[t]$ 被视为单个输入。大规模的网络会向量化$X[t]$，并且$W$被视作一个矩阵但在这里作为一个单独输入对一个单独神经元来说。最后考虑尖峰和膜电位重置给出：
![[Pasted image 20241010103801.png]]
$S_{out}[t]\in\{0,1\}$是神经元产生的输出尖峰，如果激活是1，重置减去阈值。
![[Pasted image 20241010104227.png]]
探索leaky IF 神经元时，会遇到一些轻微变体。
1）可以在更新膜电位之前应用脉冲阈值。这会导致输入信号 X 和触发脉冲之间存在一步延迟。

2）上述推导使用了“减法复位”（或软复位）机制。然而，附录 A1 中显示的替代方案是“复位为零”机制（或硬复位）。

3）可以将 (3) 中的因子 $(1 − β)$ 作为输入项 $W X$ 的系数。这将允许您模拟具有实际时间常数的神经元模型，但在最终应用于深度学习时不会带来任何优势。

![[Pasted image 20241010104703.png]]

## B.可替代尖峰神经元模型
Leaky IF 神经元模型是众多模型之一，其他模型有：
1）IF：leakage 机制被移除，$\beta=1$ 在（4）中。

2）Current-based：这些模型通常被称为 CuBa 神经元模型，将突触传导变化
纳入leaky IF 神经元中。如果默认的 LIF 神经元是一阶低通滤波器，那么 CuBa 神经元
就是二阶低通滤波器。输入脉冲序列经过两轮“平滑”，这意味着膜电位具有有限的上升时间，而不是在响应传入脉冲时经历不连续的跳跃。电位有限上升时间显示在Fig.5(d)。

3）Recurrent neurons：神经元的输出脉冲被路由回输入端，如图 6(a) 所示，具有明确的递归性。递归不是一种替代模型，而是一种可以应用于任何其他神经元的拓扑结构，可以以不同的方式实现：一对一递归，其中每个神经元将自己的脉冲路由到自身；或全对全递归，其中对整个层的输出脉冲进行加权和求和（例如，通过密集层或卷积层），然后再反馈回整个层。

4）Kernel-based models：也称为尖峰响应模型，其中预定义内核（例如“alpha 函数”：参见附录 C1）与输入尖峰进行卷积。可以选择将内核定义为任意形状，这提供了很大的灵活性。

5）Deep learning inspired spiking neurons：与其借鉴神经科学，不如从深度学习的原语开始，应用脉冲阈值。这有助于扩展基本循环神经元的短期容量。一些例子包括脉冲 LSTM 和 Legendre 记忆单元。最近，Transformer 已被用来进一步改善数据中的长距离记忆依赖性。以类似的方式，SpikeGPT 将自注意力近似为循环模型，首次展示了使用 SNN 进行自然语言生成。

6）Higher complexity neuroscience-inspired models：目前存在大量更详细的神经元模型。这些模型考虑了简单的泄漏积分器无法表示的生物物理现实主义和/或形态细节。最著名的模型包括 Hodgkin-Huxley 模型和 Izhikevich（或谐振器）模型，它们可以更准确地再现电生理结果。

主要要点如下：使用适合您任务的神经元模型。节能的深度学习将需要 LIF 模型。提高性能可能需要使用循环SNN。进一步提高性能（通常以牺牲效率为代价）可能需要从深度学习中衍生的方法，例如脉冲 LSTM 和循环脉冲transformers。或者，深度学习不是您的目标。如果您的目标是构建一个大脑模型，或者负责探索将低级动力学（离子、电导驱动或其他）与高阶大脑功能联系起来，那么也许更详细、生物物理上更准确的模型会成为您的朋友。

在以离散时间递归形式制定脉冲神经元之后，我们现在可以“借用”训练循环神经网络 (RNN) 和基于序列的模型方面的进展。此递归使用“隐式”循环连接来说明膜电位的衰减，并与“显式”循环区分开来，在“显式”循环中，输出脉冲 Sout 反馈到输入，如循环 SNN 中一样（见图 6）。

虽然有很多生理上更准确的神经元模型，但由于其计算效率高且易于训练，泄漏 IF 模型在基于梯度的学习中最为普遍。在进入第四节中的训练 SNN 之前，让我们先在第三节中了解一下脉冲的实际含义以及它们如何表示信息。

# 3.神经编码
![[Pasted image 20241012114922.png]]

## A.输入编码
输入数据不一定要被编码为spikes，可以将连续值当作输入，就像我们对光的感知始于连续数量的光子撞击我们的感光细胞一样。

静态数据（例如图像）可以视为直流 (dc) 输入，每个时间步骤中将相同的特征传递到 SNN 的输入层。然而，这并没有利用 SNN 从时间数据中提取含义的方式。一般来说，针对输入数据，三种编码机制已经得到普及。
1）Rate coding：将输入强度转换为firing rate或spike count。

2）Latenc coding：

3）Delta modulation

这是一个非详尽的列表，并且这些代码不一定相互独立。

1）Rate-Coded Inputs：感觉外围如何将有关世界的信息编码成脉冲？当明亮的光线照射到我们的感光细胞上时，视网膜会触发一连串脉冲到视觉皮层。Hubel 和 Wiesel 的诺贝尔视觉处理研究成果表明，更明亮的输入或有利的光线方向对应更高的放电率。举个简单的例子，亮像素被编码为高频放电率，而暗像素则会导致低频放电。测量神经元的放电率可能会变得非常微妙。最简单的方法是将输入刺激施加到神经元，计算其产生的动作电位总数，然后将其除以试验持续时间。虽然很简单，但这里的问题是神经元动力学会随时间而变化。无法保证试验开始时的放电率接近结束时的放电率。

另一种方法是在非常短的时间间隔 $∆t$ 内计算脉冲。对于足够小的 $∆t$，脉冲计数可以限制为 0 或 1，将可能结果的总数限制为仅两个。通过多次重复此实验，可以找到在 $∆t$ 内发生的平均脉冲数（在试验中）。此平均值必须等于或小于 1，解释为神经元在短暂时间间隔内发射的观测概率。要将其转换为时间相关的发射率，需要将试验平均值除以间隔的持续时间。频率编码的这种概率解释可以分布在多个神经元上，其中从一组神经元中计算脉冲支持群体编码。

这种表示对于顺序神经网络来说非常方便。RNN 中的每个离散时间步骤都可以被认为是持续短暂的持续时间 $∆t$，在此期间，要么出现尖峰，要么不出现尖峰。见附录B1

2）Latenc-Coded Inputs：延迟或时间代码与脉冲的时间有关。脉冲的总数不再重要。相反，脉冲发生的时间才是最重要的。例如，第一次脉冲时间机制将亮像素编码为早期脉冲，而暗输入将最后出现脉冲或根本不出现脉冲。与rate code相比，延迟编码机制为每个单独的脉冲赋予了更多含义。

神经元可以在巨大的动态范围内对感官刺激作出反应。在视网膜中，神经元可以检测到单个光子和数百万个光子的流入。为了处理这种变化如此大的刺激，感觉传导系统可能会以对数依赖性压缩刺激强度。出于这个原因，尖峰时间和输入特征强度之间的对数关系在文献中无处不在（见附录 B2）

尽管感觉通路似乎将频率编码的脉冲序列传输到我们的大脑，但延迟编码很可能主导大脑内部进行的实际处理。

3）Delta Modulated Inputs：增量调制基于神经元因变化而生长的概念，这是硅视网膜相机操作的基础，只有当输入强度随时间发生足够变化时，它才会产生输入。如果你的视野没有变化，那么你的感光细胞就不太容易发射。从计算上讲，这将采用时间序列输入并将阈值矩阵差异馈送到网络。虽然精确的实现可能有所不同，但一种常见的方法要求差异既为正值，又大于某个预定义的阈值，才能产生尖峰。这种编码技术也称为“阈值交叉”。或者，可以在多个时间步骤中跟踪强度的变化，其他方法可以解释负变化。

先前的技术倾向于将数据“转换”为尖峰。但是，以“预编码”尖峰形式本地捕获数据更有效。DVS 摄像机中的每个像素和硅耳蜗中的每个通道都使用增量调制来记录视觉或音频场景的变化。

## B. 输出编码
将输入数据编码成脉冲可以被认为是感觉外围如何将信号传输到大脑。另一方面，解码这些脉冲可以深入了解大脑如何处理这些编码信号。在训练 SNN 的背景下，编码机制不会限制解码机制。将我们的注意力从 SNN 的输入转移，我们如何解释输出神经元的放电行为？

1）Rate coding：选择具有最高发放率的神经元作为预测类。

2）延迟编码：选择第一个发放的神经元作为预测类。

3）Population coding 群体编码：依赖于每个类别的多个神经元。这通常与频率编码、等级顺序编码或 N-of-M 编码结合使用。

1）Rated-Coded Outputs：考虑一个多类分类问题，其中 $N_C$ 是类别数。非 SNN 将选择具有最大输出激活的神经元作为预测类别。对于频率编码脉冲网络，使用以最高频率发放的神经元。由于每个神经元都模拟相同数量的时步，因此只需选择具有最高脉冲计数的神经元即可。

2）延迟编码Outputs：神经元可以通过多种方式在脉冲时间中编码数据。与延迟编码输入的情况一样，可能首先正确类别的神经元发放。这解决了频率编码中需要多个脉冲而产生的能量负担。在硬件中，对更少脉冲的需求会降低内存访问的频率，这是深度学习加速器中的另一个计算负担。

从生物学角度来看，神经元按照第一次发放原则运作是否合理？如果我们的大脑没有不断重置为某个初始默认状态，我们如何定义“第一次”？这在概念上很容易解决。延迟编码的概念源于我们对突然输入刺激的反应。例如，当观看静态、不变的视觉场景时，视网膜会经历快速但微妙的扫视运动。投射到视网膜上的场景每隔几百毫秒就会发生变化。很有可能第一个脉冲必须相对于此扫视产生的参考信号发生。

3）群体编码：我们知道，人类的反应时间大约为 250 毫秒。如果人类大脑中神经元的平均放电频率为 10 Hz 左右，那么我们在反应时间内只能处理大约两到三个脉冲。然而，这个经常被引用的 10 Hz 假设应该被视为上限。当实验人员使用单个微电极寻找神经元时，低发放率神经元可能会被绕过，因为它们不会产生足够的脉冲来进行数据分析，或者完全被忽略。因此，高发放率神经元可能已经变得非常重要。从猕猴海马体中的慢性植入物收集的数据支持了这一点，这些植入物通常会产生背景放电频率低于 0.1 Hz 的神经元。人们必须等待至少 10 秒才能观察到单个脉冲！
可以通过在神经元群中使用分布式信息表示来解决此问题：如果单个神经元在短时间窗口内的脉冲数量有限，则只需使用更多神经元。来自神经元子群的脉冲可以汇集在一起​​以做出更快的决策。有趣的是，群体编码用并行性来代替顺序处理，这在 GPU 上训练 SNN 时更为理想。

4）Rate vs Latency Code：神经元是否将信息编码为频率、延迟或完全不同的东西，是一个备受争议的话题。我们并不寻求破解神经代码，而是旨在提供直觉，了解 SNN 何时可能从一种编码中受益。

Rate Codes的优点：
1）错误容忍：如果一个神经元未能激发，那么理想情况下，会有更多的脉冲来减轻这个错误带来的负担
2）更多的发放促进更多的学习：额外的脉冲通过误差反向传播为学习提供更强的梯度信号。如第四节所述，缺少脉冲会阻碍学习收敛（通常称为“死神经元问题”）。

Latency Codes优点：
1）能源消耗：生成和传输更少的尖峰意味着定制硬件中的动态功耗更低。由于全零输入向量的向量矩阵乘积返回零输出，因此它还降低了由于稀疏性而导致的内存访问频率。
2）速度：人类的反应时间大约为 250 毫秒。如果人脑中神经元的平均放电频率约为 10 Hz（这可能是高估），那么人们只能在此反应时间窗口中处理大约两到三个脉冲。相比之下，延迟代码依靠单个脉冲来表示信息。频率编码的这个问题可以通过将其与群体编码结合来解决：如果单个神经元在短时间窗口内的脉冲数量有限，那么只需使用更多神经元。这是以进一步加剧频率编码的功耗问题为代价的。

延迟代码的功耗优势也得到了生物学观察的支持，自然界会优化效率。Olshausen 和 Field's的工作在
“What is the other 85% of V1 doning?"理论上证明了频率编码最多只能解释初级视觉皮层15%的神经元活动。如果我们的神经元不加区分地默认使用频率编码，这将比延迟编码消耗更多能量。我们皮层神经元的平均发放率必须相当低，而延迟编码支持这一点。

基于梯度的 SNN 中较少探索的编码机制包括使用尖峰来表示预测或重建误差。大脑可能被视为一种基于其预测采取行动的预测机器。当这些预测与现实不符时，就会触发尖峰来更新系统。

一些人断言，真正的编码必须介于频率编码和延时编码之间，而另一些人则认为，这两者可能共存，只是在观察的时间尺度上有所不同：在长时间尺度上观察到频率，在短时间尺度上观察到延迟。有些人完全拒绝频率编码。这是深度学习从业者可能不太关心大脑在做什么，而更愿意关注最有用的东西的例子之一。

## C.目标函数
虽然我们的大脑不太可能使用像交叉熵损失函数这样明确的东西，但可以说人类和动物都有基线目标。生物变量，如多巴胺释放，与强化学习的目标函数有着有意义的联系。预测编码模型通常旨在最小化感官编码的信息熵，以便大脑可以主动预测传入信号并抑制它已经预期的信号。大脑功能的多面性可能需要存在多个目标。如何使用这些目标优化大脑仍然是一个谜，尽管我们可能会从多目标优化中获得启发。

可以使用各种损失函数来鼓励网络的输出层以频率或延迟编码的形式触发。最佳选择在很大程度上尚未确定，并且往往取决于网络超参数和手头任务的复杂性。下面描述的所有目标函数都已成功训练网络在各种数据集上获得具有竞争力的结果，尽管它们都有自己的权衡。

1）发放率目标函数：表 2 总结了在监督学习分类任务中常用的 SNN 方法，以促进正确的神经元类别以最高频率激发。一般来说，交叉熵损失或 mse 应用于神经元输出层的脉冲计数或膜电位。
![[Pasted image 20241012134715.png]]

有了足够多的时间步骤，通过尖峰计数，目标函数被更广泛地采用，因为它直接作用于尖峰。膜电位作为增加尖峰计数的代理，也不被视为可观察变量，这可能会部分抵消使用尖峰的计算优势。

交叉熵方法旨在抑制来自不正确类别的尖峰，这可能会使网络中的权重变为零。在没有额外正则化的情况下，这可能会导致神经元变得安静。通过使用均方尖峰速率（为每个类别指定一个目标尖峰数），可以将输出神经元置于触发的尖端。因此，与触发完全被抑制的神经元相比，网络可以以更快的响应时间适应不断变化的输入。

在模拟有限时间步长的网络中，权重的微小变化不太可能导致输出的脉冲数发生变化。将损失函数直接应用于更“连续”的信号（例如膜电位）可能更好。这是以在全精度隐藏状态而不是脉冲上进行操作为代价的。或者，使用群体编码可以将成本负担分散到多个神经元上，以增加权重更新改变输出层脉冲行为的可能性。它还增加了错误反向传播可能发生的路径数量，并提高了权重更新产生全局损失变化的可能性。

2）发放时间目标函数：实现发放时间目标的损失函数比发放率目标使用得少。两个可能的原因可以解释原因：1）错误率通常被认为是深度学习文献中最重要的指标，而频率编码对噪声的容忍度更高；2）时间编码的实现难度略大。下表3总结了方法：
![[Pasted image 20241012135717.png]]
这些目标的用例类似于发放率目标函数。使用发放时间的一个微妙挑战是，默认实现假设每个神经元至少脉冲一次，但事实并非如此。这可以通过在神经元不激发的情况下在最后时间步骤强制发放来处理。

一些最先进的模型完全放弃了输出端的脉冲神经元，并使用“读出层”来训练模型。这通常由具有无限高阈值（即它们永远不会触发）的 IF 层或使用标准激活函数（ReLU、sigmoid、softmax 等）的典型人工神经元组成。虽然这通常可以提高准确性，但这可能不符合完全脉冲网络的条件。这真的很重要吗？如果仍然可以实现功率效率，那么工程师就会很高兴，而这往往才是最重要的。


## D.学习法则

1）时间与空间的Credit Assignment：一旦确定了损失，就必须以某种方式使用它来更新网络参数，希望网络能够在训练任务中不断改进。每个权重都要为总损失的贡献承担一些责任，这被称为“credit assignment”。这可以分为空间和时间credit assignment(CS)问题。空间CS旨在找到导致误差的权重的空间位置，而时间CS问题旨在找到权重导致误差的时间。反向传播已被证明是一种非常强大的CS的方法，但大脑在开发这些挑战的解决方案方面受到的限制要大得多。

反向传播通过在学习过程中在前向传递之后应用不同的反向传递来解决空间CS问题。反向传递镜像前向传递，因此必须回忆前向传递的计算路径。相反，沿轴突的动作电位传播被认为是单向的，这可能会拒绝在大脑中进行反向传播的合理性。空间CS不仅涉及计算权重对误差的贡献，还涉及将误差分配回权重。即使大脑可以以某种方式计算梯度（或近似值），一个主要的挑战是如何将该梯度投射回突触并且如何知道哪个梯度属于哪个突触。

神经元充当有向边的这种限制正在逐渐放松，这可能是将错误分配给突触的一种机制。单个神经元内会发生许多双向非线性现象，这可能有助于帮助错误找到通往正确突触的路径。例如，在大多数存在前馈连接的地方都会观察到反馈连接。


# 摘要
大脑是一个理想的地方去探索更高效的神经网络。我们突触和神经元工作的原理给未来的深度学习带来了展望。我们也探索了脉冲编码和学习过程的复杂交互联系。

# 1.介绍
大脑能量效率远远高于神经网络，新一代受大脑启发的脉冲神经网络有望弥补这一差距。

## A.神经形态计算：A Quick Snapshot
类脑工程致力于模仿大脑计算原则来降低人工智能系统的功耗，为了复制生物系统，我们建立了三个部分。

	1）神经形态传感器受生物传感器启发，比如视网膜或耳蜗，通常记录信号的变化，而不是定期采用。仅当发生变化，才产生信号，信号通常称为“尖峰”。Static suppression

2）学习理解尖峰意义的神经形态算法被称为SNN，SNN不使用浮点值，而是使用单位二进制激活(尖峰)，这些激活随时间而不是强度编码信息。因此，SNN有着低精度参数和高时空稀疏性优点。
Spikes and Sparsity

3）这些模型在设计时考虑到了在专用神经形态硬件上的高能效执行。稀疏激活减少了芯片内外的数据移动，以加速神经形态工作负载，与传统硬件上的相同任务相比，这可以带来巨大的功耗和延迟增益。

## B.野外神经系统
总体目标是将已经在广泛领域证明其价值的人工神经网络(ANN)与SNN的潜在效率相结合。

![[Pasted image 20241009105021.png]]

尖峰算法已用于在医疗、机器人和混合现实领域以及许多其他领域实现低功耗人工智能算法。鉴于他们的能源效率，最初的商业产品通常目标是边缘计算应用。

在生物信号监测中，脑机或生物信号接口的神经植入物必须以最小功率在本地预处理信息，并且缺乏传输云计算数据的带宽。使用SNN完成的工作包括spike sorting,生物信号异常检测和脑机接口。除了生物医学干预之外，SNN 模型还用于机器人技术，旨在使其更像人类，并降低运营成本。无人机还必须在低功耗环境中运行，才能从轻型电池中获取尽可能多的价值，并且已从使用神经形态处理器中受益。由于 SNN 为时间信号处理提供了高效的计算机制，因此音频信号可以在神经形态硬件上以亚毫瓦 (mW) 功耗和低延迟进行处理。

[49] 中回顾了大量使用 SNN 的高效计算机视觉应用。SNN 同样适用于跟踪天空中的卫星等物体，以实现空间态势感知，并且已经进行了研究以促进人工智能的可持续利用，例如监测智能建筑中的材料应变和面临电力输送挑战的偏远地区的风力预测。在 2018-19 年特柳赖德神经形态和认知研讨会上，甚至建造了一个神经形态机器人来玩桌上足球！

除了神经形态应用之外，SNN 还用于测试关于自然智能如何产生的理论，从大脑的高级学习规则和记忆的形成方式到神经元和突触层的低级动态。


# 2. 从人工神经网络到脉冲神经网络
关于神经元是如何编码的，有个3s主题，脉冲神经网络的三个优点

1）Spikes：在大部分神经元中，动作电位的出现远比其出现微小变化重要。现在技术模型将神经元发放简化为离散1bit变化。在传统神经网络芯片中，各层激活传播所存储的高精度激活值使得其代价按昂贵。高精度乘法会带来进位延迟。相反，基于脉冲方法只需要权重去乘以脉冲“1”。这样就可以用简单的内存读出花费代价。
尽管激活受限于单个比特，脉冲网络不同于二分神经网络。重要的是脉冲时间，时间不是二进制量，并且可以使用数电的时钟信号。

2）Sparsity：只需存储发放值出现时间位置，因为有大量0的存在

3）Static suppression（即事件驱动处理）感觉外围具有多种机制，当受到刺激时促进神经元的兴奋性动态的、变化的刺激，同时抑制其对静态、不变的信息的响应。这也意味着每个像素独立激活所有其他像素，而不是等待全局快门产生静止帧，这将会带来许多能量节省，这低功耗和异步像素的混合允许时钟速度快，提供商用 DVS相机的时间分辨率为微秒。

## A.脉冲神经元
SNN的神经元单元是脉冲神经单元，就像ANN一样，脉冲神经单元也是将输入加权求和。但不是将这结果传入激活函数，这个结果代表这个神经元的膜电压。当膜电压达到一个阈值时，神经元就会给后续连接的神经元发放一个尖峰，然而，大多数神经元输入是脉冲，通常是一段很小时间的电活动。所有输入脉冲不会同时到达神经元体。这意味着存在随着时间推移维持膜电压的时间动力学。

![[Pasted image 20241010101247.png]]
这些动态早在1907年就被量化了。Lapicque 使用一个组装的电流源刺激了青蛙腿的神经纤维，并根据驱动电流 $I_{in}$ 的幅度和持续时间观察了青蛙腿抽搐所需的时间。他得出的结论是，
脉冲神经元大致类似于由电阻器 R 和电容器 C 组成的低通滤波器电路，后来被称为
the leaky integrate-and-fire (IF) 神经元。一个世纪后，这一理论依然成立：从生理学上讲，电容来自构成神经元膜的绝缘脂质双层。阻力来自门控离子通道的打开和关闭，调节跨膜的电荷载体扩散。使用 RC 电路建模的无源膜的动态可以表示为
![[Pasted image 20241010102209.png]]
$\tau=RC$是这个电路的时间常数，典型值为1-100ms，如果恒流输入，解是：
![[Pasted image 20241010102401.png]]
这表明，在电流注入后，$U(t)$ 呈指数松弛至稳态值，其中 $U_0$ 是 $t = 0$ 时的初始膜电位。为了使这个时变解与基于序列的神经网络兼容，在最简单的情况下使用前向欧拉方法来找到近似解
![[Pasted image 20241010102557.png]]
其中时间被明确离散化，$\beta=e^{-\frac{1}{\tau}}$是$U(t)$的衰减率（或者逆时间常数），完整推导看附录A1。

在深度学习中，输入的权重因子通常是一个可学习的参数。放宽迄今为止所做的物理上可行的假设，将 (3) 中的输入电流系数$(1 − β)$ 纳入可学习的权重 $W$，并简化 $(1-\beta)I_{in}[t] = W X[t]$ 以分离 $β$ 对输入 $X[t]$ 的影响。这里，$X[t]$ 被视为单个输入。大规模的网络会向量化$X[t]$，并且$W$被视作一个矩阵但在这里作为一个单独输入对一个单独神经元来说。最后考虑尖峰和膜电位重置给出：
![[Pasted image 20241010103801.png]]
$S_{out}[t]\in\{0,1\}$是神经元产生的输出尖峰，如果激活是1，重置减去阈值。
![[Pasted image 20241010104227.png]]
探索leaky IF 神经元时，会遇到一些轻微变体。
1）可以在更新膜电位之前应用脉冲阈值。这会导致输入信号 X 和触发脉冲之间存在一步延迟。

2）上述推导使用了“减法复位”（或软复位）机制。然而，附录 A1 中显示的替代方案是“复位为零”机制（或硬复位）。

3）可以将 (3) 中的因子 $(1 − β)$ 作为输入项 $W X$ 的系数。这将允许您模拟具有实际时间常数的神经元模型，但在最终应用于深度学习时不会带来任何优势。

![[Pasted image 20241010104703.png]]

## B.可替代尖峰神经元模型
Leaky IF 神经元模型是众多模型之一，其他模型有：
1）IF：leakage 机制被移除，$\beta=1$ 在（4）中。

2）Current-based：这些模型通常被称为 CuBa 神经元模型，将突触传导变化
纳入leaky IF 神经元中。如果默认的 LIF 神经元是一阶低通滤波器，那么 CuBa 神经元
就是二阶低通滤波器。输入脉冲序列经过两轮“平滑”，这意味着膜电位具有有限的上升时间，而不是在响应传入脉冲时经历不连续的跳跃。电位有限上升时间显示在Fig.5(d)。

3）Recurrent neurons：神经元的输出脉冲被路由回输入端，如图 6(a) 所示，具有明确的递归性。递归不是一种替代模型，而是一种可以应用于任何其他神经元的拓扑结构，可以以不同的方式实现：一对一递归，其中每个神经元将自己的脉冲路由到自身；或全对全递归，其中对整个层的输出脉冲进行加权和求和（例如，通过密集层或卷积层），然后再反馈回整个层。

4）Kernel-based models：也称为尖峰响应模型，其中预定义内核（例如“alpha 函数”：参见附录 C1）与输入尖峰进行卷积。可以选择将内核定义为任意形状，这提供了很大的灵活性。

5）Deep learning inspired spiking neurons：与其借鉴神经科学，不如从深度学习的原语开始，应用脉冲阈值。这有助于扩展基本循环神经元的短期容量。一些例子包括脉冲 LSTM 和 Legendre 记忆单元。最近，Transformer 已被用来进一步改善数据中的长距离记忆依赖性。以类似的方式，SpikeGPT 将自注意力近似为循环模型，首次展示了使用 SNN 进行自然语言生成。

6）Higher complexity neuroscience-inspired models：目前存在大量更详细的神经元模型。这些模型考虑了简单的泄漏积分器无法表示的生物物理现实主义和/或形态细节。最著名的模型包括 Hodgkin-Huxley 模型和 Izhikevich（或谐振器）模型，它们可以更准确地再现电生理结果。

主要要点如下：使用适合您任务的神经元模型。节能的深度学习将需要 LIF 模型。提高性能可能需要使用循环SNN。进一步提高性能（通常以牺牲效率为代价）可能需要从深度学习中衍生的方法，例如脉冲 LSTM 和循环脉冲transformers。或者，深度学习不是您的目标。如果您的目标是构建一个大脑模型，或者负责探索将低级动力学（离子、电导驱动或其他）与高阶大脑功能联系起来，那么也许更详细、生物物理上更准确的模型会成为您的朋友。

在以离散时间递归形式制定脉冲神经元之后，我们现在可以“借用”训练循环神经网络 (RNN) 和基于序列的模型方面的进展。此递归使用“隐式”循环连接来说明膜电位的衰减，并与“显式”循环区分开来，在“显式”循环中，输出脉冲 Sout 反馈到输入，如循环 SNN 中一样（见图 6）。

虽然有很多生理上更准确的神经元模型，但由于其计算效率高且易于训练，泄漏 IF 模型在基于梯度的学习中最为普遍。在进入第四节中的训练 SNN 之前，让我们先在第三节中了解一下脉冲的实际含义以及它们如何表示信息。